+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-0
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-0 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 1 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-7
+ shift
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-5
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-7 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 8 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-5 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 6 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-2
+ shift
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-6
+ shift
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-1
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-2 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 3 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-6 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 7 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-3
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-4
+ shift
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-1 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 2 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-3 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 4 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-4 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2485321728" -mca ess_base_vpid 5 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2485321728.0;tcp://172.21.154.78:47813" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2485321728.0;tcp://172.21.154.78:47813" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
mpirun2pytorch: MASTER_ADDR=172.21.76.114 MASTER_PORT=29500 GROUP_RANK=0 WORLD_SIZE=64
STARTING TIMING RUN AT 2025-04-30 01:47:47 PM
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********


************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..


************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 79079
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 79079
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

setting number of microbatches to constant 1
Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
[W430 13:48:03.292349967 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
[W430 13:48:03.299798677 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
[W430 13:48:03.469146992 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
[W430 13:48:03.478400581 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
[W430 13:48:03.480602983 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
[W430 13:48:03.401981823 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
[W430 13:48:03.402896919 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:03.403743597 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:03.422725403 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
[W430 13:48:03.437202676 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:03.437866097 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
[W430 13:48:05.441881812 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:05.442115466 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:05.442728053 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
[W430 13:48:08.566616858 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.568307527 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.057722184 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.670946409 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.682189356 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.686033894 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.601084365 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.621610529 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.712300436 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.712681810 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.624264128 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.112968648 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.116280402 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.121162872 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.124578668 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.724618959 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.305091296 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.312215774 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.357265541 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.374297950 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.389919603 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.398184072 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.514078477 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.430127891 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.431435645 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.550134259 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.553569377 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.556847575 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.469810638 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.478891520 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.576274798 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:08.468718470 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:09.504801904 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:09.511604825 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:09.535651931 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:09.536681723 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:09.539377050 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:10.514242159 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:10.556892191 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:11.620847255 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:11.630029206 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:11.666982413 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:13.943416841 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:13.431200364 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:13.096905273 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:13.111771906 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:13.055792364 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:13.564812544 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:14.709854867 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 13:48:14.237307995 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
Loading distributed checkpoint directly on the GPU
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 1, achieved: 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (11141120 elements):
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00036, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')

  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 17.3 B | n/a 
---------------------------------------------
11.1 M    Trainable params
17.2 B    Non-trainable params
17.3 B    Total params
69,029.364Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
:::MLLOG {"namespace": "", "time_ms": 1745992231453, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 332}}
:::MLLOG {"namespace": "", "time_ms": 1745992231453, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 333}}
:::MLLOG {"namespace": "", "time_ms": 1745992231453, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745992231453, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745992231453, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745992231453, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745992231453, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745992231454, "event_type": "POINT_IN_TIME", "key": "seed", "value": 79079, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 335}}
:::MLLOG {"namespace": "", "time_ms": 1745992231454, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 341}}
:::MLLOG {"namespace": "", "time_ms": 1745992231920, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 346}}
:::MLLOG {"namespace": "", "time_ms": 1745992231943, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 351}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 356}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 360}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 364}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 369}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 370}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00036, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 371}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 372}}
:::MLLOG {"namespace": "", "time_ms": 1745992231944, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 373}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
!!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
!!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
!!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
!!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
!!! [UB] Number of physical nodes: 8
!!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
!!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
!!! [UB] Create Userbuffers Communicator
!!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
!!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
MC initialized succesfully, window size = 549755813888
!!! [UBP2P] Register UBuf 1
!!! [UBP2P] Register UBuf 2
!!! [UBP2P] Register UBuf 3
!!! [UBP2P] Register UBuf 4
!!! [UBP2P] Register UBuf 5
!!! [UB] Register UBuf 6
!!! [UB] Register UBuf 7
!!! [UB] Register UBuf 8
!!! [UB] Register UBuf 9
!!! [UB] Register UBuf 10
:::MLLOG {"namespace": "", "time_ms": 1745992340313, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1745992340314, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1745992340314, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
:::MLLOG {"namespace": "", "time_ms": 1745992345755, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.221100091934204, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 80, "lr": 0.0003599152951501968}}
:::MLLOG {"namespace": "", "time_ms": 1745992351205, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.496211051940918, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.00035966126032202684}}
:::MLLOG {"namespace": "", "time_ms": 1745992356641, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4831799268722534, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 240, "lr": 0.0003592381346041788}}
:::MLLOG {"namespace": "", "time_ms": 1745992362087, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3429796695709229, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.00035864631622776783}}
:::MLLOG {"namespace": "", "time_ms": 1745992367555, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3140318393707275, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 400, "lr": 0.0003578863621915349}}
:::MLLOG {"namespace": "", "time_ms": 1745992373019, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3687406778335571, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00035695898773761896}}
:::MLLOG {"namespace": "", "time_ms": 1745992378484, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3187226057052612, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 560, "lr": 0.0003558650656783958}}
:::MLLOG {"namespace": "", "time_ms": 1745992383948, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3414045572280884, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00035460562557501797}}
:::MLLOG {"namespace": "", "time_ms": 1745992389416, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3505533933639526, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 720, "lr": 0.00035318185276842753}}
:::MLLOG {"namespace": "", "time_ms": 1745992394886, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3189610242843628, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.0003515950872637549}}
:::MLLOG {"namespace": "", "time_ms": 1745992400357, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4091891050338745, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 880, "lr": 0.00034984682246915285}}
:::MLLOG {"namespace": "", "time_ms": 1745992405846, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.326363444328308, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.000347938703790253}}
:::MLLOG {"namespace": "", "time_ms": 1745992411324, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3592090606689453, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1040, "lr": 0.00034587252708156757}}
:::MLLOG {"namespace": "", "time_ms": 1745992416808, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2974579334259033, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.00034365023695629403}}
:::MLLOG {"namespace": "", "time_ms": 1745992422297, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3404591083526611, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1200, "lr": 0.00034127392495611334}}
:::MLLOG {"namespace": "", "time_ms": 1745992427785, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.313457727432251, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003387458275827039}}
:::MLLOG {"namespace": "", "time_ms": 1745992433263, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3247697353363037, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1360, "lr": 0.0003360683241928247}}
:::MLLOG {"namespace": "", "time_ms": 1745992438743, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.209972620010376, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.00033324393475894776}}
:::MLLOG {"namespace": "", "time_ms": 1745992444223, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3154250383377075, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1520, "lr": 0.0003302753174975484}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
:::MLLOG {"namespace": "", "time_ms": 1745992455264, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.633805882228279}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745992455264, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745992455264, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1536}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745992464628, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9394552707672119, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745992464628, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745992464628, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745992469013, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3809067010879517, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003271652663672851}}
:::MLLOG {"namespace": "", "time_ms": 1745992474492, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3260024785995483, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1680, "lr": 0.00032391670843942295}}
:::MLLOG {"namespace": "", "time_ms": 1745992479968, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2340803146362305, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.000320532701142977}}
:::MLLOG {"namespace": "", "time_ms": 1745992485446, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3379895687103271, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1840, "lr": 0.00031701642938716714}}
:::MLLOG {"namespace": "", "time_ms": 1745992490924, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2937523126602173, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003133712025638927}}
:::MLLOG {"namespace": "", "time_ms": 1745992490930, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.609706809831106}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745992490930, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745992490930, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745992499638, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9346891045570374, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745992499639, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745992499639, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745992505125, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2938780784606934, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2000, "lr": 0.0003096004514330487}}
:::MLLOG {"namespace": "", "time_ms": 1745992510622, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3177965879440308, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00030570772489361514}}
:::MLLOG {"namespace": "", "time_ms": 1745992516100, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3236849308013916, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2160, "lr": 0.0003016966866435569}}
:::MLLOG {"namespace": "", "time_ms": 1745992521584, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2755526304244995, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0002975711117316798}}
:::MLLOG {"namespace": "", "time_ms": 1745992525987, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.583600093574795}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745992525987, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745992525987, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745992534730, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9308872222900391, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745992534730, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745992534731, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745992535821, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.318509578704834, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2320, "lr": 0.0002933348830046869}}
:::MLLOG {"namespace": "", "time_ms": 1745992541309, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.244160532951355, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.0002889919874527786}}
:::MLLOG {"namespace": "", "time_ms": 1745992546808, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3046150207519531, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2480, "lr": 0.0002845465124572376}}
:::MLLOG {"namespace": "", "time_ms": 1745992552308, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3414411544799805, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0002800026419435284}}
:::MLLOG {"namespace": "", "time_ms": 1745992557812, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3367096185684204, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2640, "lr": 0.0002753646524435331}}
:::MLLOG {"namespace": "", "time_ms": 1745992561113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.565381101231953}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745992561113, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745992561113, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745992569890, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9239693284034729, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745992569890, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745992569890, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9239693284034729, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 321, "samples_count": 2688, "status": "success"}}
[rank40]:[W430 13:56:14.672867472 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank12]:[W430 13:56:14.771011498 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank49]:[W430 13:56:14.890327421 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank55]:[W430 13:56:14.899642638 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank54]:[W430 13:56:14.906474193 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank24]:[W430 13:56:14.808635322 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank46]:[W430 13:56:14.820997636 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank45]:[W430 13:56:14.823394197 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank52]:[W430 13:56:14.037938337 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank26]:[W430 13:56:14.966620223 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank31]:[W430 13:56:14.980105240 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank63]:[W430 13:56:14.078761453 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank30]:[W430 13:56:14.986703021 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank28]:[W430 13:56:14.987134551 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank56]:[W430 13:56:14.088371635 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank16]:[W430 13:56:14.978881092 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank29]:[W430 13:56:14.999295829 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank47]:[W430 13:56:14.005936401 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank9]:[W430 13:56:14.076515322 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank51]:[W430 13:56:14.135327735 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W430 13:56:14.636128933 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W430 13:56:14.636509021 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank19]:[W430 13:56:14.043444562 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W430 13:56:14.648575709 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank36]:[W430 13:56:14.755527888 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank17]:[W430 13:56:14.070293867 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank38]:[W430 13:56:14.775391753 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank21]:[W430 13:56:14.089249436 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank53]:[W430 13:56:14.201442531 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank23]:[W430 13:56:14.090004190 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W430 13:56:14.700210727 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank59]:[W430 13:56:14.223434556 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank14]:[W430 13:56:14.194279358 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank8]:[W430 13:56:14.216783691 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank10]:[W430 13:56:14.224205621 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank11]:[W430 13:56:14.231210608 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank42]:[W430 13:56:14.173723239 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank20]:[W430 13:56:14.157583735 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank44]:[W430 13:56:14.174129739 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank48]:[W430 13:56:14.302669626 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank57]:[W430 13:56:14.300661010 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank25]:[W430 13:56:14.202044239 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank39]:[W430 13:56:14.889382798 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank50]:[W430 13:56:14.319761366 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank37]:[W430 13:56:14.898898515 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W430 13:56:14.841378644 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank61]:[W430 13:56:14.360209756 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank41]:[W430 13:56:14.268519469 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank18]:[W430 13:56:14.258135628 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank13]:[W430 13:56:14.337718156 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W430 13:56:14.865138665 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W430 13:56:14.867855763 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank27]:[W430 13:56:14.286216292 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank15]:[W430 13:56:14.358487920 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank33]:[W430 13:56:14.970834502 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank35]:[W430 13:56:14.974473909 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank22]:[W430 13:56:14.308229566 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank43]:[W430 13:56:14.352536763 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W430 13:56:14.941437254 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank62]:[W430 13:56:14.478524119 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank58]:[W430 13:56:14.508535085 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank32]:[W430 13:56:14.151102060 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank34]:[W430 13:56:14.158184925 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank60]:[W430 13:56:14.589253624 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ENDING TIMING RUN AT 2025-04-30 01:56:29 PM
RESULT,LLM_FINETUNING,522,nvidia,2025-04-30 01:47:47 PM
