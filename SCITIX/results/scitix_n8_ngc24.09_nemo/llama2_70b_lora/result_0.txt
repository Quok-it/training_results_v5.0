+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-3
+ shift
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-1
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-3 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 4 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-1 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 2 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-4
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-6
+ shift
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-6 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 7 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-4 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 5 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-7
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-7 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 8 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-5
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-5 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 6 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-0
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-0 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 1 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-2
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-2 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "4169990144" -mca ess_base_vpid 3 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "4169990144.0;tcp://172.21.154.78:50911" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "4169990144.0;tcp://172.21.154.78:50911" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
mpirun2pytorch: MASTER_ADDR=172.21.76.114 MASTER_PORT=29500 GROUP_RANK=0 WORLD_SIZE=64
STARTING TIMING RUN AT 2025-04-30 10:49:47 AM
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********
GPU available: True (cuda), used: True

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 95628
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 95628
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
setting number of microbatches to constant 1
Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
[W430 10:50:03.876272661 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
[W430 10:50:07.890143654 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.954218014 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.974696647 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.330498850 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.009633295 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.342467695 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.413397780 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.427641142 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:07.433455872 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.627640831 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.542243056 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.674752999 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.643920592 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.686102617 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.658140320 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.699595408 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.609887607 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.612697991 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.623050565 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.720874315 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.628151998 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.706357774 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.721978307 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.733459742 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.786171097 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.798169397 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.845738759 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.846103935 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.861739781 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.452096910 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.453096320 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.459137582 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.459298681 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.885459988 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.892303160 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.896123592 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.917111815 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.920814178 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:08.716352482 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:12.020210865 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:12.060760190 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:12.093136076 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.799656676 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.853948490 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.924943155 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.034069304 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.028264381 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.969878938 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.075037288 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.074089926 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.217916737 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.136802064 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.120314796 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.145983084 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.320756356 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.335178028 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.334236892 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.901491206 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.429127470 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.984326385 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:13.012792649 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:14.586999350 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 10:50:14.196487719 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
Loading distributed checkpoint directly on the GPU
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 1, achieved: 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (11141120 elements):
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00036, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')

  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 17.3 B | n/a 
---------------------------------------------
11.1 M    Trainable params
17.2 B    Non-trainable params
17.3 B    Total params
69,029.364Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
:::MLLOG {"namespace": "", "time_ms": 1745981550780, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 332}}
:::MLLOG {"namespace": "", "time_ms": 1745981550780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 333}}
:::MLLOG {"namespace": "", "time_ms": 1745981550780, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745981550780, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745981550781, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745981550781, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745981550781, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745981550781, "event_type": "POINT_IN_TIME", "key": "seed", "value": 95628, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 335}}
:::MLLOG {"namespace": "", "time_ms": 1745981550781, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 341}}
:::MLLOG {"namespace": "", "time_ms": 1745981551254, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 346}}
:::MLLOG {"namespace": "", "time_ms": 1745981551276, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 351}}
:::MLLOG {"namespace": "", "time_ms": 1745981551276, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 356}}
:::MLLOG {"namespace": "", "time_ms": 1745981551276, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 360}}
:::MLLOG {"namespace": "", "time_ms": 1745981551276, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 364}}
:::MLLOG {"namespace": "", "time_ms": 1745981551276, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 369}}
:::MLLOG {"namespace": "", "time_ms": 1745981551277, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 370}}
:::MLLOG {"namespace": "", "time_ms": 1745981551277, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00036, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 371}}
:::MLLOG {"namespace": "", "time_ms": 1745981551277, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 372}}
:::MLLOG {"namespace": "", "time_ms": 1745981551277, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 373}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
!!! [UB] Number of physical nodes: 8
!!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
!!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
!!! [UB] Create Userbuffers Communicator
!!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
!!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
!!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
!!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
!!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
!!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
MC initialized succesfully, window size = 549755813888
!!! [UBP2P] Register UBuf 1
!!! [UBP2P] Register UBuf 2
!!! [UBP2P] Register UBuf 3
!!! [UBP2P] Register UBuf 4
!!! [UBP2P] Register UBuf 5
!!! [UB] Register UBuf 6
!!! [UB] Register UBuf 7
!!! [UB] Register UBuf 8
!!! [UB] Register UBuf 9
!!! [UB] Register UBuf 10
:::MLLOG {"namespace": "", "time_ms": 1745981657985, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1745981657985, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1745981657985, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
:::MLLOG {"namespace": "", "time_ms": 1745981663401, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.1906027793884277, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 80, "lr": 0.0003599152951501968}}
:::MLLOG {"namespace": "", "time_ms": 1745981668836, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4543073177337646, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.00035966126032202684}}
:::MLLOG {"namespace": "", "time_ms": 1745981674284, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4077833890914917, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 240, "lr": 0.0003592381346041788}}
:::MLLOG {"namespace": "", "time_ms": 1745981679745, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3061763048171997, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.00035864631622776783}}
:::MLLOG {"namespace": "", "time_ms": 1745981685198, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3718547821044922, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 400, "lr": 0.0003578863621915349}}
:::MLLOG {"namespace": "", "time_ms": 1745981690655, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3056761026382446, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00035695898773761896}}
:::MLLOG {"namespace": "", "time_ms": 1745981696118, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3665685653686523, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 560, "lr": 0.0003558650656783958}}
:::MLLOG {"namespace": "", "time_ms": 1745981701583, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2525336742401123, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00035460562557501797}}
:::MLLOG {"namespace": "", "time_ms": 1745981707055, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2904480695724487, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 720, "lr": 0.00035318185276842753}}
:::MLLOG {"namespace": "", "time_ms": 1745981712527, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3766934871673584, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.0003515950872637549}}
:::MLLOG {"namespace": "", "time_ms": 1745981717996, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3268948793411255, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 880, "lr": 0.00034984682246915285}}
:::MLLOG {"namespace": "", "time_ms": 1745981723466, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3492079973220825, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.000347938703790253}}
:::MLLOG {"namespace": "", "time_ms": 1745981728937, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2719777822494507, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1040, "lr": 0.00034587252708156757}}
:::MLLOG {"namespace": "", "time_ms": 1745981734410, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.289717435836792, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.00034365023695629403}}
:::MLLOG {"namespace": "", "time_ms": 1745981739891, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3320515155792236, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1200, "lr": 0.00034127392495611334}}
:::MLLOG {"namespace": "", "time_ms": 1745981745364, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.262097954750061, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003387458275827039}}
:::MLLOG {"namespace": "", "time_ms": 1745981750849, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2682470083236694, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1360, "lr": 0.0003360683241928247}}
:::MLLOG {"namespace": "", "time_ms": 1745981756339, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2631438970565796, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.00033324393475894776}}
:::MLLOG {"namespace": "", "time_ms": 1745981761813, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3669040203094482, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1520, "lr": 0.0003302753174975484}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
:::MLLOG {"namespace": "", "time_ms": 1745981772866, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.645725375793022}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745981772866, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745981772867, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1536}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745981782237, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9381526708602905, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745981782237, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745981782237, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745981786620, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2022736072540283, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003271652663672851}}
:::MLLOG {"namespace": "", "time_ms": 1745981792111, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.1575798988342285, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1680, "lr": 0.00032391670843942295}}
:::MLLOG {"namespace": "", "time_ms": 1745981797599, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.329541802406311, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.000320532701142977}}
:::MLLOG {"namespace": "", "time_ms": 1745981803092, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.348550796508789, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1840, "lr": 0.00031701642938716714}}
:::MLLOG {"namespace": "", "time_ms": 1745981808580, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3207546472549438, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003133712025638927}}
:::MLLOG {"namespace": "", "time_ms": 1745981808587, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.583176093392508}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745981808587, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745981808587, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745981817282, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9330617189407349, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745981817282, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745981817282, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745981822747, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2630261182785034, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2000, "lr": 0.0003096004514330487}}
:::MLLOG {"namespace": "", "time_ms": 1745981828225, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3221737146377563, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00030570772489361514}}
:::MLLOG {"namespace": "", "time_ms": 1745981833696, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.359847903251648, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2160, "lr": 0.0003016966866435569}}
:::MLLOG {"namespace": "", "time_ms": 1745981839168, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.346772313117981, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0002975711117316798}}
:::MLLOG {"namespace": "", "time_ms": 1745981843552, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.627182848656663}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745981843552, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745981843552, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745981852201, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9281702041625977, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745981852201, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745981852201, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745981853292, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3069002628326416, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2320, "lr": 0.0002933348830046869}}
:::MLLOG {"namespace": "", "time_ms": 1745981858768, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.337644338607788, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.0002889919874527786}}
:::MLLOG {"namespace": "", "time_ms": 1745981864249, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3244143724441528, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2480, "lr": 0.0002845465124572376}}
:::MLLOG {"namespace": "", "time_ms": 1745981869741, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2681233882904053, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0002800026419435284}}
:::MLLOG {"namespace": "", "time_ms": 1745981875222, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2988660335540771, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2640, "lr": 0.0002753646524435331}}
:::MLLOG {"namespace": "", "time_ms": 1745981878515, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.6029446324838}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745981878515, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745981878515, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745981887251, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9276310205459595, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745981887251, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745981887251, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745981889435, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2444554567337036, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.0002706369090706292}}
:::MLLOG {"namespace": "", "time_ms": 1745981894915, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2448153495788574, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2800, "lr": 0.00026582386141139804}}
:::MLLOG {"namespace": "", "time_ms": 1745981900393, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3177844285964966, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.0002609300393378292}}
:::MLLOG {"namespace": "", "time_ms": 1745981905881, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3270299434661865, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2960, "lr": 0.000255960048743964}}
:::MLLOG {"namespace": "", "time_ms": 1745981911358, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3337827920913696, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.00025091856721098867}}
:::MLLOG {"namespace": "", "time_ms": 1745981913552, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.609962186476668}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745981913552, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745981913552, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745981922295, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9244829416275024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745981922296, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745981922296, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9244829416275024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 321, "samples_count": 3072, "status": "success"}}
[rank40]:[W430 10:58:46.102768610 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank20]:[W430 10:58:46.125097449 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank38]:[W430 10:58:46.838918822 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank34]:[W430 10:58:46.841817893 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank52]:[W430 10:58:46.267105063 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank32]:[W430 10:58:46.865651343 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank25]:[W430 10:58:46.266706793 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank48]:[W430 10:58:46.369802057 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank55]:[W430 10:58:46.373265890 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank23]:[W430 10:58:46.291233918 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W430 10:58:46.898734999 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank22]:[W430 10:58:46.301577757 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank50]:[W430 10:58:46.426621273 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank37]:[W430 10:58:46.029200011 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank39]:[W430 10:58:46.050707614 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank15]:[W430 10:58:46.449279620 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank26]:[W430 10:58:46.385619111 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W430 10:58:46.026977425 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank63]:[W430 10:58:46.542272151 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank31]:[W430 10:58:46.443747159 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank36]:[W430 10:58:46.125465161 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W430 10:58:46.041477341 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank62]:[W430 10:58:46.552497796 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank28]:[W430 10:58:46.457821540 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank12]:[W430 10:58:46.523966680 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank43]:[W430 10:58:46.466782341 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank61]:[W430 10:58:46.560957287 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank54]:[W430 10:58:46.565004356 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank30]:[W430 10:58:46.463836520 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank42]:[W430 10:58:46.471842013 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank49]:[W430 10:58:46.569197270 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W430 10:58:46.057223837 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank51]:[W430 10:58:46.572052130 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank29]:[W430 10:58:46.472452623 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank45]:[W430 10:58:46.478349709 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank46]:[W430 10:58:46.479459418 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank10]:[W430 10:58:46.542595541 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank18]:[W430 10:58:46.465753916 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank14]:[W430 10:58:46.560261557 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W430 10:58:46.083477686 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W430 10:58:47.103384836 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank13]:[W430 10:58:47.598946694 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank60]:[W430 10:58:47.667479358 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank33]:[W430 10:58:47.246364404 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank16]:[W430 10:58:47.559564085 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank19]:[W430 10:58:47.600929176 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank53]:[W430 10:58:47.714641006 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank17]:[W430 10:58:47.608523206 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank21]:[W430 10:58:47.613157202 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank24]:[W430 10:58:47.630786549 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank41]:[W430 10:58:47.655702756 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank9]:[W430 10:58:47.716305653 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank57]:[W430 10:58:47.749251347 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank35]:[W430 10:58:47.339870701 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W430 10:58:47.256721020 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W430 10:58:47.270330506 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank56]:[W430 10:58:47.780284442 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank47]:[W430 10:58:47.701317361 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank27]:[W430 10:58:47.702377945 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank44]:[W430 10:58:47.715404487 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank58]:[W430 10:58:47.837032718 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank11]:[W430 10:58:47.818733248 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank59]:[W430 10:58:47.851700202 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank8]:[W430 10:58:47.848223731 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ENDING TIMING RUN AT 2025-04-30 10:59:00 AM
RESULT,LLM_FINETUNING,553,nvidia,2025-04-30 10:49:47 AM
