+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-1
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-1 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 2 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-5
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-3
+ shift
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-5 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 6 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-3 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 4 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-6
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-6 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 7 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-7
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-7 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 8 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-2
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-2 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 3 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-4
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-4 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 5 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-0
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-0 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "2662072320" -mca ess_base_vpid 1 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "2662072320.0;tcp://172.21.154.78:33955" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "2662072320.0;tcp://172.21.154.78:33955" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
mpirun2pytorch: MASTER_ADDR=172.21.76.114 MASTER_PORT=29500 GROUP_RANK=0 WORLD_SIZE=64
STARTING TIMING RUN AT 2025-04-30 02:34:21 PM
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 87999
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 87999
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

setting number of microbatches to constant 1
Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
[W430 14:34:37.737744639 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
[W430 14:34:39.793660231 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
[W430 14:34:39.854549851 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
[W430 14:34:42.821082418 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.824671972 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.826236056 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.542582516 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.903136862 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.904133447 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.576669622 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.905153805 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.912045298 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.913746061 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.622504589 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.634076490 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.958409584 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.639939642 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.966710394 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.980282819 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.984773116 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.991928820 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.990302007 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.997518066 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.095774770 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.096692550 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.024711733 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.029365552 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.167744893 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.170022545 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.081418537 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.177697244 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.180976207 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.183116312 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.226044739 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.243295434 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.244872444 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.769896381 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.775346586 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.193860197 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.813655768 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.815816882 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:42.833690594 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:44.928467651 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:44.930320522 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:44.108224418 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:44.222361269 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:44.228647558 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:44.249709495 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.842316299 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.183270783 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.277577539 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.220872372 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.969820245 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.364610212 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.384695106 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.052577727 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:47.181859332 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:48.648742806 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:48.573817614 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:48.713922820 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:48.745712342 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:48.802099647 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:48.901689918 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 14:34:48.392063615 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
Loading distributed checkpoint directly on the GPU
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 1, achieved: 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (11141120 elements):
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00036, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')

  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 17.3 B | n/a 
---------------------------------------------
11.1 M    Trainable params
17.2 B    Non-trainable params
17.3 B    Total params
69,029.364Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
:::MLLOG {"namespace": "", "time_ms": 1745995025730, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 332}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 333}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "POINT_IN_TIME", "key": "seed", "value": 87999, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 335}}
:::MLLOG {"namespace": "", "time_ms": 1745995025731, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 341}}
:::MLLOG {"namespace": "", "time_ms": 1745995026213, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 346}}
:::MLLOG {"namespace": "", "time_ms": 1745995026236, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 351}}
:::MLLOG {"namespace": "", "time_ms": 1745995026236, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 356}}
:::MLLOG {"namespace": "", "time_ms": 1745995026236, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 360}}
:::MLLOG {"namespace": "", "time_ms": 1745995026236, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 364}}
:::MLLOG {"namespace": "", "time_ms": 1745995026236, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 369}}
:::MLLOG {"namespace": "", "time_ms": 1745995026236, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 370}}
:::MLLOG {"namespace": "", "time_ms": 1745995026237, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00036, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 371}}
:::MLLOG {"namespace": "", "time_ms": 1745995026237, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 372}}
:::MLLOG {"namespace": "", "time_ms": 1745995026237, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 373}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
!!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
!!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
!!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
!!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
!!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
!!! [UB] Number of physical nodes: 8
!!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
!!! [UB] Create Userbuffers Communicator
!!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
!!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
MC initialized succesfully, window size = 549755813888
!!! [UBP2P] Register UBuf 1
!!! [UBP2P] Register UBuf 2
!!! [UBP2P] Register UBuf 3
!!! [UBP2P] Register UBuf 4
!!! [UBP2P] Register UBuf 5
!!! [UB] Register UBuf 6
!!! [UB] Register UBuf 7
!!! [UB] Register UBuf 8
!!! [UB] Register UBuf 9
!!! [UB] Register UBuf 10
:::MLLOG {"namespace": "", "time_ms": 1745995135952, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1745995135952, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1745995135952, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
:::MLLOG {"namespace": "", "time_ms": 1745995141387, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.2846555709838867, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 80, "lr": 0.0003599152951501968}}
:::MLLOG {"namespace": "", "time_ms": 1745995146828, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.5294671058654785, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.00035966126032202684}}
:::MLLOG {"namespace": "", "time_ms": 1745995152287, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.379960536956787, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 240, "lr": 0.0003592381346041788}}
:::MLLOG {"namespace": "", "time_ms": 1745995157751, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3977644443511963, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.00035864631622776783}}
:::MLLOG {"namespace": "", "time_ms": 1745995163211, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.362784743309021, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 400, "lr": 0.0003578863621915349}}
:::MLLOG {"namespace": "", "time_ms": 1745995168691, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.322356104850769, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00035695898773761896}}
:::MLLOG {"namespace": "", "time_ms": 1745995174158, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.327073574066162, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 560, "lr": 0.0003558650656783958}}
:::MLLOG {"namespace": "", "time_ms": 1745995179633, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.325318455696106, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00035460562557501797}}
:::MLLOG {"namespace": "", "time_ms": 1745995185103, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3556249141693115, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 720, "lr": 0.00035318185276842753}}
:::MLLOG {"namespace": "", "time_ms": 1745995190577, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3342952728271484, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.0003515950872637549}}
:::MLLOG {"namespace": "", "time_ms": 1745995196051, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3131992816925049, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 880, "lr": 0.00034984682246915285}}
:::MLLOG {"namespace": "", "time_ms": 1745995201523, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3524848222732544, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.000347938703790253}}
:::MLLOG {"namespace": "", "time_ms": 1745995207003, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3454411029815674, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1040, "lr": 0.00034587252708156757}}
:::MLLOG {"namespace": "", "time_ms": 1745995212480, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3583691120147705, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.00034365023695629403}}
:::MLLOG {"namespace": "", "time_ms": 1745995217954, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2381738424301147, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1200, "lr": 0.00034127392495611334}}
:::MLLOG {"namespace": "", "time_ms": 1745995223440, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3693029880523682, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003387458275827039}}
:::MLLOG {"namespace": "", "time_ms": 1745995228931, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2952649593353271, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1360, "lr": 0.0003360683241928247}}
:::MLLOG {"namespace": "", "time_ms": 1745995234407, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2450069189071655, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.00033324393475894776}}
:::MLLOG {"namespace": "", "time_ms": 1745995239898, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2112501859664917, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1520, "lr": 0.0003302753174975484}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
:::MLLOG {"namespace": "", "time_ms": 1745995250750, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.629044877407978}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745995250750, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745995250750, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1536}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745995260467, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9398173689842224, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745995260467, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745995260467, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1745995264863, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3423432111740112, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003271652663672851}}
:::MLLOG {"namespace": "", "time_ms": 1745995270350, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2868493795394897, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1680, "lr": 0.00032391670843942295}}
:::MLLOG {"namespace": "", "time_ms": 1745995275842, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2428803443908691, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.000320532701142977}}
:::MLLOG {"namespace": "", "time_ms": 1745995281338, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3114945888519287, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1840, "lr": 0.00031701642938716714}}
:::MLLOG {"namespace": "", "time_ms": 1745995286822, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.323333740234375, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003133712025638927}}
:::MLLOG {"namespace": "", "time_ms": 1745995286828, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.576614695524249}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745995286828, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745995286828, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745995295435, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9335823059082031, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745995295436, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745995295436, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1745995300902, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2697432041168213, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2000, "lr": 0.0003096004514330487}}
:::MLLOG {"namespace": "", "time_ms": 1745995306381, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3712304830551147, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00030570772489361514}}
:::MLLOG {"namespace": "", "time_ms": 1745995311860, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2733627557754517, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2160, "lr": 0.0003016966866435569}}
:::MLLOG {"namespace": "", "time_ms": 1745995317351, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3105380535125732, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0002975711117316798}}
:::MLLOG {"namespace": "", "time_ms": 1745995321747, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.603982594069157}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745995321747, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745995321747, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745995330560, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9307425618171692, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745995330560, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745995330560, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1745995331651, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3039501905441284, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2320, "lr": 0.0002933348830046869}}
:::MLLOG {"namespace": "", "time_ms": 1745995337129, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2715787887573242, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.0002889919874527786}}
:::MLLOG {"namespace": "", "time_ms": 1745995342623, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2886046171188354, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2480, "lr": 0.0002845465124572376}}
:::MLLOG {"namespace": "", "time_ms": 1745995348106, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2623417377471924, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0002800026419435284}}
:::MLLOG {"namespace": "", "time_ms": 1745995353586, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3075370788574219, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2640, "lr": 0.0002753646524435331}}
:::MLLOG {"namespace": "", "time_ms": 1745995356878, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.600840826275228}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745995356878, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745995356878, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745995365437, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9286192059516907, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745995365437, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745995365437, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1745995367627, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2805393934249878, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.0002706369090706292}}
:::MLLOG {"namespace": "", "time_ms": 1745995373104, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.354367971420288, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2800, "lr": 0.00026582386141139804}}
:::MLLOG {"namespace": "", "time_ms": 1745995378580, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2762281894683838, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.0002609300393378292}}
:::MLLOG {"namespace": "", "time_ms": 1745995384062, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3155685663223267, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2960, "lr": 0.000255960048743964}}
:::MLLOG {"namespace": "", "time_ms": 1745995389546, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3583037853240967, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.00025091856721098867}}
:::MLLOG {"namespace": "", "time_ms": 1745995391742, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.607583036959069}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745995391743, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745995391743, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1745995400436, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9248459935188293, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745995400436, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1745995400436, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9248459935188293, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 321, "samples_count": 3072, "status": "success"}}
[rank36]:[W430 14:43:24.838953239 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank60]:[W430 14:43:24.300507690 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W430 14:43:24.839991322 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank43]:[W430 14:43:24.394054959 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank25]:[W430 14:43:24.420924698 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank30]:[W430 14:43:24.432189589 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank58]:[W430 14:43:24.536924187 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank23]:[W430 14:43:24.434255976 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank47]:[W430 14:43:24.482006212 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank22]:[W430 14:43:24.472093336 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W430 14:43:24.091180467 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank17]:[W430 14:43:25.492320992 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W430 14:43:25.121444730 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W430 14:43:25.123570691 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W430 14:43:25.132592710 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank59]:[W430 14:43:25.650903819 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank56]:[W430 14:43:25.653044870 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W430 14:43:25.148044533 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank62]:[W430 14:43:25.682273273 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank57]:[W430 14:43:25.682715505 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank45]:[W430 14:43:25.618483627 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank53]:[W430 14:43:25.724688688 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank42]:[W430 14:43:25.645193387 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank40]:[W430 14:43:25.650545502 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank32]:[W430 14:43:25.322661344 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank27]:[W430 14:43:25.682849254 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank11]:[W430 14:43:25.748950757 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank21]:[W430 14:43:25.692775133 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank20]:[W430 14:43:25.692940128 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank39]:[W430 14:43:25.406165241 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank55]:[W430 14:43:25.840197279 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank26]:[W430 14:43:25.750239539 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank28]:[W430 14:43:25.754309355 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank46]:[W430 14:43:25.760285366 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank12]:[W430 14:43:25.820575543 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank16]:[W430 14:43:25.744804686 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank24]:[W430 14:43:25.756130543 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank52]:[W430 14:43:25.865739310 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank31]:[W430 14:43:25.768806966 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank10]:[W430 14:43:25.836323553 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank29]:[W430 14:43:25.774227265 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank13]:[W430 14:43:25.840575205 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank37]:[W430 14:43:25.462377514 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank44]:[W430 14:43:25.790332486 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank61]:[W430 14:43:25.883272799 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank14]:[W430 14:43:25.851309835 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W430 14:43:25.374599237 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank63]:[W430 14:43:25.888134354 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank15]:[W430 14:43:25.855485931 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank51]:[W430 14:43:25.894611846 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank54]:[W430 14:43:25.896678359 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank38]:[W430 14:43:25.473430240 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank35]:[W430 14:43:25.481669612 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank9]:[W430 14:43:25.871446003 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank8]:[W430 14:43:25.872415553 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W430 14:43:25.398776224 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank41]:[W430 14:43:25.851445823 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank33]:[W430 14:43:25.523858564 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank19]:[W430 14:43:25.838056197 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank49]:[W430 14:43:25.967472800 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank48]:[W430 14:43:25.978319614 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank50]:[W430 14:43:25.992453358 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank34]:[W430 14:43:25.629271863 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank18]:[W430 14:43:25.991218615 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ENDING TIMING RUN AT 2025-04-30 02:43:35 PM
RESULT,LLM_FINETUNING,554,nvidia,2025-04-30 02:34:21 PM
