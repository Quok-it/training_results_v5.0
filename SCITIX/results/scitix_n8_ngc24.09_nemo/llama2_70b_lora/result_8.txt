+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-0
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-0 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 1 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-4
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-4 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 5 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-6
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-6 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 7 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-3
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-3 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 4 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-2
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-2 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 3 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-5
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-5 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 6 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-7
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-7 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 8 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-1
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-1 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1170538496" -mca ess_base_vpid 2 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1170538496.0;tcp://172.21.154.78:54885" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1170538496.0;tcp://172.21.154.78:54885" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
mpirun2pytorch: MASTER_ADDR=172.21.76.114 MASTER_PORT=29500 GROUP_RANK=0 WORLD_SIZE=64
STARTING TIMING RUN AT 2025-04-30 08:47:40 PM
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 104954
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 104954
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

setting number of microbatches to constant 1
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
[W430 20:47:56.320978769 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:47:56.320988882 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
[W430 20:47:56.536977986 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
[W430 20:47:56.545166569 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
[W430 20:47:56.549403792 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
[W430 20:47:57.883116592 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
[W430 20:47:57.885248831 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
[W430 20:47:57.906507581 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
[W430 20:48:01.125187179 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.732078460 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.153967940 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.162015670 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.766967343 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.767124499 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.190156170 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.209032838 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.800558914 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.889607287 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.298186912 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.916414487 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.229988225 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.922455978 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.924821473 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.350647429 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.930169343 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.260483134 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.261995470 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.263974772 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.342129094 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.275135475 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.385477256 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.353953661 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.355422074 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.396583242 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.367718619 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.401958488 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.422685410 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.564085823 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:01.598760668 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.628800958 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.646080002 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.648930748 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.962649422 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.962954415 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.984852595 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.996925249 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:02.998594925 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:06.952899446 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:06.033778304 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:06.362003645 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:06.025230667 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.580802779 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.113483965 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.145487100 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.679849422 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.263816749 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.609689474 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.624411620 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.679514483 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.692037763 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.785377721 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.757971885 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.012707226 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:48:07.537081010 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
Loading distributed checkpoint directly on the GPU
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 1, achieved: 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (11141120 elements):
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00036, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')

  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 17.3 B | n/a 
---------------------------------------------
11.1 M    Trainable params
17.2 B    Non-trainable params
17.3 B    Total params
69,029.364Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
:::MLLOG {"namespace": "", "time_ms": 1746017424449, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 332}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 333}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "POINT_IN_TIME", "key": "seed", "value": 104954, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 335}}
:::MLLOG {"namespace": "", "time_ms": 1746017424450, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 341}}
:::MLLOG {"namespace": "", "time_ms": 1746017424920, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 346}}
:::MLLOG {"namespace": "", "time_ms": 1746017424942, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 351}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 356}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 360}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 364}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 369}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 370}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00036, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 371}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 372}}
:::MLLOG {"namespace": "", "time_ms": 1746017424943, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 373}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
!!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
!!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
!!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
!!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
!!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
!!! [UB] Number of physical nodes: 8
!!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
!!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
!!! [UB] Create Userbuffers Communicator
!!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
MC initialized succesfully, window size = 549755813888
!!! [UBP2P] Register UBuf 1
!!! [UBP2P] Register UBuf 2
!!! [UBP2P] Register UBuf 3
!!! [UBP2P] Register UBuf 4
!!! [UBP2P] Register UBuf 5
!!! [UB] Register UBuf 6
!!! [UB] Register UBuf 7
!!! [UB] Register UBuf 8
!!! [UB] Register UBuf 9
!!! [UB] Register UBuf 10
:::MLLOG {"namespace": "", "time_ms": 1746017532802, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1746017532802, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1746017532803, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
:::MLLOG {"namespace": "", "time_ms": 1746017538262, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.4085943698883057, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 80, "lr": 0.0003599152951501968}}
:::MLLOG {"namespace": "", "time_ms": 1746017543705, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.533084750175476, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.00035966126032202684}}
:::MLLOG {"namespace": "", "time_ms": 1746017549161, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4305304288864136, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 240, "lr": 0.0003592381346041788}}
:::MLLOG {"namespace": "", "time_ms": 1746017554623, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4447370767593384, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.00035864631622776783}}
:::MLLOG {"namespace": "", "time_ms": 1746017560094, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.390606164932251, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 400, "lr": 0.0003578863621915349}}
:::MLLOG {"namespace": "", "time_ms": 1746017565569, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.337166428565979, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00035695898773761896}}
:::MLLOG {"namespace": "", "time_ms": 1746017571053, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3206034898757935, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 560, "lr": 0.0003558650656783958}}
:::MLLOG {"namespace": "", "time_ms": 1746017576542, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2899904251098633, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00035460562557501797}}
:::MLLOG {"namespace": "", "time_ms": 1746017582031, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2848479747772217, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 720, "lr": 0.00035318185276842753}}
:::MLLOG {"namespace": "", "time_ms": 1746017587526, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3462295532226562, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.0003515950872637549}}
:::MLLOG {"namespace": "", "time_ms": 1746017593018, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4019272327423096, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 880, "lr": 0.00034984682246915285}}
:::MLLOG {"namespace": "", "time_ms": 1746017598511, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2872052192687988, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.000347938703790253}}
:::MLLOG {"namespace": "", "time_ms": 1746017604002, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2449238300323486, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1040, "lr": 0.00034587252708156757}}
:::MLLOG {"namespace": "", "time_ms": 1746017609472, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3197133541107178, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.00034365023695629403}}
:::MLLOG {"namespace": "", "time_ms": 1746017614945, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3237489461898804, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1200, "lr": 0.00034127392495611334}}
:::MLLOG {"namespace": "", "time_ms": 1746017620421, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3571710586547852, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003387458275827039}}
:::MLLOG {"namespace": "", "time_ms": 1746017625904, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3249993324279785, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1360, "lr": 0.0003360683241928247}}
:::MLLOG {"namespace": "", "time_ms": 1746017631375, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2119003534317017, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.00033324393475894776}}
:::MLLOG {"namespace": "", "time_ms": 1746017636841, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3693045377731323, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1520, "lr": 0.0003302753174975484}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
:::MLLOG {"namespace": "", "time_ms": 1746017646985, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.616391122549224}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746017646985, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746017646985, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1536}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746017657492, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9372619390487671, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746017657492, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746017657492, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746017661873, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3616611957550049, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003271652663672851}}
:::MLLOG {"namespace": "", "time_ms": 1746017667346, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3296709060668945, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1680, "lr": 0.00032391670843942295}}
:::MLLOG {"namespace": "", "time_ms": 1746017672817, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2988457679748535, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.000320532701142977}}
:::MLLOG {"namespace": "", "time_ms": 1746017678294, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.28074312210083, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1840, "lr": 0.00031701642938716714}}
:::MLLOG {"namespace": "", "time_ms": 1746017683768, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3506062030792236, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003133712025638927}}
:::MLLOG {"namespace": "", "time_ms": 1746017683774, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.620538267935068}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746017683774, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746017683775, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746017692551, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9339834451675415, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746017692552, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746017692552, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746017698014, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2606110572814941, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2000, "lr": 0.0003096004514330487}}
:::MLLOG {"namespace": "", "time_ms": 1746017703488, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.287837028503418, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00030570772489361514}}
:::MLLOG {"namespace": "", "time_ms": 1746017708964, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2674466371536255, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2160, "lr": 0.0003016966866435569}}
:::MLLOG {"namespace": "", "time_ms": 1746017714446, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3231600522994995, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0002975711117316798}}
:::MLLOG {"namespace": "", "time_ms": 1746017718844, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.61495608333889}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746017718844, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746017718844, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746017727515, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9280033707618713, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746017727515, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746017727515, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746017728603, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2318446636199951, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2320, "lr": 0.0002933348830046869}}
:::MLLOG {"namespace": "", "time_ms": 1746017734080, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3892520666122437, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.0002889919874527786}}
:::MLLOG {"namespace": "", "time_ms": 1746017739569, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3312983512878418, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2480, "lr": 0.0002845465124572376}}
:::MLLOG {"namespace": "", "time_ms": 1746017745048, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2968615293502808, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0002800026419435284}}
:::MLLOG {"namespace": "", "time_ms": 1746017750525, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.355289101600647, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2640, "lr": 0.0002753646524435331}}
:::MLLOG {"namespace": "", "time_ms": 1746017753819, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.608413763023254}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746017753819, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746017753819, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746017762564, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9243943691253662, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746017762564, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746017762564, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9243943691253662, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 321, "samples_count": 2688, "status": "success"}}
[rank31]:[W430 20:56:06.443690892 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W430 20:56:06.034167716 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank15]:[W430 20:56:06.570315365 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank13]:[W430 20:56:07.590940970 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank47]:[W430 20:56:07.540403585 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank35]:[W430 20:56:07.236303766 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank58]:[W430 20:56:07.676009048 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank23]:[W430 20:56:07.584325051 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank63]:[W430 20:56:07.703586248 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank41]:[W430 20:56:07.612096716 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank34]:[W430 20:56:07.295440884 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank56]:[W430 20:56:07.725567924 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank44]:[W430 20:56:07.651817176 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank24]:[W430 20:56:07.647401974 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank42]:[W430 20:56:07.663763466 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank43]:[W430 20:56:07.665519600 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank46]:[W430 20:56:07.666860014 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W430 20:56:07.259369709 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank59]:[W430 20:56:07.809219471 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank55]:[W430 20:56:07.817238577 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank54]:[W430 20:56:07.843278586 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank25]:[W430 20:56:07.754541499 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank14]:[W430 20:56:07.848621930 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank12]:[W430 20:56:07.853251820 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank11]:[W430 20:56:07.857081538 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank32]:[W430 20:56:07.495976731 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank21]:[W430 20:56:07.811077310 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank16]:[W430 20:56:07.817720284 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank10]:[W430 20:56:07.895382329 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W430 20:56:07.446789470 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W430 20:56:07.451839183 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank28]:[W430 20:56:07.868029678 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank8]:[W430 20:56:07.934855781 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank37]:[W430 20:56:07.560126488 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank33]:[W430 20:56:07.563013148 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank38]:[W430 20:56:07.571240178 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank60]:[W430 20:56:07.006979669 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank18]:[W430 20:56:07.899005486 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank61]:[W430 20:56:07.011418301 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank62]:[W430 20:56:07.012186257 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank27]:[W430 20:56:07.942218506 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank26]:[W430 20:56:07.950344998 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank40]:[W430 20:56:07.965070035 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank52]:[W430 20:56:07.080185601 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank22]:[W430 20:56:07.993569391 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank48]:[W430 20:56:07.141202454 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank50]:[W430 20:56:07.141984915 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank53]:[W430 20:56:07.164671870 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank49]:[W430 20:56:07.177858066 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank9]:[W430 20:56:07.143036437 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank57]:[W430 20:56:07.185671659 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank51]:[W430 20:56:07.204228351 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank45]:[W430 20:56:07.119685602 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank30]:[W430 20:56:07.115544843 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank29]:[W430 20:56:07.124303784 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank20]:[W430 20:56:07.139180166 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank36]:[W430 20:56:07.837911027 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank17]:[W430 20:56:07.156695629 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank39]:[W430 20:56:07.846044174 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W430 20:56:07.871662175 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W430 20:56:07.877416898 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W430 20:56:07.880494921 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W430 20:56:07.914339147 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank19]:[W430 20:56:07.373963714 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ENDING TIMING RUN AT 2025-04-30 08:56:22 PM
RESULT,LLM_FINETUNING,522,nvidia,2025-04-30 08:47:40 PM
