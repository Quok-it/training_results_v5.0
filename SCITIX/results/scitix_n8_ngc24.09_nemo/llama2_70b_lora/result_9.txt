+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-5
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-5 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 6 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-2
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-2 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 3 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-7
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-7 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 8 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-3
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-3 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 4 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-6
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-6 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 7 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-1
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-1 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 2 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-0
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-0 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 1 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
+ POD_NAME=llama2-70b-lora-n8-mpijob-worker-4
+ shift
+ /opt/kube/kubectl exec llama2-70b-lora-n8-mpijob-worker-4 -- /bin/sh -c '   OPAL_PREFIX=/opt/hpcx/ompi ; export OPAL_PREFIX;    PATH=/opt/hpcx/ompi/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH:-} ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${DYLD_LIBRARY_PATH:-} ; export DYLD_LIBRARY_PATH ;   /opt/hpcx/ompi/bin/orted -mca ess "env" -mca ess_base_jobid "1115029504" -mca ess_base_vpid 5 -mca ess_base_num_procs "9" -mca orte_node_regex "llama[1:2]-70b-lora-n8-mpijob-launcher,llama[1:2]-70b-lora-n8-mpijob-worker-0,llama[1:2]-70b-lora-n8-mpijob-worker-1,llama[1:2]-70b-lora-n8-mpijob-worker-2,llama[1:2]-70b-lora-n8-mpijob-worker-3,llama[1:2]-70b-lora-n8-mpijob-worker-4,llama[1:2]-70b-lora-n8-mpijob-worker-5,llama[1:2]-70b-lora-n8-mpijob-worker-6,llama[1:2]-70b-lora-n8-mpijob-worker-7@0(9)" -mca orte_hnp_uri "1115029504.0;tcp://172.21.154.78:42629" -mca plm "rsh" --tree-spawn -mca routed "radix" -mca orte_parent_uri "1115029504.0;tcp://172.21.154.78:42629" -mca orte_default_hostfile "/etc/mpi/hostfile" -mca coll_hcoll_enable "0" -mca plm_rsh_agent "/etc/mpi/kubexec.sh" -mca hwloc_base_binding_policy "none" -mca pmix "^s1,s2,cray,isolated"'
mpirun2pytorch: MASTER_ADDR=172.21.76.114 MASTER_PORT=29500 GROUP_RANK=0 WORLD_SIZE=64
STARTING TIMING RUN AT 2025-04-30 08:58:32 PM
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=48
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********


************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false


model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false



************** Experiment configuration ***********

model:
  ub_tp_comm_overlap_cfg:
    qkv_fprop:
      method: ring_exchange
      aggregate: 0
    fc1_fprop:
      method: ring_exchange
      aggregate: 0
    proj_dgrad:
      method: ring_exchange
      aggregate: 0
    fc2_dgrad:
      method: ring_exchange
      aggregate: 0
    proj_fprop:
      method: pipeline
      num_sm: 32
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 16
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: ring_exchange
      num_sm: 1
      cga_size: 2
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 0
  mcore_gpt: true
  seed: 93562
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_parallel_size: 2
  cpu_offloading: false
  dist_ckpt_load_strictness: log_all
  global_batch_size: 8
  micro_batch_size: 1
  max_position_embeddings: 8192
  encoder_seq_length: 8192
  restore_from_path: /data/dataset/dataset_llama2-70b-lora/model
  resume_from_checkpoint: null
  save_nemo_on_validation_end: false
  sync_batch_comm: false
  megatron_amp_O2: true
  sequence_parallel: 1
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  activations_checkpoint_layers_per_pipeline: null
  answer_only_loss: true
  gradient_as_bucket_view: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  bias_activation_fusion: true
  bias_dropout_add_fusion: false
  transformer_engine: true
  fp8: true
  fp8_params: true
  fp8_hybrid: true
  fp8_amax_history_len: 32
  fp8_amax_compute_algo: max
  reduce_amax: false
  fp8_e4m3: false
  fp8_interval: 1
  fp8_margin: 0
  fp8_dot_product_attention: 0
  activation_func_fp8_input_store: 0
  apply_rope_fusion: true
  disable_parameter_transpose_cache: true
  ub_tp_comm_overlap: 1
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  tp_comm_overlap_rs_dgrad: false
  tp_comm_overlap_disable_qkv: true
  batch_p2p_comm: 'False'
  virtual_pipeline_model_parallel_size: 1
  sharp: true
  nccl_communicator_config_path: null
  peft:
    peft_scheme: lora
    restore_from_path: null
    lora_tuning:
      adapter_dim: 16
      alpha: 32
      adapter_dropout: 0.1
      dropout_position: pre
      target_modules:
      - attention
      column_init_method: kaiming
      row_init_method: zero
      layer_selection: null
      weight_tying: false
      position_embedding_strategy: null
      a2a_experimental: 1
  data:
    multiprocessing_context: spawn
    pin_memory: true
    sample_weight: constant
    validation_drop_last: false
    train_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/train.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/train
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: true
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: true
      concat_sampling_probabilities:
      - 1.0
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      truncation_field: input
      prompt_template: '{input} {output}'
      truncation_method: right
      seed: 93562
    validation_ds:
      file_names:
      - /data/dataset/dataset_llama2-70b-lora/gov_report/validation.npy
      packed_sequence: true
      packed_sequence_return_cu_seqlen: false
      index_mapping_dir: /results/data_index/val
      names: null
      global_batch_size: 8
      micro_batch_size: 1
      shuffle: false
      num_workers: 1
      memmap_workers: 2
      pin_memory: true
      max_seq_length: 8192
      min_seq_length: 1
      drop_last: false
      label_key: output
      add_eos: true
      add_sep: false
      add_bos: false
      write_predictions_to_file: false
      output_file_path_prefix: null
      truncation_field: input
      prompt_template: '{input} {output}'
      tokens_to_generate: 32
      truncation_method: right
      metric:
        name: loss
        average: null
        num_classes: null
  optim:
    name: mcore_distributed_optim
    overlap_grad_sync: true
    overlap_param_sync: true
    delay_grad_reduce: true
    delay_param_gather: true
    average_in_collective: false
    lr: 0.00036
    min_lr: 0
    weight_decay: 0.0001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    amsgrad: false
    sched:
      name: CosineAnnealing
      warmup_ratio: 0.0
      min_lr: 0.0
      constant_steps: 0
      monitor: val_loss
      reduce_on_plateau: false
  enable_cuda_graph: 1
  enable_cg_fp8_weight_caching: true
  custom:
    warmup: true
    warmup_train_steps: 5
    warmup_validation_steps: 5
    reset_fp8_stats_after_warmup: 1
name: megatron_gpt_peft_lora_tuning
trainer:
  devices: 8
  num_nodes: 8
  accelerator: gpu
  precision: bf16-mixed
  max_steps: 1024
  val_check_interval: 192
  check_val_every_n_epoch: null
  log_every_n_steps: 0
  gradient_clip_val: 0.3
  gradient_clip_algorithm: norm
  num_sanity_val_steps: 0
  max_epochs: 1000
  limit_val_batches: 1.0
  limit_train_batches: 1.0
  limit_test_batches: 0
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  enable_progress_bar: false
exp_manager:
  log_tflops_per_sec_per_gpu: false
  explicit_log_dir: null
  exp_dir: /results
  create_wandb_logger: false
  resume_if_exists: false
  resume_ignore_no_checkpoint: true
  create_checkpoint_callback: false
  log_global_rank_0_only: true
  create_early_stopping_callback: false
  create_tensorboard_logger: false

setting number of microbatches to constant 1
Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
[W430 20:58:48.765332557 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
[W430 20:58:53.633224270 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.668462036 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.705265506 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.721314804 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.722106099 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.823472590 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.846520946 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.880879340 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.884807820 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.889959243 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.961042287 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.974007945 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.925407003 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.927896487 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.614396691 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.046243740 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.049874177 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.050527989 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.665813091 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.998137252 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.008505849 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.682314093 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.013032622 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.688875009 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.689576553 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.162040044 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.050908812 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.052017896 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.165057831 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.101446629 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.114186510 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.226235887 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.117328817 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.234318360 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.245410139 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.785312819 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.796445294 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.801837489 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.819363857 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.889427534 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:53.892390504 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.949859729 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.170337387 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.242858125 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.292966413 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.400887889 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.330162366 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.354444444 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.434852948 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.377088640 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.073784782 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.549991452 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:58.459665640 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.189994762 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.219940212 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.548553719 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.624640382 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.745979766 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.636250249 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.747367243 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.868756350 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.777943797 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W430 20:58:59.383314891 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
Loading distributed checkpoint with TensorStoreLoadShardedStrategy
Loading distributed checkpoint directly on the GPU
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
make: Nothing to be done for 'default'.
make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 1, achieved: 1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (11141120 elements):
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00036, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')

  | Name         | Type | Params | Mode
---------------------------------------------
  | other params | n/a  | 17.3 B | n/a 
---------------------------------------------
11.1 M    Trainable params
17.2 B    Non-trainable params
17.3 B    Total params
69,029.364Total estimated model params size (MB)
0         Modules in train mode
0         Modules in eval mode
:::MLLOG {"namespace": "", "time_ms": 1746018077611, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 332}}
:::MLLOG {"namespace": "", "time_ms": 1746018077611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 333}}
:::MLLOG {"namespace": "", "time_ms": 1746018077611, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746018077612, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746018077612, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746018077612, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746018077612, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 334}}
:::MLLOG {"namespace": "", "time_ms": 1746018077612, "event_type": "POINT_IN_TIME", "key": "seed", "value": 93562, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 335}}
:::MLLOG {"namespace": "", "time_ms": 1746018077612, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 341}}
:::MLLOG {"namespace": "", "time_ms": 1746018078083, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 346}}
:::MLLOG {"namespace": "", "time_ms": 1746018078105, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 351}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 356}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 360}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 364}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 369}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 370}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00036, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 371}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 372}}
:::MLLOG {"namespace": "", "time_ms": 1746018078106, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 373}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
!!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
!!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
!!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
!!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
!!! [UB] Number of physical nodes: 8
!!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
!!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
!!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
!!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
!!! [UB] Create Userbuffers Communicator
UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
MC initialized succesfully, window size = 549755813888
!!! [UBP2P] Register UBuf 1
!!! [UBP2P] Register UBuf 2
!!! [UBP2P] Register UBuf 3
!!! [UBP2P] Register UBuf 4
!!! [UBP2P] Register UBuf 5
!!! [UB] Register UBuf 6
!!! [UB] Register UBuf 7
!!! [UB] Register UBuf 8
!!! [UB] Register UBuf 9
!!! [UB] Register UBuf 10
:::MLLOG {"namespace": "", "time_ms": 1746018184811, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1746018184811, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 271}}
:::MLLOG {"namespace": "", "time_ms": 1746018184811, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
:::MLLOG {"namespace": "", "time_ms": 1746018190257, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.379329204559326, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 80, "lr": 0.0003599152951501968}}
:::MLLOG {"namespace": "", "time_ms": 1746018195683, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3943895101547241, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.00035966126032202684}}
:::MLLOG {"namespace": "", "time_ms": 1746018201133, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4142481088638306, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 240, "lr": 0.0003592381346041788}}
:::MLLOG {"namespace": "", "time_ms": 1746018206590, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.373221755027771, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.00035864631622776783}}
:::MLLOG {"namespace": "", "time_ms": 1746018212073, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.334723949432373, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 400, "lr": 0.0003578863621915349}}
:::MLLOG {"namespace": "", "time_ms": 1746018217563, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2660102844238281, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00035695898773761896}}
:::MLLOG {"namespace": "", "time_ms": 1746018223057, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3742871284484863, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 560, "lr": 0.0003558650656783958}}
:::MLLOG {"namespace": "", "time_ms": 1746018228555, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2726426124572754, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00035460562557501797}}
:::MLLOG {"namespace": "", "time_ms": 1746018234049, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2514967918395996, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 720, "lr": 0.00035318185276842753}}
:::MLLOG {"namespace": "", "time_ms": 1746018239542, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2774091958999634, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.0003515950872637549}}
:::MLLOG {"namespace": "", "time_ms": 1746018245041, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3314794301986694, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 880, "lr": 0.00034984682246915285}}
:::MLLOG {"namespace": "", "time_ms": 1746018250536, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4172320365905762, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.000347938703790253}}
:::MLLOG {"namespace": "", "time_ms": 1746018256025, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3476276397705078, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1040, "lr": 0.00034587252708156757}}
:::MLLOG {"namespace": "", "time_ms": 1746018261511, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.247118353843689, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.00034365023695629403}}
:::MLLOG {"namespace": "", "time_ms": 1746018267016, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3569154739379883, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1200, "lr": 0.00034127392495611334}}
:::MLLOG {"namespace": "", "time_ms": 1746018272520, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3276867866516113, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003387458275827039}}
:::MLLOG {"namespace": "", "time_ms": 1746018278021, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3536945581436157, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1360, "lr": 0.0003360683241928247}}
:::MLLOG {"namespace": "", "time_ms": 1746018283503, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3151450157165527, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.00033324393475894776}}
:::MLLOG {"namespace": "", "time_ms": 1746018288976, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2403665781021118, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1520, "lr": 0.0003302753174975484}}
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
:::MLLOG {"namespace": "", "time_ms": 1746018299792, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.59905100981708}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746018299792, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746018299792, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1536}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746018309479, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9398099780082703, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746018309479, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746018309479, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1536}}
:::MLLOG {"namespace": "", "time_ms": 1746018313879, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2911458015441895, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003271652663672851}}
:::MLLOG {"namespace": "", "time_ms": 1746018319372, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3216166496276855, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1680, "lr": 0.00032391670843942295}}
:::MLLOG {"namespace": "", "time_ms": 1746018324868, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3399784564971924, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.000320532701142977}}
:::MLLOG {"namespace": "", "time_ms": 1746018330372, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2795677185058594, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1840, "lr": 0.00031701642938716714}}
:::MLLOG {"namespace": "", "time_ms": 1746018335876, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2822257280349731, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003133712025638927}}
:::MLLOG {"namespace": "", "time_ms": 1746018335882, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.553652039587934}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746018335882, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746018335882, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746018344516, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9360796213150024, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746018344516, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746018344516, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
:::MLLOG {"namespace": "", "time_ms": 1746018349992, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.298077940940857, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2000, "lr": 0.0003096004514330487}}
:::MLLOG {"namespace": "", "time_ms": 1746018355470, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2295016050338745, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00030570772489361514}}
:::MLLOG {"namespace": "", "time_ms": 1746018360951, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2997901439666748, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2160, "lr": 0.0003016966866435569}}
:::MLLOG {"namespace": "", "time_ms": 1746018366455, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3369759321212769, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0002975711117316798}}
:::MLLOG {"namespace": "", "time_ms": 1746018370845, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.594157997319684}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746018370845, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746018370845, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746018379563, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9299207329750061, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746018379563, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746018379564, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
:::MLLOG {"namespace": "", "time_ms": 1746018380654, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3896706104278564, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2320, "lr": 0.0002933348830046869}}
:::MLLOG {"namespace": "", "time_ms": 1746018386135, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3210399150848389, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.0002889919874527786}}
:::MLLOG {"namespace": "", "time_ms": 1746018391624, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.238504409790039, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2480, "lr": 0.0002845465124572376}}
:::MLLOG {"namespace": "", "time_ms": 1746018397115, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2635517120361328, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0002800026419435284}}
:::MLLOG {"namespace": "", "time_ms": 1746018402594, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2874064445495605, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2640, "lr": 0.0002753646524435331}}
:::MLLOG {"namespace": "", "time_ms": 1746018405890, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.595719933675456}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746018405890, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746018405890, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746018414644, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9264274835586548, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746018414644, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746018414645, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
:::MLLOG {"namespace": "", "time_ms": 1746018416828, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2160810232162476, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.0002706369090706292}}
:::MLLOG {"namespace": "", "time_ms": 1746018422302, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2834020853042603, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2800, "lr": 0.00026582386141139804}}
:::MLLOG {"namespace": "", "time_ms": 1746018427782, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2764806747436523, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.0002609300393378292}}
:::MLLOG {"namespace": "", "time_ms": 1746018433262, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2212634086608887, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 2960, "lr": 0.000255960048743964}}
:::MLLOG {"namespace": "", "time_ms": 1746018438750, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2306914329528809, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.00025091856721098867}}
:::MLLOG {"namespace": "", "time_ms": 1746018440955, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.604766029027891}, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 287, "step": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1746018440955, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1746018440955, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
setting number of microbatches to constant 1
setting number of microbatches to constant 1
:::MLLOG {"namespace": "", "time_ms": 1746018449609, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9247505068778992, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1746018449609, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
:::MLLOG {"namespace": "", "time_ms": 1746018449609, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9247505068778992, "metadata": {"file": "/data/mlperf_training/SCITIX/benchmarks/llama2_70b_lora/implementations/scitix_n8_ngc24.09_nemo/custom_callbacks.py", "lineno": 321, "samples_count": 3072, "status": "success"}}
[rank18]:[W430 21:07:33.467455613 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank39]:[W430 21:07:34.259768511 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank33]:[W430 21:07:34.270389155 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank9]:[W430 21:07:34.678574151 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank63]:[W430 21:07:34.721113768 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank61]:[W430 21:07:34.754408723 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W430 21:07:34.254244999 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank40]:[W430 21:07:34.706527311 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank31]:[W430 21:07:34.713255263 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank55]:[W430 21:07:34.824603235 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank43]:[W430 21:07:34.731169598 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank27]:[W430 21:07:34.745835845 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank29]:[W430 21:07:34.746410691 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank53]:[W430 21:07:34.868401468 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank24]:[W430 21:07:34.781562327 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank13]:[W430 21:07:34.893106408 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank34]:[W430 21:07:34.510694194 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank16]:[W430 21:07:34.831258376 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank15]:[W430 21:07:34.909950699 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank20]:[W430 21:07:34.845806089 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W430 21:07:34.454587584 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W430 21:07:34.454737388 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W430 21:07:34.482639053 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank14]:[W430 21:07:34.959703460 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank11]:[W430 21:07:34.962989937 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank12]:[W430 21:07:34.983180514 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank47]:[W430 21:07:34.940459342 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank52]:[W430 21:07:34.048631084 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank42]:[W430 21:07:34.969415462 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank10]:[W430 21:07:34.029961375 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank44]:[W430 21:07:34.973312198 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank45]:[W430 21:07:34.973760223 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank26]:[W430 21:07:34.004360993 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank30]:[W430 21:07:34.004749125 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank50]:[W430 21:07:34.115672619 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank25]:[W430 21:07:34.017714957 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank22]:[W430 21:07:34.007970867 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank59]:[W430 21:07:34.136347242 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank51]:[W430 21:07:34.138994021 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank54]:[W430 21:07:34.146220938 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank32]:[W430 21:07:34.751172979 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank8]:[W430 21:07:34.149466552 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W430 21:07:34.687157167 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank28]:[W430 21:07:34.098595997 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank36]:[W430 21:07:34.786416695 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W430 21:07:34.698029975 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W430 21:07:34.724743670 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W430 21:07:34.743113071 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank58]:[W430 21:07:34.264541758 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank57]:[W430 21:07:34.277447016 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank46]:[W430 21:07:34.191753570 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank38]:[W430 21:07:34.864222052 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank62]:[W430 21:07:34.305148821 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank41]:[W430 21:07:34.214531181 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank35]:[W430 21:07:34.926877533 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank49]:[W430 21:07:34.380677115 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank48]:[W430 21:07:34.386361730 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank56]:[W430 21:07:34.444643044 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank23]:[W430 21:07:34.336168850 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank17]:[W430 21:07:34.339082694 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank19]:[W430 21:07:34.349114539 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank60]:[W430 21:07:34.509455423 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank37]:[W430 21:07:34.088781632 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank21]:[W430 21:07:34.470891006 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ENDING TIMING RUN AT 2025-04-30 09:07:45 PM
RESULT,LLM_FINETUNING,553,nvidia,2025-04-30 08:58:32 PM
