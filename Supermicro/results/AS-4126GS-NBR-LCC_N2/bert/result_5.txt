+ echo 'Beginning trial 5 of 10'
Beginning trial 5 of 10
+ echo ':::DLPAL nvcr.io/nvdlfwea/mlperftv50/bert-amd:20250423 37 2 b200-bade-[1-2] '\''unknown'\'' DGXB200_2x8x48x1_pack'
:::DLPAL nvcr.io/nvdlfwea/mlperftv50/bert-amd:20250423 37 2 b200-bade-[1-2] 'unknown' DGXB200_2x8x48x1_pack
++ srun -N1 -n1 --container-name=language_model_37 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"2","host_processors_per_node":"2","host_processor_model_name":"AMD EPYC 9755 128-Core Processor","host_processor_core_count":"128","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.2 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-138-generic","nvidia_kernel_driver":"570.133.20"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"2","host_processors_per_node":"2","host_processor_model_name":"AMD EPYC 9755 128-Core Processor","host_processor_core_count":"128","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.2 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-138-generic","nvidia_kernel_driver":"570.133.20"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=language_model_37 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID 8c262b08b227bd43cefaeedd061fe71d67271031 
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && echo '\''Super@123'\'' | sudo -S /sbin/sysctl vm.drop_caches=3'
Clearing cache on b200-bade-2
Clearing cache on b200-bade-1
vm.drop_caches = 3
[sudo] password for smci: vm.drop_caches = 3
[sudo] password for smci: + srun --ntasks-per-node=1 --container-name=language_model_37 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1746079603105, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746079603314, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1746079604'
RUNANDTIME_START 1746079604
+ srun -l --mpi=none --ntasks-per-node=8 --time=12 --container-name=language_model_37 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/home/smci/mlperf-training-v50/dataset/bert/packed_data:/workspace/data_phase2,/home/smci/mlperf-training-v50/dataset/bert/phase1:/workspace/phase1,/home/smci/mlperf-training-v50/dataset/bert/hdf5/eval_varlength:/workspace/evaldata,/home/smci/mlperf-training-v50/logs/bert:/results --container-workdir=/workspace/bert --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
 2: Run vars: id 37 gpus 8 mparams ''
 5: Run vars: id 37 gpus 8 mparams ''
 7: Run vars: id 37 gpus 8 mparams ''
 6: Run vars: id 37 gpus 8 mparams ''
 0: Run vars: id 37 gpus 8 mparams ''
 1: Run vars: id 37 gpus 8 mparams ''
 3: Run vars: id 37 gpus 8 mparams ''
 4: Run vars: id 37 gpus 8 mparams ''
14: Run vars: id 37 gpus 8 mparams ''
15: Run vars: id 37 gpus 8 mparams ''
12: Run vars: id 37 gpus 8 mparams ''
 8: Run vars: id 37 gpus 8 mparams ''
13: Run vars: id 37 gpus 8 mparams ''
 9: Run vars: id 37 gpus 8 mparams ''
10: Run vars: id 37 gpus 8 mparams ''
11: Run vars: id 37 gpus 8 mparams ''
13: [W501 06:06:46.677295150 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W501 06:06:46.677346309 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W501 06:06:46.677383129 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W501 06:06:46.677420598 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [W501 06:06:46.677462538 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W501 06:06:46.677510467 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W501 06:06:46.677547927 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [W501 06:06:46.677615136 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 8:   warnings.warn(msg, DeprecatedFeatureWarning)
 9: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 9:   warnings.warn(msg, DeprecatedFeatureWarning)
10: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
10:   warnings.warn(msg, DeprecatedFeatureWarning)
11: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
11:   warnings.warn(msg, DeprecatedFeatureWarning)
12: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
12:   warnings.warn(msg, DeprecatedFeatureWarning)
13: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
13:   warnings.warn(msg, DeprecatedFeatureWarning)
14: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
14:   warnings.warn(msg, DeprecatedFeatureWarning)
15: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
15:   warnings.warn(msg, DeprecatedFeatureWarning)
 5: [W501 06:06:46.000198618 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [W501 06:06:46.000243827 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W501 06:06:46.000238597 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W501 06:06:46.000302857 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [W501 06:06:46.000376727 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [W501 06:06:46.000475647 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: [W501 06:06:46.000518927 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W501 06:06:46.000717137 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 0:   warnings.warn(msg, DeprecatedFeatureWarning)
 1: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 1:   warnings.warn(msg, DeprecatedFeatureWarning)
 2: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 2:   warnings.warn(msg, DeprecatedFeatureWarning)
 3: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 3:   warnings.warn(msg, DeprecatedFeatureWarning)
 4: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 4:   warnings.warn(msg, DeprecatedFeatureWarning)
 5: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 5:   warnings.warn(msg, DeprecatedFeatureWarning)
 6: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 6:   warnings.warn(msg, DeprecatedFeatureWarning)
 7: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 7:   warnings.warn(msg, DeprecatedFeatureWarning)
 4: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 4:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 4: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 4:   warnings.warn(msg)
 3: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 3:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 3: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 3:   warnings.warn(msg)
 0: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 0:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 0: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 0:   warnings.warn(msg)
 6: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 6:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 1: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 1:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 5: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 5:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 2: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 2:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 7: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 7:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 6: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 6:   warnings.warn(msg)
 1: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 1:   warnings.warn(msg)
 5: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 5:   warnings.warn(msg)
 2: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 2:   warnings.warn(msg)
 7: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 7:   warnings.warn(msg)
10: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
10:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
15: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
15:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 9: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 9:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
11: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
11:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 8: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 8:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
10: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
10:   warnings.warn(msg)
15: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
15:   warnings.warn(msg)
12: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
12:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
14: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
14:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 9: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 9:   warnings.warn(msg)
11: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
11:   warnings.warn(msg)
 8: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 8:   warnings.warn(msg)
12: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
12:   warnings.warn(msg)
14: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
14:   warnings.warn(msg)
13: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
13:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
13: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/NVIDIA/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
13:   warnings.warn(msg)
 4: [W501 06:06:58.952973423 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [W501 06:06:58.957715241 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [W501 06:06:58.958076870 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: [W501 06:06:58.958483440 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [W501 06:06:58.958544390 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W501 06:06:58.958512990 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [W501 06:06:58.958604290 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W501 06:06:58.958607940 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 4:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 5: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 5:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 3: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 3:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 6: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 6:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 1: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 1:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 2: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 2:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 7: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 7:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 4: :::MLLOG {"namespace": "", "time_ms": 1746079618728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 0: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 0:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 5: :::MLLOG {"namespace": "", "time_ms": 1746079618729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 6: :::MLLOG {"namespace": "", "time_ms": 1746079618729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 3: :::MLLOG {"namespace": "", "time_ms": 1746079618729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 1: :::MLLOG {"namespace": "", "time_ms": 1746079618729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 2: :::MLLOG {"namespace": "", "time_ms": 1746079618729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 7: :::MLLOG {"namespace": "", "time_ms": 1746079618729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079618732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 8: [W501 06:06:59.181213217 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 8:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 8: :::MLLOG {"namespace": "", "time_ms": 1746079619199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
12: [W501 06:06:59.188892121 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [W501 06:06:59.189906711 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 8: device: cuda:0 n_gpu: 16, distributed training: True, 16-bits training: True
12: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
12:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
12: :::MLLOG {"namespace": "", "time_ms": 1746079619202, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
10: [W501 06:06:59.192600945 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
10:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
10: :::MLLOG {"namespace": "", "time_ms": 1746079619205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
14: [W501 06:06:59.195585685 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
14:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
14: :::MLLOG {"namespace": "", "time_ms": 1746079619211, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
11: [W501 06:06:59.202164231 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W501 06:06:59.204889074 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W501 06:06:59.205794095 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
11:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
15: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
15:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
15: :::MLLOG {"namespace": "", "time_ms": 1746079619218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
11: :::MLLOG {"namespace": "", "time_ms": 1746079619219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 9: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 9:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
13: [W501 06:06:59.209337710 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: :::MLLOG {"namespace": "", "time_ms": 1746079619220, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
13: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
13:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
13: :::MLLOG {"namespace": "", "time_ms": 1746079619225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 8: Torch distributed is available.
 8: Torch distributed is initialized.
 4: [W501 06:07:00.433240217 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 4: device: cuda:4 n_gpu: 16, distributed training: True, 16-bits training: True
 4: Torch distributed is available.
 4: Torch distributed is initialized.
 1: [W501 06:07:00.461121825 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 1: device: cuda:1 n_gpu: 16, distributed training: True, 16-bits training: True
 1: Torch distributed is available.
 1: Torch distributed is initialized.
 5: [W501 06:07:00.465604733 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 5: device: cuda:5 n_gpu: 16, distributed training: True, 16-bits training: True
 3: [W501 06:07:00.467893512 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 3: device: cuda:3 n_gpu: 16, distributed training: True, 16-bits training: True
 7: [W501 06:07:00.468236151 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 7: device: cuda:7 n_gpu: 16, distributed training: True, 16-bits training: True
 2: [W501 06:07:00.468733811 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 2: device: cuda:2 n_gpu: 16, distributed training: True, 16-bits training: True
 5: Torch distributed is available.
 5: Torch distributed is initialized.
 6: [W501 06:07:00.469358531 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 6: device: cuda:6 n_gpu: 16, distributed training: True, 16-bits training: True
 3: Torch distributed is available.
 3: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
 2: Torch distributed is available.
 2: Torch distributed is initialized.
 6: Torch distributed is available.
 6: Torch distributed is initialized.
14: [W501 06:07:00.820920983 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
14: device: cuda:6 n_gpu: 16, distributed training: True, 16-bits training: True
10: [W501 06:07:00.826136832 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
10: device: cuda:2 n_gpu: 16, distributed training: True, 16-bits training: True
14: Torch distributed is available.
14: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
 9: [W501 06:07:00.853229476 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 9: device: cuda:1 n_gpu: 16, distributed training: True, 16-bits training: True
 9: Torch distributed is available.
 9: Torch distributed is initialized.
11: [W501 06:07:00.856372875 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
11: device: cuda:3 n_gpu: 16, distributed training: True, 16-bits training: True
15: [W501 06:07:00.857477554 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
15: device: cuda:7 n_gpu: 16, distributed training: True, 16-bits training: True
11: Torch distributed is available.
11: Torch distributed is initialized.
12: [W501 06:07:00.861242987 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
12: device: cuda:4 n_gpu: 16, distributed training: True, 16-bits training: True
13: [W501 06:07:00.862222417 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
15: Torch distributed is available.
15: Torch distributed is initialized.
13: device: cuda:5 n_gpu: 16, distributed training: True, 16-bits training: True
12: Torch distributed is available.
12: Torch distributed is initialized.
 0: [W501 06:07:00.116109481 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 0: device: cuda:0 n_gpu: 16, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620883, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620883, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620883, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620883, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620883, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620884, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2139, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1399}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620884, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 1536, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1401}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620884, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 48, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620884, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1405}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620884, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1407}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620884, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 3680.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1409}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079620884, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1411}}
 0: parsed args:
 0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=48, eval_batch_size=16, learning_rate=0.00096, weight_decay_rate=0.1, opt_lamb_beta_1=0.60466, opt_lamb_beta_2=0.85437, max_steps=3680.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=0.0, start_warmup_step=0.0, local_rank=0, seed=2139, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_ch
 0: eckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, eval_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num
 0: _ar_pg=1, dwu_num_ag_pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine=False, use_transformer_engine2=True, n_gpu=16, device=device(type='cuda', index=0))
13: Torch distributed is available.
13: Torch distributed is initialized.
 0: Torch distributed is available.
 0: Torch distributed is initialized.
 8: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 8:   warnings.warn(
14: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
14:   warnings.warn(
13: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
13:   warnings.warn(
12: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
12:   warnings.warn(
 9: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 9:   warnings.warn(
15: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
15:   warnings.warn(
10: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
10:   warnings.warn(
 4: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 4:   warnings.warn(
11: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
11:   warnings.warn(
 2: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 2:   warnings.warn(
 3: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 3:   warnings.warn(
 1: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 1:   warnings.warn(
 0: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 0:   warnings.warn(
 5: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 5:   warnings.warn(
 6: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 6:   warnings.warn(
 7: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 7:   warnings.warn(
14: BertForPreTrainingSegmented(
14:   (bert_model_segment): BertForPreTrainingModelOnly(
14:     (bert): BertModel(
14:       (embeddings): BertEmbeddings(
14:         (word_embeddings): Embedding(30528, 1024)
14:         (position_embeddings): Embedding(512, 1024)
14:         (token_type_embeddings): Embedding(2, 1024)
14:         (LayerNorm): FastLayerNorm()
14:         (dropout): Dropout(p=0.1, inplace=False)
14:       )
14:       (encoder): BertEncoder(
14:         (layer): ModuleList(
14:           (0-23): 24 x BertTransformerLayer2(
14:             (attention): FP8_MHA()
14:             (layernorm_mlp): LayerNormMLP()
14:             (output_LayerNorm): FastLayerNorm()
14:           )
14:         )
14:       )
14:       (pooler): BertPooler(
14:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
14:         (activation): Tanh()
14:       )
14:     )
14:   )
14:   (heads_only_segment): BertForPreTrainingHeadsOnly(
14:     (cls): BertPreTrainingHeads(
14:       (predictions): BertLMPredictionHead(
14:         (transform): BertPredictionHeadTransform(
14:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
14:           (LayerNorm): FastLayerNorm()
14:         )
14:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
14:       )
14:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
14:     )
14:   )
14: )
 8: BertForPreTrainingSegmented(
 8:   (bert_model_segment): BertForPreTrainingModelOnly(
 8:     (bert): BertModel(
 8:       (embeddings): BertEmbeddings(
 8:         (word_embeddings): Embedding(30528, 1024)
 8:         (position_embeddings): Embedding(512, 1024)
 8:         (token_type_embeddings): Embedding(2, 1024)
 8:         (LayerNorm): FastLayerNorm()
 8:         (dropout): Dropout(p=0.1, inplace=False)
 8:       )
 8:       (encoder): BertEncoder(
 8:         (layer): ModuleList(
 8:           (0-23): 24 x BertTransformerLayer2(
 8:             (attention): FP8_MHA()
 8:             (layernorm_mlp): LayerNormMLP()
 8:             (output_LayerNorm): FastLayerNorm()
 8:           )
 8:         )
 8:       )
 8:       (pooler): BertPooler(
 8:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 8:         (activation): Tanh()
 8:       )
 8:     )
 8:   )
 8:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 8:     (cls): BertPreTrainingHeads(
 8:       (predictions): BertLMPredictionHead(
 8:         (transform): BertPredictionHeadTransform(
 8:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 8:           (LayerNorm): FastLayerNorm()
 8:         )
 8:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 8:       )
 8:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 8:     )
 8:   )
 8: )
 9: BertForPreTrainingSegmented(
 9:   (bert_model_segment): BertForPreTrainingModelOnly(
 9:     (bert): BertModel(
 9:       (embeddings): BertEmbeddings(
 9:         (word_embeddings): Embedding(30528, 1024)
 9:         (position_embeddings): Embedding(512, 1024)
 9:         (token_type_embeddings): Embedding(2, 1024)
 9:         (LayerNorm): FastLayerNorm()
 9:         (dropout): Dropout(p=0.1, inplace=False)
 9:       )
 9:       (encoder): BertEncoder(
 9:         (layer): ModuleList(
 9:           (0-23): 24 x BertTransformerLayer2(
 9:             (attention): FP8_MHA()
 9:             (layernorm_mlp): LayerNormMLP()
 9:             (output_LayerNorm): FastLayerNorm()
 9:           )
 9:         )
 9:       )
 9:       (pooler): BertPooler(
 9:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 9:         (activation): Tanh()
 9:       )
 9:     )
 9:   )
 9:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 9:     (cls): BertPreTrainingHeads(
 9:       (predictions): BertLMPredictionHead(
 9:         (transform): BertPredictionHeadTransform(
 9:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 9:           (LayerNorm): FastLayerNorm()
 9:         )
 9:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 9:       )
 9:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 9:     )
 9:   )
 9: )
11: BertForPreTrainingSegmented(
11:   (bert_model_segment): BertForPreTrainingModelOnly(
11:     (bert): BertModel(
11:       (embeddings): BertEmbeddings(
11:         (word_embeddings): Embedding(30528, 1024)
11:         (position_embeddings): Embedding(512, 1024)
11:         (token_type_embeddings): Embedding(2, 1024)
11:         (LayerNorm): FastLayerNorm()
11:         (dropout): Dropout(p=0.1, inplace=False)
11:       )
11:       (encoder): BertEncoder(
11:         (layer): ModuleList(
11:           (0-23): 24 x BertTransformerLayer2(
11:             (attention): FP8_MHA()
11:             (layernorm_mlp): LayerNormMLP()
11:             (output_LayerNorm): FastLayerNorm()
11:           )
11:         )
11:       )
11:       (pooler): BertPooler(
11:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
11:         (activation): Tanh()
11:       )
11:     )
11:   )
11:   (heads_only_segment): BertForPreTrainingHeadsOnly(
11:     (cls): BertPreTrainingHeads(
11:       (predictions): BertLMPredictionHead(
11:         (transform): BertPredictionHeadTransform(
11:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
11:           (LayerNorm): FastLayerNorm()
11:         )
11:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
11:       )
11:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
11:     )
11:   )
11: )
12: BertForPreTrainingSegmented(
12:   (bert_model_segment): BertForPreTrainingModelOnly(
12:     (bert): BertModel(
12:       (embeddings): BertEmbeddings(
12:         (word_embeddings): Embedding(30528, 1024)
12:         (position_embeddings): Embedding(512, 1024)
12:         (token_type_embeddings): Embedding(2, 1024)
12:         (LayerNorm): FastLayerNorm()
12:         (dropout): Dropout(p=0.1, inplace=False)
12:       )
12:       (encoder): BertEncoder(
12:         (layer): ModuleList(
12:           (0-23): 24 x BertTransformerLayer2(
12:             (attention): FP8_MHA()
12:             (layernorm_mlp): LayerNormMLP()
12:             (output_LayerNorm): FastLayerNorm()
12:           )
12:         )
12:       )
12:       (pooler): BertPooler(
12:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
12:         (activation): Tanh()
12:       )
12:     )
12:   )
12:   (heads_only_segment): BertForPreTrainingHeadsOnly(
12:     (cls): BertPreTrainingHeads(
12:       (predictions): BertLMPredictionHead(
12:         (transform): BertPredictionHeadTransform(
12:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
12:           (LayerNorm): FastLayerNorm()
12:         )
12:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
12:       )
12:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
12:     )
12:   )
12: )
13: BertForPreTrainingSegmented(
13:   (bert_model_segment): BertForPreTrainingModelOnly(
13:     (bert): BertModel(
13:       (embeddings): BertEmbeddings(
13:         (word_embeddings): Embedding(30528, 1024)
13:         (position_embeddings): Embedding(512, 1024)
13:         (token_type_embeddings): Embedding(2, 1024)
13:         (LayerNorm): FastLayerNorm()
13:         (dropout): Dropout(p=0.1, inplace=False)
13:       )
13:       (encoder): BertEncoder(
13:         (layer): ModuleList(
13:           (0-23): 24 x BertTransformerLayer2(
13:             (attention): FP8_MHA()
13:             (layernorm_mlp): LayerNormMLP()
13:             (output_LayerNorm): FastLayerNorm()
13:           )
13:         )
13:       )
13:       (pooler): BertPooler(
13:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
13:         (activation): Tanh()
13:       )
13:     )
13:   )
13:   (heads_only_segment): BertForPreTrainingHeadsOnly(
13:     (cls): BertPreTrainingHeads(
13:       (predictions): BertLMPredictionHead(
13:         (transform): BertPredictionHeadTransform(
13:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
13:           (LayerNorm): FastLayerNorm()
13:         )
13:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
13:       )
13:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
13:     )
13:   )
13: )
15: BertForPreTrainingSegmented(
15:   (bert_model_segment): BertForPreTrainingModelOnly(
15:     (bert): BertModel(
15:       (embeddings): BertEmbeddings(
15:         (word_embeddings): Embedding(30528, 1024)
15:         (position_embeddings): Embedding(512, 1024)
15:         (token_type_embeddings): Embedding(2, 1024)
15:         (LayerNorm): FastLayerNorm()
15:         (dropout): Dropout(p=0.1, inplace=False)
15:       )
15:       (encoder): BertEncoder(
15:         (layer): ModuleList(
15:           (0-23): 24 x BertTransformerLayer2(
15:             (attention): FP8_MHA()
15:             (layernorm_mlp): LayerNormMLP()
15:             (output_LayerNorm): FastLayerNorm()
15:           )
15:         )
15:       )
15:       (pooler): BertPooler(
15:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
15:         (activation): Tanh()
15:       )
15:     )
15:   )
15:   (heads_only_segment): BertForPreTrainingHeadsOnly(
15:     (cls): BertPreTrainingHeads(
15:       (predictions): BertLMPredictionHead(
15:         (transform): BertPredictionHeadTransform(
15:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
15:           (LayerNorm): FastLayerNorm()
15:         )
15:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
15:       )
15:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
15:     )
15:   )
15: )
10: BertForPreTrainingSegmented(
10:   (bert_model_segment): BertForPreTrainingModelOnly(
10:     (bert): BertModel(
10:       (embeddings): BertEmbeddings(
10:         (word_embeddings): Embedding(30528, 1024)
10:         (position_embeddings): Embedding(512, 1024)
10:         (token_type_embeddings): Embedding(2, 1024)
10:         (LayerNorm): FastLayerNorm()
10:         (dropout): Dropout(p=0.1, inplace=False)
10:       )
10:       (encoder): BertEncoder(
10:         (layer): ModuleList(
10:           (0-23): 24 x BertTransformerLayer2(
10:             (attention): FP8_MHA()
10:             (layernorm_mlp): LayerNormMLP()
10:             (output_LayerNorm): FastLayerNorm()
10:           )
10:         )
10:       )
10:       (pooler): BertPooler(
10:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
10:         (activation): Tanh()
10:       )
10:     )
10:   )
10:   (heads_only_segment): BertForPreTrainingHeadsOnly(
10:     (cls): BertPreTrainingHeads(
10:       (predictions): BertLMPredictionHead(
10:         (transform): BertPredictionHeadTransform(
10:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
10:           (LayerNorm): FastLayerNorm()
10:         )
10:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
10:       )
10:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
10:     )
10:   )
10: )
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625836, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/word_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625836, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/position_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/token_type_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
 6: BertForPreTrainingSegmented(
 6:   (bert_model_segment): BertForPreTrainingModelOnly(
 6:     (bert): BertModel(
 6:       (embeddings): BertEmbeddings(
 6:         (word_embeddings): Embedding(30528, 1024)
 6:         (position_embeddings): Embedding(512, 1024)
 6:         (token_type_embeddings): Embedding(2, 1024)
 6:         (LayerNorm): FastLayerNorm()
 6:         (dropout): Dropout(p=0.1, inplace=False)
 6:       )
 6:       (encoder): BertEncoder(
 6:         (layer): ModuleList(
 6:           (0-23): 24 x BertTransformerLayer2(
 6:             (attention): FP8_MHA()
 6:             (layernorm_mlp): LayerNormMLP()
 6:             (output_LayerNorm): FastLayerNorm()
 6:           )
 6:         )
 6:       )
 6:       (pooler): BertPooler(
 6:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 6:         (activation): Tanh()
 6:       )
 6:     )
 6:   )
 6:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 6:     (cls): BertPreTrainingHeads(
 6:       (predictions): BertLMPredictionHead(
 6:         (transform): BertPredictionHeadTransform(
 6:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 6:           (LayerNorm): FastLayerNorm()
 6:         )
 6:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 6:       )
 6:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 6:     )
 6:   )
 6: )
 7: BertForPreTrainingSegmented(
 7:   (bert_model_segment): BertForPreTrainingModelOnly(
 7:     (bert): BertModel(
 7:       (embeddings): BertEmbeddings(
 7:         (word_embeddings): Embedding(30528, 1024)
 7:         (position_embeddings): Embedding(512, 1024)
 7:         (token_type_embeddings): Embedding(2, 1024)
 7:         (LayerNorm): FastLayerNorm()
 7:         (dropout): Dropout(p=0.1, inplace=False)
 7:       )
 7:       (encoder): BertEncoder(
 7:         (layer): ModuleList(
 7:           (0-23): 24 x BertTransformerLayer2(
 7:             (attention): FP8_MHA()
 7:             (layernorm_mlp): LayerNormMLP()
 7:             (output_LayerNorm): FastLayerNorm()
 7:           )
 7:         )
 7:       )
 7:       (pooler): BertPooler(
 7:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 7:         (activation): Tanh()
 7:       )
 7:     )
 7:   )
 7:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 7:     (cls): BertPreTrainingHeads(
 7:       (predictions): BertLMPredictionHead(
 7:         (transform): BertPredictionHeadTransform(
 7:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 7:           (LayerNorm): FastLayerNorm()
 7:         )
 7:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 7:       )
 7:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 7:     )
 7:   )
 7: )
 5: BertForPreTrainingSegmented(
 5:   (bert_model_segment): BertForPreTrainingModelOnly(
 5:     (bert): BertModel(
 5:       (embeddings): BertEmbeddings(
 5:         (word_embeddings): Embedding(30528, 1024)
 5:         (position_embeddings): Embedding(512, 1024)
 5:         (token_type_embeddings): Embedding(2, 1024)
 5:         (LayerNorm): FastLayerNorm()
 5:         (dropout): Dropout(p=0.1, inplace=False)
 5:       )
 5:       (encoder): BertEncoder(
 5:         (layer): ModuleList(
 5:           (0-23): 24 x BertTransformerLayer2(
 5:             (attention): FP8_MHA()
 5:             (layernorm_mlp): LayerNormMLP()
 5:             (output_LayerNorm): FastLayerNorm()
 5:           )
 5:         )
 5:       )
 5:       (pooler): BertPooler(
 5:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 5:         (activation): Tanh()
 5:       )
 5:     )
 5:   )
 5:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 5:     (cls): BertPreTrainingHeads(
 5:       (predictions): BertLMPredictionHead(
 5:         (transform): BertPredictionHeadTransform(
 5:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 5:           (LayerNorm): FastLayerNorm()
 5:         )
 5:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 5:       )
 5:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 5:     )
 5:   )
 5: )
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
 3: BertForPreTrainingSegmented(
 3:   (bert_model_segment): BertForPreTrainingModelOnly(
 3:     (bert): BertModel(
 3:       (embeddings): BertEmbeddings(
 3:         (word_embeddings): Embedding(30528, 1024)
 3:         (position_embeddings): Embedding(512, 1024)
 3:         (token_type_embeddings): Embedding(2, 1024)
 3:         (LayerNorm): FastLayerNorm()
 3:         (dropout): Dropout(p=0.1, inplace=False)
 3:       )
 3:       (encoder): BertEncoder(
 3:         (layer): ModuleList(
 3:           (0-23): 24 x BertTransformerLayer2(
 3:             (attention): FP8_MHA()
 3:             (layernorm_mlp): LayerNormMLP()
 3:             (output_LayerNorm): FastLayerNorm()
 3:           )
 3:         )
 3:       )
 3:       (pooler): BertPooler(
 3:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 3:         (activation): Tanh()
 3:       )
 3:     )
 3:   )
 3:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 3:     (cls): BertPreTrainingHeads(
 3:       (predictions): BertLMPredictionHead(
 3:         (transform): BertPredictionHeadTransform(
 3:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 3:           (LayerNorm): FastLayerNorm()
 3:         )
 3:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 3:       )
 3:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 3:     )
 3:   )
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
 1: BertForPreTrainingSegmented(
 1:   (bert_model_segment): BertForPreTrainingModelOnly(
 1:     (bert): BertModel(
 1:       (embeddings): BertEmbeddings(
 1:         (word_embeddings): Embedding(30528, 1024)
 1:         (position_embeddings): Embedding(512, 1024)
 1:         (token_type_embeddings): Embedding(2, 1024)
 1:         (LayerNorm): FastLayerNorm()
 1:         (dropout): Dropout(p=0.1, inplace=False)
 1:       )
 1:       (encoder): BertEncoder(
 1:         (layer): ModuleList(
 1:           (0-23): 24 x BertTransformerLayer2(
 1:             (attention): FP8_MHA()
 1:             (layernorm_mlp): LayerNormMLP()
 1:             (output_LayerNorm): FastLayerNorm()
 1:           )
 1:         )
 1:       )
 1:       (pooler): BertPooler(
 1:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 1:         (activation): Tanh()
 1:       )
 1:     )
 1:   )
 1:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 1:     (cls): BertPreTrainingHeads(
 1:       (predictions): BertLMPredictionHead(
 1:         (transform): BertPredictionHeadTransform(
 1:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 1:           (LayerNorm): FastLayerNorm()
 1:         )
 1:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 1:       )
 1:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 1:     )
 1:   )
 1: )
 2: BertForPreTrainingSegmented(
 2:   (bert_model_segment): BertForPreTrainingModelOnly(
 2:     (bert): BertModel(
 2:       (embeddings): BertEmbeddings(
 2:         (word_embeddings): Embedding(30528, 1024)
 2:         (position_embeddings): Embedding(512, 1024)
 2:         (token_type_embeddings): Embedding(2, 1024)
 2:         (LayerNorm): FastLayerNorm()
 2:         (dropout): Dropout(p=0.1, inplace=False)
 2:       )
 2:       (encoder): BertEncoder(
 2:         (layer): ModuleList(
 2:           (0-23): 24 x BertTransformerLayer2(
 2:             (attention): FP8_MHA()
 2:             (layernorm_mlp): LayerNormMLP()
 2:             (output_LayerNorm): FastLayerNorm()
 2:           )
 2:         )
 2:       )
 2:       (pooler): BertPooler(
 2:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 2:         (activation): Tanh()
 2:       )
 2:     )
 2:   )
 2:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 2:     (cls): BertPreTrainingHeads(
 2:       (predictions): BertLMPredictionHead(
 2:         (transform): BertPredictionHeadTransform(
 2:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 2:           (LayerNorm): FastLayerNorm()
 2:         )
 2:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 2:       )
 2:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 2:     )
 2:   )
 2: )
 3: )
 4: BertForPreTrainingSegmented(
 4:   (bert_model_segment): BertForPreTrainingModelOnly(
 4:     (bert): BertModel(
 4:       (embeddings): BertEmbeddings(
 4:         (word_embeddings): Embedding(30528, 1024)
 4:         (position_embeddings): Embedding(512, 1024)
 4:         (token_type_embeddings): Embedding(2, 1024)
 4:         (LayerNorm): FastLayerNorm()
 4:         (dropout): Dropout(p=0.1, inplace=False)
 4:       )
 4:       (encoder): BertEncoder(
 4:         (layer): ModuleList(
 4:           (0-23): 24 x BertTransformerLayer2(
 4:             (attention): FP8_MHA()
 4:             (layernorm_mlp): LayerNormMLP()
 4:             (output_LayerNorm): FastLayerNorm()
 4:           )
 4:         )
 4:       )
 4:       (pooler): BertPooler(
 4:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 4:         (activation): Tanh()
 4:       )
 4:     )
 4:   )
 4:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 4:     (cls): BertPreTrainingHeads(
 4:       (predictions): BertLMPredictionHead(
 4:         (transform): BertPredictionHeadTransform(
 4:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 4:           (LayerNorm): FastLayerNorm()
 4:         )
 4:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 4:       )
 4:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 4:     )
 4:   )
 4: )
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625837, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625838, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625839, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625840, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625841, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625842, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625843, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625844, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
 8: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 8:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
14: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
14:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
13: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
13:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625846, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625847, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625848, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625849, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625850, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625854, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625855, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625859, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625860, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/pooler/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/pooler/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/output_bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/seq_relationship/output_weights"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079625863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/seq_relationship/output_bias"}}
 0: BertForPreTrainingSegmented(
 0:   (bert_model_segment): BertForPreTrainingModelOnly(
 0:     (bert): BertModel(
 0:       (embeddings): BertEmbeddings(
 0:         (word_embeddings): Embedding(30528, 1024)
 0:         (position_embeddings): Embedding(512, 1024)
 0:         (token_type_embeddings): Embedding(2, 1024)
 0:         (LayerNorm): FastLayerNorm()
 0:         (dropout): Dropout(p=0.1, inplace=False)
 0:       )
 0:       (encoder): BertEncoder(
 0:         (layer): ModuleList(
 0:           (0-23): 24 x BertTransformerLayer2(
 0:             (attention): FP8_MHA()
 0:             (layernorm_mlp): LayerNormMLP()
 0:             (output_LayerNorm): FastLayerNorm()
 0:           )
 0:         )
 0:       )
 0:       (pooler): BertPooler(
 0:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 0:         (activation): Tanh()
 0:       )
 0:     )
 0:   )
 0:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 0:     (cls): BertPreTrainingHeads(
 0:       (predictions): BertLMPredictionHead(
 0:         (transform): BertPredictionHeadTransform(
 0:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 0:           (LayerNorm): FastLayerNorm()
 0:         )
 0:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 0:       )
 0:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 0:     )
 0:   )
 0: )
 4: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 4:   self._overflow_buf = torch.cuda.IntTensor([0])
 7: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 7:   self._overflow_buf = torch.cuda.IntTensor([0])
 6: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 6:   self._overflow_buf = torch.cuda.IntTensor([0])
12: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
12:   self._overflow_buf = torch.cuda.IntTensor([0])
15: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
15:   self._overflow_buf = torch.cuda.IntTensor([0])
 9: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 9:   self._overflow_buf = torch.cuda.IntTensor([0])
11: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
11:   self._overflow_buf = torch.cuda.IntTensor([0])
10: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
10:   self._overflow_buf = torch.cuda.IntTensor([0])
 1: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 1:   self._overflow_buf = torch.cuda.IntTensor([0])
 5: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 5:   self._overflow_buf = torch.cuda.IntTensor([0])
 2: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 2:   self._overflow_buf = torch.cuda.IntTensor([0])
 3: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 3:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: :::MLLOG {"namespace": "", "time_ms": 1746079626225, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00096, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 991}}
 0: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 0:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: [rank0]:[W501 06:07:08.167886703 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 8: [rank8]:[W501 06:07:08.923338741 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 1: [rank1]:[W501 06:07:08.168046163 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 3: [rank3]:[W501 06:07:08.168050023 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 2: [rank2]:[W501 06:07:08.168122483 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 7: [rank7]:[W501 06:07:08.168137693 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
11: [rank11]:[W501 06:07:08.923492060 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 9: [rank9]:[W501 06:07:08.923711207 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
15: [rank15]:[W501 06:07:08.923770377 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 5: [rank5]:[W501 06:07:08.168424683 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
10: [rank10]:[W501 06:07:08.923826406 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
12: [rank12]:[W501 06:07:08.923841376 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 6: [rank6]:[W501 06:07:08.168562212 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 4: [rank4]:[W501 06:07:08.168817082 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
13: [rank13]:[W501 06:07:08.924277472 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
14: [rank14]:[W501 06:07:08.924369141 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631106, "event_type": "POINT_IN_TIME", "key": "opt_lamb_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1030}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631106, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1031}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631106, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.60466, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1033}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631106, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.85437, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1034}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631106, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1035}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631153, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631154, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079631154, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
 8: Torch distributed is available.
 8: Torch distributed is initialized.
13: Torch distributed is available.
13: Torch distributed is initialized.
14: Torch distributed is available.
14: Torch distributed is initialized.
 4: Torch distributed is available.
 4: Torch distributed is initialized.
 6: Torch distributed is available.
 6: Torch distributed is initialized.
 0: Torch distributed is available.
 0: Torch distributed is initialized.
 5: Torch distributed is available.
 5: Torch distributed is initialized.
 9: Torch distributed is available.
 9: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
11: Torch distributed is available.
11: Torch distributed is initialized.
12: Torch distributed is available.
12: Torch distributed is initialized.
15: Torch distributed is available.
15: Torch distributed is initialized.
 1: Torch distributed is available.
 1: Torch distributed is initialized.
 2: Torch distributed is available.
 2: Torch distributed is initialized.
 3: Torch distributed is available.
 3: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
14: Enabling make_graphed_callables for encoder!!
 8: Enabling make_graphed_callables for encoder!!
 6: Enabling make_graphed_callables for encoder!!
13: Enabling make_graphed_callables for encoder!!
 4: Enabling make_graphed_callables for encoder!!
 5: Enabling make_graphed_callables for encoder!!
12: Enabling make_graphed_callables for encoder!!
15: Enabling make_graphed_callables for encoder!!
11: Enabling make_graphed_callables for encoder!!
10: Enabling make_graphed_callables for encoder!!
 9: Enabling make_graphed_callables for encoder!!
 0: Enabling make_graphed_callables for encoder!!
 7: Enabling make_graphed_callables for encoder!!
 3: Enabling make_graphed_callables for encoder!!
 1: Enabling make_graphed_callables for encoder!!
 2: Enabling make_graphed_callables for encoder!!
 1: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 7: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 6: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 2: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 4: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 3: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 5: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 9: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
10: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 0: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
15: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 8: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
11: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
14: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
13: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 9: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
10: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
11: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
 2: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
13: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
 5: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
 6: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
 3: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
 0: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
15: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
 4: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
 7: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
12: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
 8: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
 1: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
14: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
 0: :::MLLOG {"namespace": "", "time_ms": 1746079635659, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1783}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079635659, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1783}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079635681, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1805, "epoch_num": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079635681, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1807, "first_epoch_num": 1, "epoch_count": 1}}
 0: parsed args:
 0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=48, eval_batch_size=16, learning_rate=0.00096, weight_decay_rate=0.1, opt_lamb_beta_1=0.60466, opt_lamb_beta_2=0.85437, max_steps=3680.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=0.0, start_warmup_step=0.0, local_rank=0, seed=2139, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_ch
 0: eckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, eval_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num
 0: _ar_pg=1, dwu_num_ag_pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine=False, use_transformer_engine2=True, n_gpu=16, device=device(type='cuda', index=0), resume_step=0)
 0: epoch: 1
 0: :::MLLOG {"namespace": "", "time_ms": 1746079635682, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data_phase2/part_01870", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1844}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079643195, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 20022.986627204067}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 150437}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079643196, "event_type": "INTERVAL_START", "key": "eval_start", "value": 150437, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 150437}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079643767, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 150437, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 150437}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079643768, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3738578259944916, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 150437}}
 0: {'global_steps': 98, 'eval_loss': 4.106517314910889, 'eval_mlm_accuracy': 0.3738578259944916}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079650351, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 20994.417219712366}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 300657}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079650351, "event_type": "INTERVAL_START", "key": "eval_start", "value": 300657, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 300657}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079650656, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 300657, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 300657}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079650657, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3856247067451477, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 300657}}
 0: {'global_steps': 196, 'eval_loss': 4.001625061035156, 'eval_mlm_accuracy': 0.3856247067451477}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079657115, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21978.92574464111}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 449326}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079657115, "event_type": "INTERVAL_START", "key": "eval_start", "value": 449326, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 449326}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079657421, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 449326, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 449326}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079657422, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4003269374370575, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 449326}}
 0: {'global_steps': 293, 'eval_loss': 3.8444182872772217, 'eval_mlm_accuracy': 0.4003269374370575}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079664330, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 20869.730617278}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 599900}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079664331, "event_type": "INTERVAL_START", "key": "eval_start", "value": 599900, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 599900}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079664637, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 599900, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 599900}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079664638, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.44476279616355896, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 599900}}
 0: {'global_steps': 391, 'eval_loss': 3.4428305625915527, 'eval_mlm_accuracy': 0.44476279616355896}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079671165, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22055.87266185688}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 750665}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079671166, "event_type": "INTERVAL_START", "key": "eval_start", "value": 750665, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 750665}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079671472, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 750665, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 750665}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079671473, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5145095586776733, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 750665}}
 0: {'global_steps': 489, 'eval_loss': 2.85591197013855, 'eval_mlm_accuracy': 0.5145095586776733}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079677929, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21994.369041499718}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 899435}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079677930, "event_type": "INTERVAL_START", "key": "eval_start", "value": 899435, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 899435}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079678236, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 899435, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 899435}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079678236, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6080633401870728, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 899435}}
 0: {'global_steps': 586, 'eval_loss': 2.1073620319366455, 'eval_mlm_accuracy': 0.6080633401870728}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079684744, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22093.045471497102}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1049998}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079684745, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1049998, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1049998}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079685052, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1049998, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1049998}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079685053, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6848973035812378, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1049998}}
 0: {'global_steps': 684, 'eval_loss': 1.5365887880325317, 'eval_mlm_accuracy': 0.6848973035812378}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079691932, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 20906.35295072976}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1200263}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079691933, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200263, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1200263}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079692239, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200263, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1200263}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079692240, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7038261890411377, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1200263}}
 0: {'global_steps': 782, 'eval_loss': 1.4080007076263428, 'eval_mlm_accuracy': 0.7038261890411377}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079698701, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21977.905250218926}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1349029}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079698701, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1349029, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1349029}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079699008, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1349029, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1349029}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079699009, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7091456651687622, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1349029}}
 0: {'global_steps': 879, 'eval_loss': 1.3739908933639526, 'eval_mlm_accuracy': 0.7091456651687622}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079705532, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21989.037178441606}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1499252}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079705533, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1499252, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1499252}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079705839, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1499252, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1499252}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079705840, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7117470502853394, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1499252}}
 0: {'global_steps': 977, 'eval_loss': 1.3584871292114258, 'eval_mlm_accuracy': 0.7117470502853394}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079712364, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21977.882651702435}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1649393}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079712365, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1649393, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1649393}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079712671, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1649393, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1649393}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079712671, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7133162617683411, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1649393}}
 0: {'global_steps': 1075, 'eval_loss': 1.3465156555175781, 'eval_mlm_accuracy': 0.7133162617683411}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079719488, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 20911.754283461858}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1798368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079719489, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1798368, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1798368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079719795, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1798368, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1798368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079719796, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7146146297454834, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1798368}}
 0: {'global_steps': 1172, 'eval_loss': 1.339462399482727, 'eval_mlm_accuracy': 0.7146146297454834}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079726319, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22027.097445162828}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1948830}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079726319, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1948830, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1948830}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079726626, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1948830, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1948830}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079726627, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.715555727481842, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1948830}}
 0: {'global_steps': 1270, 'eval_loss': 1.3305798768997192, 'eval_mlm_accuracy': 0.715555727481842}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079733152, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22050.48324566781}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2099504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079733152, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2099504, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2099504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079733459, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2099504, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2099504}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079733460, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7165715098381042, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2099504}}
 0: {'global_steps': 1368, 'eval_loss': 1.3233013153076172, 'eval_mlm_accuracy': 0.7165715098381042}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079739915, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22040.26846915493}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2248575}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079739916, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2248575, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2248575}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079740223, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2248575, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2248575}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079740224, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7173304557800293, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2248575}}
 0: {'global_steps': 1465, 'eval_loss': 1.3215341567993164, 'eval_mlm_accuracy': 0.7173304557800293}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079747112, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 20874.219574344705}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2398799}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079747113, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2398799, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2398799}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079747420, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2398799, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2398799}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079747421, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7181594371795654, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2398799}}
 0: {'global_steps': 1563, 'eval_loss': 1.316972255706787, 'eval_mlm_accuracy': 0.7181594371795654}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079753947, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21967.72646221261}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2548955}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079753948, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2548955, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2548955}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079754255, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2548955, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2548955}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079754255, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7185003757476807, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2548955}}
 0: {'global_steps': 1661, 'eval_loss': 1.3164559602737427, 'eval_mlm_accuracy': 0.7185003757476807}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079760698, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22088.384424580297}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2698078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079760699, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2698078, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2698078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079761006, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2698078, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2698078}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079761007, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7190771698951721, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2698078}}
 0: {'global_steps': 1758, 'eval_loss': 1.3110859394073486, 'eval_mlm_accuracy': 0.7190771698951721}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079767524, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22020.522970662343}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2848390}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079767525, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2848390, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2848390}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079767831, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2848390, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2848390}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079767832, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7194507718086243, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2848390}}
 0: {'global_steps': 1856, 'eval_loss': 1.309954285621643, 'eval_mlm_accuracy': 0.7194507718086243}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079774794, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 20770.930276395567}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2999382}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079774795, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2999382, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2999382}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079775102, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2999382, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2999382}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079775103, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7198827862739563, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2999382}}
 0: {'global_steps': 1954, 'eval_loss': 1.3053799867630005, 'eval_mlm_accuracy': 0.7198827862739563}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781564, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21980.607626603723}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 3148206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781565, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3148206, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 3148206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781873, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3148206, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 3148206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781874, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7205809950828552, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 3148206}}
 0: {'global_steps': 2051, 'eval_loss': 1.3060710430145264, 'eval_mlm_accuracy': 0.7205809950828552}
 0: 0.720581 > 0.720000, Target MLM Accuracy reached at 2051
 0: Training runs 2.5089728275934857 mins sustained_training_time 0
 0: (1, 2051.0) {'final_loss': 0.0}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781875, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2157, "first_epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781876, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2160, "epoch_num": 3148206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781877, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3148206, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2162}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781877, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2165}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781878, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2168, "status": "success"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746079781879, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21530.699310724376, "epoch_num": 3148206}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2207, "step": [2, 2051]}}
 0: {'e2e_time': 163.28020977973938, 'training_sequences_per_second': 37548.43113441854, 'final_loss': 0.0, 'raw_train_time': 150.5383801460266}
++ date +%s
+ echo 'RUNANDTIME_STOP 1746079788'
RUNANDTIME_STOP 1746079788
+ set -e
