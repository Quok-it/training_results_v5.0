+ echo 'Beginning trial 4 of 5'
Beginning trial 4 of 5
+ echo ':::DLPAL /mlperf_data/nvdlfwea+mlperftv50+retinanet-arm+20250423.sqsh 40 18 gb200node[01-18] '\''unknown'\'' GB200_018x04x004'
:::DLPAL /mlperf_data/nvdlfwea+mlperftv50+retinanet-arm+20250423.sqsh 40 18 gb200node[01-18] 'unknown' GB200_018x04x004
++ srun -N1 -n1 --container-name=single_stage_detector_40 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"18","host_processors_per_node":"2","host_processor_model_name":"Neoverse-V2","host_processor_core_count":"72","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"4","accelerator_model_name":"HGX GB200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"189471 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3-fix-v2.25.1-1","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.8.0-1026-nvidia-64k","nvidia_kernel_driver":"570.124.06"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"18","host_processors_per_node":"2","host_processor_model_name":"Neoverse-V2","host_processor_core_count":"72","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"4","accelerator_model_name":"HGX GB200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"189471 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3-fix-v2.25.1-1","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.8.0-1026-nvidia-64k","nvidia_kernel_driver":"570.124.06"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=single_stage_detector_40 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID 8c262b08b227bd43cefaeedd061fe71d67271031 
+ srun -N1 -n1 --container-name=single_stage_detector_40 --no-container-mount-home --container-remap-root --container-writable python -c ''
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on gb200node08
Clearing cache on gb200node04
Clearing cache on gb200node12
Clearing cache on gb200node16
Clearing cache on gb200node09
Clearing cache on gb200node15
Clearing cache on gb200node14
Clearing cache on gb200node05
Clearing cache on gb200node02
Clearing cache on gb200node17
Clearing cache on gb200node01
Clearing cache on gb200node13
Clearing cache on gb200node07
Clearing cache on gb200node11
Clearing cache on gb200node10
Clearing cache on gb200node03
Clearing cache on gb200node06
Clearing cache on gb200node18
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks-per-node=1 --container-name=single_stage_detector_40 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1746155418912, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418930, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418929, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418928, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418940, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418941, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418954, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418956, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418958, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418960, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418967, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418975, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418980, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155418984, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155419002, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155419012, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155419014, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746155419016, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ sleep 10
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1746155429'
RUNANDTIME_START 1746155429
+ srun -l --ntasks-per-node=4 --time=7 --container-name=single_stage_detector_40 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/mlperf_data/openimages-v6-mlperf:/datasets/open-images-v6,./results:/results,/mlperf_data/ssd:/root/.cache/torch --container-workdir=/workspace/ssd --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
63: RANK 63: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 63, SLURM_LOCALID 3, OMP_NUM_THREADS 1
63: running benchmark
46: RANK 46: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 46, SLURM_LOCALID 2, OMP_NUM_THREADS 1
46: running benchmark
57: RANK 57: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 57, SLURM_LOCALID 1, OMP_NUM_THREADS 1
57: running benchmark
48: RANK 48: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 48, SLURM_LOCALID 0, OMP_NUM_THREADS 1
48: running benchmark
60: RANK 60: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 60, SLURM_LOCALID 0, OMP_NUM_THREADS 1
60: running benchmark
61: RANK 61: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 61, SLURM_LOCALID 1, OMP_NUM_THREADS 1
61: running benchmark
62: RANK 62: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 62, SLURM_LOCALID 2, OMP_NUM_THREADS 1
62: running benchmark
13: RANK 13: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 13, SLURM_LOCALID 1, OMP_NUM_THREADS 1
13: running benchmark
29: RANK 29: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 29, SLURM_LOCALID 1, OMP_NUM_THREADS 1
29: running benchmark
 7: RANK 7: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 7, SLURM_LOCALID 3, OMP_NUM_THREADS 1
 7: running benchmark
18: RANK 18: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 18, SLURM_LOCALID 2, OMP_NUM_THREADS 1
18: running benchmark
30: RANK 30: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 30, SLURM_LOCALID 2, OMP_NUM_THREADS 1
30: running benchmark
32: RANK 32: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 32, SLURM_LOCALID 0, OMP_NUM_THREADS 1
32: running benchmark
26: RANK 26: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 26, SLURM_LOCALID 2, OMP_NUM_THREADS 1
26: running benchmark
 5: RANK 5: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 5, SLURM_LOCALID 1, OMP_NUM_THREADS 1
 5: running benchmark
 4: RANK 4: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 4, SLURM_LOCALID 0, OMP_NUM_THREADS 1
 4: running benchmark
31: RANK 31: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 31, SLURM_LOCALID 3, OMP_NUM_THREADS 1
31: running benchmark
15: RANK 15: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 15, SLURM_LOCALID 3, OMP_NUM_THREADS 1
15: running benchmark
28: RANK 28: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 28, SLURM_LOCALID 0, OMP_NUM_THREADS 1
28: running benchmark
 6: RANK 6: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 6, SLURM_LOCALID 2, OMP_NUM_THREADS 1
 6: running benchmark
12: RANK 12: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 12, SLURM_LOCALID 0, OMP_NUM_THREADS 1
12: running benchmark
14: RANK 14: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 14, SLURM_LOCALID 2, OMP_NUM_THREADS 1
14: running benchmark
64: RANK 64: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 64, SLURM_LOCALID 0, OMP_NUM_THREADS 1
64: running benchmark
 2: RANK 2: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 2, SLURM_LOCALID 2, OMP_NUM_THREADS 1
 2: running benchmark
47: RANK 47: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 47, SLURM_LOCALID 3, OMP_NUM_THREADS 1
47: running benchmark
44: RANK 44: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 44, SLURM_LOCALID 0, OMP_NUM_THREADS 1
44: running benchmark
45: RANK 45: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 45, SLURM_LOCALID 1, OMP_NUM_THREADS 1
45: running benchmark
49: RANK 49: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 49, SLURM_LOCALID 1, OMP_NUM_THREADS 1
49: running benchmark
51: RANK 51: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 51, SLURM_LOCALID 3, OMP_NUM_THREADS 1
51: running benchmark
50: RANK 50: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 50, SLURM_LOCALID 2, OMP_NUM_THREADS 1
50: running benchmark
58: RANK 58: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 58, SLURM_LOCALID 2, OMP_NUM_THREADS 1
58: running benchmark
56: RANK 56: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 56, SLURM_LOCALID 0, OMP_NUM_THREADS 1
56: running benchmark
59: RANK 59: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 59, SLURM_LOCALID 3, OMP_NUM_THREADS 1
59: running benchmark
10: RANK 10: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 10, SLURM_LOCALID 2, OMP_NUM_THREADS 1
10: running benchmark
20: RANK 20: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 20, SLURM_LOCALID 0, OMP_NUM_THREADS 1
20: running benchmark
55: RANK 55: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 55, SLURM_LOCALID 3, OMP_NUM_THREADS 1
55: running benchmark
19: RANK 19: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 19, SLURM_LOCALID 3, OMP_NUM_THREADS 1
19: running benchmark
16: RANK 16: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 16, SLURM_LOCALID 0, OMP_NUM_THREADS 1
16: running benchmark
17: RANK 17: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 17, SLURM_LOCALID 1, OMP_NUM_THREADS 1
17: running benchmark
27: RANK 27: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 27, SLURM_LOCALID 3, OMP_NUM_THREADS 1
27: running benchmark
25: RANK 25: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 25, SLURM_LOCALID 1, OMP_NUM_THREADS 1
25: running benchmark
24: RANK 24: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 24, SLURM_LOCALID 0, OMP_NUM_THREADS 1
24: running benchmark
34: RANK 34: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 34, SLURM_LOCALID 2, OMP_NUM_THREADS 1
34: running benchmark
33: RANK 33: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 33, SLURM_LOCALID 1, OMP_NUM_THREADS 1
33: running benchmark
35: RANK 35: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 35, SLURM_LOCALID 3, OMP_NUM_THREADS 1
35: running benchmark
54: RANK 54: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 54, SLURM_LOCALID 2, OMP_NUM_THREADS 1
54: running benchmark
43: RANK 43: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 43, SLURM_LOCALID 3, OMP_NUM_THREADS 1
43: running benchmark
53: RANK 53: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 53, SLURM_LOCALID 1, OMP_NUM_THREADS 1
53: running benchmark
69: RANK 69: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 69, SLURM_LOCALID 1, OMP_NUM_THREADS 1
69: running benchmark
52: RANK 52: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 52, SLURM_LOCALID 0, OMP_NUM_THREADS 1
52: running benchmark
65: RANK 65: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 65, SLURM_LOCALID 1, OMP_NUM_THREADS 1
65: running benchmark
67: RANK 67: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 67, SLURM_LOCALID 3, OMP_NUM_THREADS 1
67: running benchmark
66: RANK 66: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 66, SLURM_LOCALID 2, OMP_NUM_THREADS 1
66: running benchmark
36: RANK 36: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 36, SLURM_LOCALID 0, OMP_NUM_THREADS 1
36: running benchmark
 1: RANK 1: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 1, SLURM_LOCALID 1, OMP_NUM_THREADS 1
 1: running benchmark
 0: RANK 0: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 0, SLURM_LOCALID 0, OMP_NUM_THREADS 1
 0: running benchmark
 3: RANK 3: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 3, SLURM_LOCALID 3, OMP_NUM_THREADS 1
 3: running benchmark
 9: RANK 9: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 9, SLURM_LOCALID 1, OMP_NUM_THREADS 1
 9: running benchmark
11: RANK 11: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 11, SLURM_LOCALID 3, OMP_NUM_THREADS 1
11: running benchmark
 8: RANK 8: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 8, SLURM_LOCALID 0, OMP_NUM_THREADS 1
 8: running benchmark
23: RANK 23: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 23, SLURM_LOCALID 3, OMP_NUM_THREADS 1
23: running benchmark
22: RANK 22: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 22, SLURM_LOCALID 2, OMP_NUM_THREADS 1
22: running benchmark
21: RANK 21: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 21, SLURM_LOCALID 1, OMP_NUM_THREADS 1
21: running benchmark
41: RANK 41: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 41, SLURM_LOCALID 1, OMP_NUM_THREADS 1
41: running benchmark
40: RANK 40: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 40, SLURM_LOCALID 0, OMP_NUM_THREADS 1
40: running benchmark
42: RANK 42: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 42, SLURM_LOCALID 2, OMP_NUM_THREADS 1
42: running benchmark
68: RANK 68: LOCAL_RANK 0, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 68, SLURM_LOCALID 0, OMP_NUM_THREADS 1
68: running benchmark
70: RANK 70: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 70, SLURM_LOCALID 2, OMP_NUM_THREADS 1
70: running benchmark
71: RANK 71: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 71, SLURM_LOCALID 3, OMP_NUM_THREADS 1
71: running benchmark
38: RANK 38: LOCAL_RANK 2, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 38, SLURM_LOCALID 2, OMP_NUM_THREADS 1
38: running benchmark
39: RANK 39: LOCAL_RANK 3, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 39, SLURM_LOCALID 3, OMP_NUM_THREADS 1
39: running benchmark
37: RANK 37: LOCAL_RANK 1, MASTER_ADDR gb200node01, MASTER_PORT 29500, WORLD_SIZE 72, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 40, SLURM_NTASKS 72, SLURM_PROCID 37, SLURM_LOCALID 1, OMP_NUM_THREADS 1
37: running benchmark
 0: | distributed init (rank 0): env://
 0: :::MLLOG {"namespace": "", "time_ms": 1746155439642, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "retinanet", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155439643, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155439643, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155439643, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155439643, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "18xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155439644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 353}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440460, "event_type": "POINT_IN_TIME", "key": "seed", "value": 438397308, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 366}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440461, "event_type": "POINT_IN_TIME", "key": "local_batch_size", "value": 4, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 369}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440461, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 288, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 370}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440461, "event_type": "POINT_IN_TIME", "key": "epoch_count", "value": 6, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 371}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440461, "event_type": "POINT_IN_TIME", "key": "first_epoch_num", "value": 0, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 372}}
 0: Namespace(backbone='resnext50_32x4d', trainable_backbone_layers=3, sync_bn=False, data_layout='channels_last', amp=True, async_coco=True, async_coco_check_freq=20, num_eval_ranks=72, dataset='openimages-mlperf', dataset_path='/datasets/open-images-v6', num_classes=None, train_data_path=None, train_annotations_file=None, val_data_path=None, val_annotations_file=None, image_size=[800, 800], data_augmentation='hflip', epochs=6, max_iters_per_epoch=None, max_eval_iters_per_epoch=None, start_epoch=0, output_dir=None, target_map=0.34, resume='', pretrained=False, batch_size=4, eval_batch_size=32, lr=0.0001, warmup_epochs=1, warmup_factor=0.001, workers=4, print_freq=20, eval_print_freq=20, test_only=False, seed=438397308, device='cuda', cocoeval='nvidia', coco_threads=8, world_size=72, dist_url='env://', frozen_bn_opt=True, frozen_bn_fp16=True, jit=True, cuda_graphs=True, cuda_graphs_eval=False, cls_head_pad=True, reg_head_pad=True, cuda_graphs_syn=True, model_warmup_epochs=16, master_weights=True, dali=True, dali_
 0: matched_idxs=True, dali_eval=True, dali_eval_cache=False, dali_prefetch_queue_depth=2, dali_cpu_decode=False, dali_pinned_memory_size=268435456, dali_cmn=0, dali_cmn_hint=0, dali_decoder_hint_height=7360, dali_decoder_hint_width=7360, dali_decoder_hw_load=0.65, dali_input_batch_multiplier=1, dali_eval_cmn_hint=0, dali_eval_decoder_hint_height=0, dali_eval_decoder_hint_width=0, dali_eval_decoder_hw_load=0.65, dali_eval_input_batch_multiplier=1, dali_sync=False, dali_resize_first=False, apex_adam=True, apex_focal_loss=True, apex_backbone_fusion=False, apex_head_fusion=True, broadcast_buffers=False, fp16_allreduce=False, ddp_bucket_sz=25, ddp_first_bucket_sz=None, no_gradient_as_bucket_view=False, max_boxes=1000, cudnn_bench=False, deterministic=False, not_graphed_prologues=False, metric_loss=False, syn_dataset=0, sync_after_graph_replay=False, allreduce_barrier=False, skip_eval=False, cuda_profiler=False, cuda_profiler_eval=False, cuda_profiler_start=-1, cuda_profiler_stop=-1, power_benchmark=False, power_susta
 0: in_time=600, rank=0, gpu=0, distributed=True, dist_backend='nccl', ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], num_train_ranks=72, train_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], eval_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], train_rank=0, eval_rank=0)
 0: Getting dataset information
 0: Creating model
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 209, "tensor": "module.backbone.fpn.extra_blocks.p6.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440475, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 211, "tensor": "module.backbone.fpn.extra_blocks.p6.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440475, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 209, "tensor": "module.backbone.fpn.extra_blocks.p7.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440477, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 211, "tensor": "module.backbone.fpn.extra_blocks.p7.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440554, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440555, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440555, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440555, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440556, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440556, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440556, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440556, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440557, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440557, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440557, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440558, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440558, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440559, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440560, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440561, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440562, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440562, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440563, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440564, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440564, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440565, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.3.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440566, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.3.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440567, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.3.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440568, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440570, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440570, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440574, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440578, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440582, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440582, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440586, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440590, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440590, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440594, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.3.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440598, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.3.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440599, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.3.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440602, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.4.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440606, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.4.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440607, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.4.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440611, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.5.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440614, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.5.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440615, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.5.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440619, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440626, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440628, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440643, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440657, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440672, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440674, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440688, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440703, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440705, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440759, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.inner_blocks.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440760, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.inner_blocks.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440760, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.inner_blocks.1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440760, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.inner_blocks.1.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440761, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.inner_blocks.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440762, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.inner_blocks.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440762, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.layer_blocks.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440764, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.layer_blocks.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440764, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.layer_blocks.1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440766, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.layer_blocks.1.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440766, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.layer_blocks.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440767, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.layer_blocks.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440775, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440779, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440780, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440784, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440784, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.4.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440788, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.4.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440788, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.6.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440792, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.6.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440808, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 148, "tensor": "module.head.classification_head.cls_logits.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440845, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 150, "tensor": "module.head.classification_head.cls_logits.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 317, "tensor": "module.head.regression_head.bbox_reg.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 319, "tensor": "module.head.regression_head.bbox_reg.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440857, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440862, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.4.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440866, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.4.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440866, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.6.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155440870, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.6.bias"}}
 0: Casting convolutional layers to half
 0: :::MLLOG {"namespace": "", "time_ms": 1746155441047, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 449}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155441048, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0001, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 450}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155441048, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 451}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155441048, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 452}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155441048, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.001, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 453}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155441049, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 454}}
18: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
18:   warnings.warn(
60: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
60:   warnings.warn(
27: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
27:   warnings.warn(
67: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
67:   warnings.warn(
54: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
54:   warnings.warn(
56: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
56:   warnings.warn(
12: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
41: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
41:   warnings.warn(
66: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
66:   warnings.warn(
37: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
37:   warnings.warn(
40: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
40:   warnings.warn(
49: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
49:   warnings.warn(
55: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
55:   warnings.warn(
65: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
65:   warnings.warn(
19: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
19:   warnings.warn(
51: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
51:   warnings.warn(
61: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
61:   warnings.warn(
 4: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
71: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
71:   warnings.warn(
68: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
68:   warnings.warn(
24: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
24:   warnings.warn(
39: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
39:   warnings.warn(
44: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
44:   warnings.warn(
 0: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
69: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
69:   warnings.warn(
58: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
58:   warnings.warn(
22: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
22:   warnings.warn(
42: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
42:   warnings.warn(
50: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
50:   warnings.warn(
17: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
17:   warnings.warn(
63: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
63:   warnings.warn(
10: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
 8: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
70: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
70:   warnings.warn(
 5: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
36: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
36:   warnings.warn(
53: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
53:   warnings.warn(
26: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
26:   warnings.warn(
64: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
64:   warnings.warn(
16: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
16:   warnings.warn(
43: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
43:   warnings.warn(
52: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
52:   warnings.warn(
38: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
38:   warnings.warn(
25: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
25:   warnings.warn(
35: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
35:   warnings.warn(
59: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
59:   warnings.warn(
62: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
62:   warnings.warn(
57: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
57:   warnings.warn(
34: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
34:   warnings.warn(
 7: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
 6: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
13: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
15: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
14: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
 9: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
11: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
46: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
46:   warnings.warn(
48: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
48:   warnings.warn(
45: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
45:   warnings.warn(
47: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
47:   warnings.warn(
32: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
32:   warnings.warn(
33: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
33:   warnings.warn(
21: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
21:   warnings.warn(
29: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
29:   warnings.warn(
 1: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
31: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
31:   warnings.warn(
20: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
20:   warnings.warn(
28: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
28:   warnings.warn(
30: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
30:   warnings.warn(
 3: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
23: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
23:   warnings.warn(
 2: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
 0: Model eval warmup
 0: Time: 11.627085208892822 sec
 0: Creating Dali training dataloader
 0: Creating Dali eval dataloader
 0: CUDA graph capture for training
 0: CUDA graphs: data preprocessing complete
 0: CUDA graphs: warmup iterations complete
 0: CUDA graphs: capture complete
 0: CUDA graph capture for training complete
 0: :::MLLOG {"namespace": "", "time_ms": 1746155472361, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 574}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155472361, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 578}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155472362, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 4064, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155472363, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 11, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 635}}
 0: Running ...
 0: :::MLLOG {"namespace": "", "time_ms": 1746155472363, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 0, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 0}}
 0: Epoch: [0]  [   0/4064]  eta: 0:00:11    time: 0.0028  data: 0.0002  max mem: 14785
 0: Epoch: [0]  [  20/4064]  eta: 0:00:59    time: 0.0154  data: 0.0149  max mem: 14785
 0: Epoch: [0]  [  40/4064]  eta: 0:00:57    time: 0.0136  data: 0.0128  max mem: 14785
 0: Epoch: [0]  [  60/4064]  eta: 0:00:58    time: 0.0157  data: 0.0147  max mem: 14785
 0: Epoch: [0]  [  80/4064]  eta: 0:00:59    time: 0.0153  data: 0.0145  max mem: 14785
 0: Epoch: [0]  [ 100/4064]  eta: 0:00:56    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [ 120/4064]  eta: 0:00:55    time: 0.0122  data: 0.0117  max mem: 14785
 0: Epoch: [0]  [ 140/4064]  eta: 0:00:54    time: 0.0130  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [ 160/4064]  eta: 0:00:53    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [0]  [ 180/4064]  eta: 0:00:54    time: 0.0166  data: 0.0154  max mem: 14785
 0: Epoch: [0]  [ 200/4064]  eta: 0:00:53    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [ 220/4064]  eta: 0:00:52    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [ 240/4064]  eta: 0:00:51    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [ 260/4064]  eta: 0:00:51    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [ 280/4064]  eta: 0:00:50    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [ 300/4064]  eta: 0:00:49    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [ 320/4064]  eta: 0:00:49    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [ 340/4064]  eta: 0:00:49    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 360/4064]  eta: 0:00:49    time: 0.0143  data: 0.0138  max mem: 14785
 0: Epoch: [0]  [ 380/4064]  eta: 0:00:49    time: 0.0162  data: 0.0157  max mem: 14785
 0: Epoch: [0]  [ 400/4064]  eta: 0:00:49    time: 0.0137  data: 0.0132  max mem: 14785
 0: Epoch: [0]  [ 420/4064]  eta: 0:00:48    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [ 440/4064]  eta: 0:00:48    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [ 460/4064]  eta: 0:00:47    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [ 480/4064]  eta: 0:00:47    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [ 500/4064]  eta: 0:00:46    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [ 520/4064]  eta: 0:00:46    time: 0.0128  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 540/4064]  eta: 0:00:46    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [ 560/4064]  eta: 0:00:45    time: 0.0134  data: 0.0127  max mem: 14785
 0: Epoch: [0]  [ 580/4064]  eta: 0:00:45    time: 0.0139  data: 0.0130  max mem: 14785
 0: Epoch: [0]  [ 600/4064]  eta: 0:00:45    time: 0.0130  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [ 620/4064]  eta: 0:00:45    time: 0.0130  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [ 640/4064]  eta: 0:00:45    time: 0.0139  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [ 660/4064]  eta: 0:00:44    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [ 680/4064]  eta: 0:00:44    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [ 700/4064]  eta: 0:00:43    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [ 720/4064]  eta: 0:00:43    time: 0.0119  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [ 740/4064]  eta: 0:00:43    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [ 760/4064]  eta: 0:00:42    time: 0.0128  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [ 780/4064]  eta: 0:00:42    time: 0.0128  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [ 800/4064]  eta: 0:00:42    time: 0.0135  data: 0.0130  max mem: 14785
 0: Epoch: [0]  [ 820/4064]  eta: 0:00:42    time: 0.0143  data: 0.0132  max mem: 14785
 0: Epoch: [0]  [ 840/4064]  eta: 0:00:41    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [ 860/4064]  eta: 0:00:41    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [ 880/4064]  eta: 0:00:41    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [ 900/4064]  eta: 0:00:40    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [ 920/4064]  eta: 0:00:40    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [ 940/4064]  eta: 0:00:40    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [ 960/4064]  eta: 0:00:39    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [ 980/4064]  eta: 0:00:39    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [1000/4064]  eta: 0:00:39    time: 0.0128  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1020/4064]  eta: 0:00:38    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [1040/4064]  eta: 0:00:38    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1060/4064]  eta: 0:00:38    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [1080/4064]  eta: 0:00:38    time: 0.0121  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [1100/4064]  eta: 0:00:37    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1120/4064]  eta: 0:00:37    time: 0.0118  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [1140/4064]  eta: 0:00:37    time: 0.0121  data: 0.0117  max mem: 14785
 0: Epoch: [0]  [1160/4064]  eta: 0:00:36    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1180/4064]  eta: 0:00:36    time: 0.0147  data: 0.0142  max mem: 14785
 0: Epoch: [0]  [1200/4064]  eta: 0:00:36    time: 0.0138  data: 0.0132  max mem: 14785
 0: Epoch: [0]  [1220/4064]  eta: 0:00:36    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [1240/4064]  eta: 0:00:35    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1260/4064]  eta: 0:00:35    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [1280/4064]  eta: 0:00:35    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [1300/4064]  eta: 0:00:35    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [1320/4064]  eta: 0:00:34    time: 0.0140  data: 0.0135  max mem: 14785
 0: Epoch: [0]  [1340/4064]  eta: 0:00:34    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [1360/4064]  eta: 0:00:34    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [1380/4064]  eta: 0:00:34    time: 0.0128  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1400/4064]  eta: 0:00:33    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [1420/4064]  eta: 0:00:33    time: 0.0164  data: 0.0157  max mem: 14785
 0: Epoch: [0]  [1440/4064]  eta: 0:00:33    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1460/4064]  eta: 0:00:33    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [1480/4064]  eta: 0:00:32    time: 0.0123  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [1500/4064]  eta: 0:00:32    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1520/4064]  eta: 0:00:32    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [1540/4064]  eta: 0:00:31    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1560/4064]  eta: 0:00:31    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1580/4064]  eta: 0:00:31    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [1600/4064]  eta: 0:00:31    time: 0.0128  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [1620/4064]  eta: 0:00:30    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [1640/4064]  eta: 0:00:30    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [1660/4064]  eta: 0:00:30    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1680/4064]  eta: 0:00:30    time: 0.0135  data: 0.0127  max mem: 14785
 0: Epoch: [0]  [1700/4064]  eta: 0:00:29    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [1720/4064]  eta: 0:00:29    time: 0.0162  data: 0.0157  max mem: 14785
 0: Epoch: [0]  [1740/4064]  eta: 0:00:29    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1760/4064]  eta: 0:00:29    time: 0.0123  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [1780/4064]  eta: 0:00:28    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [1800/4064]  eta: 0:00:28    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1820/4064]  eta: 0:00:28    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [1840/4064]  eta: 0:00:28    time: 0.0141  data: 0.0134  max mem: 14785
 0: Epoch: [0]  [1860/4064]  eta: 0:00:27    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [1880/4064]  eta: 0:00:27    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [1900/4064]  eta: 0:00:27    time: 0.0125  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [1920/4064]  eta: 0:00:27    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [1940/4064]  eta: 0:00:26    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1960/4064]  eta: 0:00:26    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [1980/4064]  eta: 0:00:26    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2000/4064]  eta: 0:00:25    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2020/4064]  eta: 0:00:25    time: 0.0128  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [2040/4064]  eta: 0:00:25    time: 0.0129  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [2060/4064]  eta: 0:00:25    time: 0.0114  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [2080/4064]  eta: 0:00:24    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2100/4064]  eta: 0:00:24    time: 0.0122  data: 0.0117  max mem: 14785
 0: Epoch: [0]  [2120/4064]  eta: 0:00:24    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [2140/4064]  eta: 0:00:24    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [2160/4064]  eta: 0:00:23    time: 0.0129  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [2180/4064]  eta: 0:00:23    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2200/4064]  eta: 0:00:23    time: 0.0128  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2220/4064]  eta: 0:00:23    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [2240/4064]  eta: 0:00:22    time: 0.0150  data: 0.0143  max mem: 14785
 0: Epoch: [0]  [2260/4064]  eta: 0:00:22    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2280/4064]  eta: 0:00:22    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2300/4064]  eta: 0:00:22    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [2320/4064]  eta: 0:00:21    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [2340/4064]  eta: 0:00:21    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2360/4064]  eta: 0:00:21    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [2380/4064]  eta: 0:00:21    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2400/4064]  eta: 0:00:20    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2420/4064]  eta: 0:00:20    time: 0.0127  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2440/4064]  eta: 0:00:20    time: 0.0147  data: 0.0140  max mem: 14785
 0: Epoch: [0]  [2460/4064]  eta: 0:00:20    time: 0.0125  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2480/4064]  eta: 0:00:19    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2500/4064]  eta: 0:00:19    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [2520/4064]  eta: 0:00:19    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [2540/4064]  eta: 0:00:19    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [2560/4064]  eta: 0:00:18    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [2580/4064]  eta: 0:00:18    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [2600/4064]  eta: 0:00:18    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2620/4064]  eta: 0:00:18    time: 0.0123  data: 0.0118  max mem: 14785
 0: Epoch: [0]  [2640/4064]  eta: 0:00:17    time: 0.0130  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [2660/4064]  eta: 0:00:17    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2680/4064]  eta: 0:00:17    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2700/4064]  eta: 0:00:17    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2720/4064]  eta: 0:00:16    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [2740/4064]  eta: 0:00:16    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2760/4064]  eta: 0:00:16    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2780/4064]  eta: 0:00:15    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [2800/4064]  eta: 0:00:15    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [2820/4064]  eta: 0:00:15    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [2840/4064]  eta: 0:00:15    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [2860/4064]  eta: 0:00:14    time: 0.0126  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2880/4064]  eta: 0:00:14    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [2900/4064]  eta: 0:00:14    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2920/4064]  eta: 0:00:14    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2940/4064]  eta: 0:00:13    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2960/4064]  eta: 0:00:13    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [2980/4064]  eta: 0:00:13    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3000/4064]  eta: 0:00:13    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3020/4064]  eta: 0:00:12    time: 0.0127  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [3040/4064]  eta: 0:00:12    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [0]  [3060/4064]  eta: 0:00:12    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3080/4064]  eta: 0:00:12    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [3100/4064]  eta: 0:00:11    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [0]  [3120/4064]  eta: 0:00:11    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3140/4064]  eta: 0:00:11    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3160/4064]  eta: 0:00:11    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [3180/4064]  eta: 0:00:10    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [0]  [3200/4064]  eta: 0:00:10    time: 0.0146  data: 0.0138  max mem: 14785
 0: Epoch: [0]  [3220/4064]  eta: 0:00:10    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3240/4064]  eta: 0:00:10    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3260/4064]  eta: 0:00:09    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3280/4064]  eta: 0:00:09    time: 0.0142  data: 0.0137  max mem: 14785
 0: Epoch: [0]  [3300/4064]  eta: 0:00:09    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [3320/4064]  eta: 0:00:09    time: 0.0138  data: 0.0131  max mem: 14785
 0: Epoch: [0]  [3340/4064]  eta: 0:00:08    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3360/4064]  eta: 0:00:08    time: 0.0129  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [3380/4064]  eta: 0:00:08    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [3400/4064]  eta: 0:00:08    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3420/4064]  eta: 0:00:07    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [3440/4064]  eta: 0:00:07    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3460/4064]  eta: 0:00:07    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [3480/4064]  eta: 0:00:07    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [0]  [3500/4064]  eta: 0:00:06    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3520/4064]  eta: 0:00:06    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3540/4064]  eta: 0:00:06    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3560/4064]  eta: 0:00:06    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3580/4064]  eta: 0:00:05    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3600/4064]  eta: 0:00:05    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3620/4064]  eta: 0:00:05    time: 0.0146  data: 0.0141  max mem: 14785
 0: Epoch: [0]  [3640/4064]  eta: 0:00:05    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3660/4064]  eta: 0:00:04    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [3680/4064]  eta: 0:00:04    time: 0.0140  data: 0.0135  max mem: 14785
 0: Epoch: [0]  [3700/4064]  eta: 0:00:04    time: 0.0124  data: 0.0117  max mem: 14785
 0: Epoch: [0]  [3720/4064]  eta: 0:00:04    time: 0.0126  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3740/4064]  eta: 0:00:04    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3760/4064]  eta: 0:00:03    time: 0.0125  data: 0.0117  max mem: 14785
 0: Epoch: [0]  [3780/4064]  eta: 0:00:03    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3800/4064]  eta: 0:00:03    time: 0.0124  data: 0.0117  max mem: 14785
 0: Epoch: [0]  [3820/4064]  eta: 0:00:03    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [3840/4064]  eta: 0:00:02    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [0]  [3860/4064]  eta: 0:00:02    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [3880/4064]  eta: 0:00:02    time: 0.0121  data: 0.0116  max mem: 14785
 0: Epoch: [0]  [3900/4064]  eta: 0:00:02    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3920/4064]  eta: 0:00:01    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3940/4064]  eta: 0:00:01    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [0]  [3960/4064]  eta: 0:00:01    time: 0.0120  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [3980/4064]  eta: 0:00:01    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4000/4064]  eta: 0:00:00    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [0]  [4020/4064]  eta: 0:00:00    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [4040/4064]  eta: 0:00:00    time: 0.0114  data: 0.0109  max mem: 14785
 0: Epoch: [0]  [4060/4064]  eta: 0:00:00    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [0]  [4063/4064]  eta: 0:00:00    time: 0.0161  data: 0.0153  max mem: 14785
 0: Epoch: [0] Total time: 0:00:50 (0.0124 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155522735, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 0, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155522735, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23297.472231593583, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155522736, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 1}}
 7: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 7:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 6: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 6:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
33: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
33:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
41: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
41:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
71: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
71:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
70: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
70:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 5: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 5:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
43: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
43:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
40: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
40:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
42: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
42:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
61: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
61:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
47: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
47:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
46: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
46:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
49: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
49:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 4: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 4:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
60: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
60:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
44: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
44:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
69: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
69:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
62: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
62:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
55: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
55:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
63: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
63:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
66: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
66:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 1: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 1:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
67: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
67:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
19: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
19:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
18: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
18:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
11: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
11:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
52: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
52:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
32: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
32:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
26: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
26:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
68: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
68:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
10: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
10:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
54: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
54:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
35: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
35:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
21: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
21:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
58: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
58:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
53: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
53:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
30: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
30:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
17: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
17:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
29: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
29:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
22: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
22:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 9: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 9:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 0: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 0:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
28: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
28:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
36: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
36:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
48: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
48:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
27: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
27:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
20: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
20:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
16: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
16:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
25: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
25:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
37: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
37:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
23: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
23:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 8: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 8:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
31: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
31:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
39: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
39:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
13: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
13:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
34: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
34:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
14: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
14:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 3: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 3:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
15: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
15:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
38: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
38:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 2: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 2:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
56: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
56:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
24: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
24:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
65: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
65:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
12: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
12:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
59: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
59:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
45: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
45:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
64: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
64:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
57: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
57:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
50: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
50:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
51: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
51:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 0: Test:  [ 0/11]  eta: 0:00:35  model_time: 3.1582 (3.1582)  evaluator_time: 0.0046 (0.0046)  time: 3.2685  data: 0.0008  max mem: 14785
 0: Test:  [10/11]  eta: 0:00:00  model_time: 0.5242 (0.8223)  evaluator_time: 0.0047 (0.0048)  time: 0.8377  data: 0.0009  max mem: 14785
 0: Test: Total time: 0:00:09 (0.8378 s / it)
 0: Averaged stats: model_time: 0.5242 (0.8343)  evaluator_time: 0.0047 (0.0049)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155533729, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 1}}
 1: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 3: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
21: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
24: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
71: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
12: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
61: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
28: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
54: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 4: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
33: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
16: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 2: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
40: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
56: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
49: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
22: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
36: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
20: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
67: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 8: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
23: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
13: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
70: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
14: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
34: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
15: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
32: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
62: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
30: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
52: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 5: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
44: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
35: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
26: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
17: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
60: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
29: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
53: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 6: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
27: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
19: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
37: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
31: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
55: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 7: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
58: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
25: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
18: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
10: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
43: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
64: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
38: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
57: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
48: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 9: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
41: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
69: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
66: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
39: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
63: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
47: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
51: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
11: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
65: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
59: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
46: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
45: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
50: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
68: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
42: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: Epoch: [1]  [   0/4064]  eta: 0:00:13    time: 0.0032  data: 0.0009  max mem: 14785
 0: Epoch: [1]  [  20/4064]  eta: 0:00:44    time: 0.0113  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [  40/4064]  eta: 0:00:45    time: 0.0119  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [  60/4064]  eta: 0:00:46    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [  80/4064]  eta: 0:00:46    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 100/4064]  eta: 0:00:46    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [ 120/4064]  eta: 0:00:45    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 140/4064]  eta: 0:00:45    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 160/4064]  eta: 0:00:46    time: 0.0144  data: 0.0136  max mem: 14785
 0: Epoch: [1]  [ 180/4064]  eta: 0:00:46    time: 0.0122  data: 0.0118  max mem: 14785
 0: Epoch: [1]  [ 200/4064]  eta: 0:00:46    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 220/4064]  eta: 0:00:46    time: 0.0130  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [ 240/4064]  eta: 0:00:45    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [ 260/4064]  eta: 0:00:45    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [ 280/4064]  eta: 0:00:45    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [ 300/4064]  eta: 0:00:44    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 320/4064]  eta: 0:00:44    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [ 340/4064]  eta: 0:00:44    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 360/4064]  eta: 0:00:44    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 380/4064]  eta: 0:00:43    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [ 400/4064]  eta: 0:00:43    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [ 420/4064]  eta: 0:00:43    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [ 440/4064]  eta: 0:00:43    time: 0.0121  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [ 460/4064]  eta: 0:00:42    time: 0.0122  data: 0.0117  max mem: 14785
 0: Epoch: [1]  [ 480/4064]  eta: 0:00:42    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [ 500/4064]  eta: 0:00:42    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [ 520/4064]  eta: 0:00:42    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 540/4064]  eta: 0:00:42    time: 0.0130  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [ 560/4064]  eta: 0:00:41    time: 0.0130  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [ 580/4064]  eta: 0:00:41    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 600/4064]  eta: 0:00:41    time: 0.0118  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [ 620/4064]  eta: 0:00:41    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [ 640/4064]  eta: 0:00:40    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [ 660/4064]  eta: 0:00:40    time: 0.0123  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [ 680/4064]  eta: 0:00:40    time: 0.0115  data: 0.0107  max mem: 14785
 0: Epoch: [1]  [ 700/4064]  eta: 0:00:40    time: 0.0118  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [ 720/4064]  eta: 0:00:39    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [ 740/4064]  eta: 0:00:39    time: 0.0116  data: 0.0108  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1746155542773, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.18270749458095553, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155542773, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 1}}
 0: Epoch: [1]  [ 760/4064]  eta: 0:00:39    time: 0.0115  data: 0.0107  max mem: 14785
 0: Epoch: [1]  [ 780/4064]  eta: 0:00:39    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 800/4064]  eta: 0:00:38    time: 0.0122  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 820/4064]  eta: 0:00:38    time: 0.0113  data: 0.0106  max mem: 14785
 0: Epoch: [1]  [ 840/4064]  eta: 0:00:38    time: 0.0127  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 860/4064]  eta: 0:00:38    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 880/4064]  eta: 0:00:37    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [ 900/4064]  eta: 0:00:37    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [ 920/4064]  eta: 0:00:37    time: 0.0126  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 940/4064]  eta: 0:00:37    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [ 960/4064]  eta: 0:00:36    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [ 980/4064]  eta: 0:00:36    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1000/4064]  eta: 0:00:36    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1020/4064]  eta: 0:00:36    time: 0.0124  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [1040/4064]  eta: 0:00:35    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1060/4064]  eta: 0:00:35    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1080/4064]  eta: 0:00:35    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [1100/4064]  eta: 0:00:35    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1120/4064]  eta: 0:00:34    time: 0.0126  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1140/4064]  eta: 0:00:34    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1160/4064]  eta: 0:00:34    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1180/4064]  eta: 0:00:34    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1200/4064]  eta: 0:00:33    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [1220/4064]  eta: 0:00:33    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [1240/4064]  eta: 0:00:33    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [1260/4064]  eta: 0:00:33    time: 0.0139  data: 0.0127  max mem: 14785
 0: Epoch: [1]  [1280/4064]  eta: 0:00:33    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [1300/4064]  eta: 0:00:32    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [1320/4064]  eta: 0:00:32    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [1340/4064]  eta: 0:00:32    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [1360/4064]  eta: 0:00:32    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [1380/4064]  eta: 0:00:31    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1400/4064]  eta: 0:00:31    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [1420/4064]  eta: 0:00:31    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1440/4064]  eta: 0:00:31    time: 0.0118  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [1460/4064]  eta: 0:00:30    time: 0.0122  data: 0.0117  max mem: 14785
 0: Epoch: [1]  [1480/4064]  eta: 0:00:30    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1500/4064]  eta: 0:00:30    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [1520/4064]  eta: 0:00:30    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [1540/4064]  eta: 0:00:29    time: 0.0138  data: 0.0133  max mem: 14785
 0: Epoch: [1]  [1560/4064]  eta: 0:00:29    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [1580/4064]  eta: 0:00:29    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1600/4064]  eta: 0:00:29    time: 0.0122  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [1620/4064]  eta: 0:00:29    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1640/4064]  eta: 0:00:28    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [1660/4064]  eta: 0:00:28    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [1680/4064]  eta: 0:00:28    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [1700/4064]  eta: 0:00:28    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1720/4064]  eta: 0:00:27    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1740/4064]  eta: 0:00:27    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1760/4064]  eta: 0:00:27    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1780/4064]  eta: 0:00:27    time: 0.0154  data: 0.0146  max mem: 14785
 0: Epoch: [1]  [1800/4064]  eta: 0:00:26    time: 0.0119  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1820/4064]  eta: 0:00:26    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1840/4064]  eta: 0:00:26    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [1860/4064]  eta: 0:00:26    time: 0.0124  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1880/4064]  eta: 0:00:25    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1900/4064]  eta: 0:00:25    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [1920/4064]  eta: 0:00:25    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [1940/4064]  eta: 0:00:25    time: 0.0121  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [1960/4064]  eta: 0:00:25    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [1980/4064]  eta: 0:00:24    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2000/4064]  eta: 0:00:24    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2020/4064]  eta: 0:00:24    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2040/4064]  eta: 0:00:24    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [2060/4064]  eta: 0:00:23    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2080/4064]  eta: 0:00:23    time: 0.0128  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [2100/4064]  eta: 0:00:23    time: 0.0123  data: 0.0118  max mem: 14785
 0: Epoch: [1]  [2120/4064]  eta: 0:00:23    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [2140/4064]  eta: 0:00:22    time: 0.0127  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [2160/4064]  eta: 0:00:22    time: 0.0121  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2180/4064]  eta: 0:00:22    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [2200/4064]  eta: 0:00:22    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [2220/4064]  eta: 0:00:21    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2240/4064]  eta: 0:00:21    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [2260/4064]  eta: 0:00:21    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2280/4064]  eta: 0:00:21    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2300/4064]  eta: 0:00:21    time: 0.0122  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [2320/4064]  eta: 0:00:20    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [2340/4064]  eta: 0:00:20    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [2360/4064]  eta: 0:00:20    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2380/4064]  eta: 0:00:20    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [2400/4064]  eta: 0:00:19    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2420/4064]  eta: 0:00:19    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2440/4064]  eta: 0:00:19    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2460/4064]  eta: 0:00:19    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2480/4064]  eta: 0:00:18    time: 0.0126  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [2500/4064]  eta: 0:00:18    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2520/4064]  eta: 0:00:18    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2540/4064]  eta: 0:00:18    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2560/4064]  eta: 0:00:17    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2580/4064]  eta: 0:00:17    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [2600/4064]  eta: 0:00:17    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [2620/4064]  eta: 0:00:17    time: 0.0136  data: 0.0131  max mem: 14785
 0: Epoch: [1]  [2640/4064]  eta: 0:00:16    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2660/4064]  eta: 0:00:16    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2680/4064]  eta: 0:00:16    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2700/4064]  eta: 0:00:16    time: 0.0133  data: 0.0128  max mem: 14785
 0: Epoch: [1]  [2720/4064]  eta: 0:00:16    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [2740/4064]  eta: 0:00:15    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2760/4064]  eta: 0:00:15    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [2780/4064]  eta: 0:00:15    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2800/4064]  eta: 0:00:15    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2820/4064]  eta: 0:00:14    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [2840/4064]  eta: 0:00:14    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [2860/4064]  eta: 0:00:14    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [2880/4064]  eta: 0:00:14    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2900/4064]  eta: 0:00:13    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [2920/4064]  eta: 0:00:13    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [2940/4064]  eta: 0:00:13    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [2960/4064]  eta: 0:00:13    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [2980/4064]  eta: 0:00:12    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3000/4064]  eta: 0:00:12    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [3020/4064]  eta: 0:00:12    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [3040/4064]  eta: 0:00:12    time: 0.0126  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3060/4064]  eta: 0:00:11    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [3080/4064]  eta: 0:00:11    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [3100/4064]  eta: 0:00:11    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [3120/4064]  eta: 0:00:11    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3140/4064]  eta: 0:00:10    time: 0.0128  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3160/4064]  eta: 0:00:10    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3180/4064]  eta: 0:00:10    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [3200/4064]  eta: 0:00:10    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [1]  [3220/4064]  eta: 0:00:10    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3240/4064]  eta: 0:00:09    time: 0.0120  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [3260/4064]  eta: 0:00:09    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [1]  [3280/4064]  eta: 0:00:09    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [3300/4064]  eta: 0:00:09    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [3320/4064]  eta: 0:00:08    time: 0.0129  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [3340/4064]  eta: 0:00:08    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3360/4064]  eta: 0:00:08    time: 0.0124  data: 0.0117  max mem: 14785
 0: Epoch: [1]  [3380/4064]  eta: 0:00:08    time: 0.0124  data: 0.0116  max mem: 14785
 0: Epoch: [1]  [3400/4064]  eta: 0:00:07    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3420/4064]  eta: 0:00:07    time: 0.0126  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [3440/4064]  eta: 0:00:07    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3460/4064]  eta: 0:00:07    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [3480/4064]  eta: 0:00:06    time: 0.0120  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [3500/4064]  eta: 0:00:06    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3520/4064]  eta: 0:00:06    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [3540/4064]  eta: 0:00:06    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3560/4064]  eta: 0:00:05    time: 0.0122  data: 0.0118  max mem: 14785
 0: Epoch: [1]  [3580/4064]  eta: 0:00:05    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3600/4064]  eta: 0:00:05    time: 0.0121  data: 0.0113  max mem: 14785
 0: Epoch: [1]  [3620/4064]  eta: 0:00:05    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3640/4064]  eta: 0:00:05    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3660/4064]  eta: 0:00:04    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [1]  [3680/4064]  eta: 0:00:04    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [3700/4064]  eta: 0:00:04    time: 0.0127  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [3720/4064]  eta: 0:00:04    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3740/4064]  eta: 0:00:03    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3760/4064]  eta: 0:00:03    time: 0.0133  data: 0.0128  max mem: 14785
 0: Epoch: [1]  [3780/4064]  eta: 0:00:03    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3800/4064]  eta: 0:00:03    time: 0.0120  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [3820/4064]  eta: 0:00:02    time: 0.0120  data: 0.0115  max mem: 14785
 0: Epoch: [1]  [3840/4064]  eta: 0:00:02    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [3860/4064]  eta: 0:00:02    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3880/4064]  eta: 0:00:02    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3900/4064]  eta: 0:00:01    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3920/4064]  eta: 0:00:01    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [3940/4064]  eta: 0:00:01    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [3960/4064]  eta: 0:00:01    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [3980/4064]  eta: 0:00:00    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [4000/4064]  eta: 0:00:00    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [1]  [4020/4064]  eta: 0:00:00    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [4040/4064]  eta: 0:00:00    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [1]  [4060/4064]  eta: 0:00:00    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [1]  [4063/4064]  eta: 0:00:00    time: 0.0149  data: 0.0142  max mem: 14785
 0: Epoch: [1] Total time: 0:00:48 (0.0119 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155582301, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155582302, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 24163.2197098846, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 2}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155582302, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 2}}
 0: Test:  [ 0/11]  eta: 0:00:04  model_time: 0.4368 (0.4368)  evaluator_time: 0.0034 (0.0034)  time: 0.4414  data: 0.0010  max mem: 14785
 0: Test:  [10/11]  eta: 0:00:00  model_time: 0.4366 (0.4280)  evaluator_time: 0.0036 (0.0037)  time: 0.4327  data: 0.0009  max mem: 14785
 0: Test: Total time: 0:00:04 (0.4328 s / it)
 0: Averaged stats: model_time: 0.4366 (0.4466)  evaluator_time: 0.0036 (0.0037)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155588607, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 2}}
 1: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 2: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 3: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
12: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
16: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
70: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
64: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
32: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
29: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
53: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 4: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
56: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
46: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
24: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 9: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
20: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
36: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
15: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
48: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
71: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
13: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
18: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
43: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
14: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
17: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
19: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
34: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
35: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
47: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
33: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
25: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 8: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
21: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
67: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
62: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
55: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 5: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
44: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
26: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
10: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
22: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
66: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
28: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
54: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 6: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
58: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
27: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
11: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
40: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
23: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
65: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
37: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
30: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
52: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 7: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
59: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
41: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
38: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
31: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
50: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
69: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
39: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
60: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
51: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
68: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
45: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
49: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
61: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
63: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
42: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
57: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: Epoch: [2]  [   0/4064]  eta: 0:00:12    time: 0.0032  data: 0.0009  max mem: 14785
 0: Epoch: [2]  [  20/4064]  eta: 0:00:46    time: 0.0120  data: 0.0115  max mem: 14785
 0: Epoch: [2]  [  40/4064]  eta: 0:00:46    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [  60/4064]  eta: 0:00:46    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [  80/4064]  eta: 0:00:46    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [ 100/4064]  eta: 0:00:46    time: 0.0119  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 120/4064]  eta: 0:00:45    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 140/4064]  eta: 0:00:46    time: 0.0128  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [ 160/4064]  eta: 0:00:45    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 180/4064]  eta: 0:00:45    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 200/4064]  eta: 0:00:45    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [ 220/4064]  eta: 0:00:45    time: 0.0119  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 240/4064]  eta: 0:00:45    time: 0.0125  data: 0.0117  max mem: 14785
 0: Epoch: [2]  [ 260/4064]  eta: 0:00:44    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [2]  [ 280/4064]  eta: 0:00:44    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [2]  [ 300/4064]  eta: 0:00:44    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [ 320/4064]  eta: 0:00:44    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [ 340/4064]  eta: 0:00:43    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 360/4064]  eta: 0:00:43    time: 0.0133  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [ 380/4064]  eta: 0:00:43    time: 0.0122  data: 0.0114  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1746155593264, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2763161743526386, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 2}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155593264, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 2}}
 0: Epoch: [2]  [ 400/4064]  eta: 0:00:43    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [ 420/4064]  eta: 0:00:43    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [2]  [ 440/4064]  eta: 0:00:42    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 460/4064]  eta: 0:00:42    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 480/4064]  eta: 0:00:42    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 500/4064]  eta: 0:00:42    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [ 520/4064]  eta: 0:00:41    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 540/4064]  eta: 0:00:41    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 560/4064]  eta: 0:00:41    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 580/4064]  eta: 0:00:41    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [2]  [ 600/4064]  eta: 0:00:40    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [2]  [ 620/4064]  eta: 0:00:40    time: 0.0124  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 640/4064]  eta: 0:00:40    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 660/4064]  eta: 0:00:40    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [ 680/4064]  eta: 0:00:39    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 700/4064]  eta: 0:00:39    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 720/4064]  eta: 0:00:39    time: 0.0121  data: 0.0117  max mem: 14785
 0: Epoch: [2]  [ 740/4064]  eta: 0:00:39    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 760/4064]  eta: 0:00:39    time: 0.0128  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 780/4064]  eta: 0:00:38    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 800/4064]  eta: 0:00:38    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [ 820/4064]  eta: 0:00:38    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [2]  [ 840/4064]  eta: 0:00:38    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [ 860/4064]  eta: 0:00:37    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [ 880/4064]  eta: 0:00:37    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [ 900/4064]  eta: 0:00:37    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [2]  [ 920/4064]  eta: 0:00:37    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [ 940/4064]  eta: 0:00:36    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [ 960/4064]  eta: 0:00:36    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [2]  [ 980/4064]  eta: 0:00:36    time: 0.0122  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [1000/4064]  eta: 0:00:36    time: 0.0122  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [1020/4064]  eta: 0:00:36    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1040/4064]  eta: 0:00:35    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1060/4064]  eta: 0:00:35    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [1080/4064]  eta: 0:00:35    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1100/4064]  eta: 0:00:35    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1120/4064]  eta: 0:00:34    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [2]  [1140/4064]  eta: 0:00:34    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [1160/4064]  eta: 0:00:34    time: 0.0125  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1180/4064]  eta: 0:00:34    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1200/4064]  eta: 0:00:33    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1220/4064]  eta: 0:00:33    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1240/4064]  eta: 0:00:33    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1260/4064]  eta: 0:00:33    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1280/4064]  eta: 0:00:32    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1300/4064]  eta: 0:00:32    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1320/4064]  eta: 0:00:32    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1340/4064]  eta: 0:00:32    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1360/4064]  eta: 0:00:31    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1380/4064]  eta: 0:00:31    time: 0.0123  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1400/4064]  eta: 0:00:31    time: 0.0114  data: 0.0110  max mem: 14785
 0: Epoch: [2]  [1420/4064]  eta: 0:00:31    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1440/4064]  eta: 0:00:30    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1460/4064]  eta: 0:00:30    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [1480/4064]  eta: 0:00:30    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1500/4064]  eta: 0:00:30    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1520/4064]  eta: 0:00:29    time: 0.0121  data: 0.0117  max mem: 14785
 0: Epoch: [2]  [1540/4064]  eta: 0:00:29    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [1560/4064]  eta: 0:00:29    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [1580/4064]  eta: 0:00:29    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [1600/4064]  eta: 0:00:29    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1620/4064]  eta: 0:00:28    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [1640/4064]  eta: 0:00:28    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [1660/4064]  eta: 0:00:28    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [1680/4064]  eta: 0:00:28    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1700/4064]  eta: 0:00:27    time: 0.0121  data: 0.0116  max mem: 14785
 0: Epoch: [2]  [1720/4064]  eta: 0:00:27    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1740/4064]  eta: 0:00:27    time: 0.0121  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [1760/4064]  eta: 0:00:27    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1780/4064]  eta: 0:00:26    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [2]  [1800/4064]  eta: 0:00:26    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [1820/4064]  eta: 0:00:26    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1840/4064]  eta: 0:00:26    time: 0.0121  data: 0.0116  max mem: 14785
 0: Epoch: [2]  [1860/4064]  eta: 0:00:25    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1880/4064]  eta: 0:00:25    time: 0.0123  data: 0.0118  max mem: 14785
 0: Epoch: [2]  [1900/4064]  eta: 0:00:25    time: 0.0121  data: 0.0117  max mem: 14785
 0: Epoch: [2]  [1920/4064]  eta: 0:00:25    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1940/4064]  eta: 0:00:25    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [2]  [1960/4064]  eta: 0:00:24    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [1980/4064]  eta: 0:00:24    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2000/4064]  eta: 0:00:24    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2020/4064]  eta: 0:00:24    time: 0.0129  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [2040/4064]  eta: 0:00:23    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [2060/4064]  eta: 0:00:23    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [2080/4064]  eta: 0:00:23    time: 0.0128  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [2100/4064]  eta: 0:00:23    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2120/4064]  eta: 0:00:22    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [2140/4064]  eta: 0:00:22    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [2160/4064]  eta: 0:00:22    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [2180/4064]  eta: 0:00:22    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2200/4064]  eta: 0:00:22    time: 0.0129  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2220/4064]  eta: 0:00:21    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [2240/4064]  eta: 0:00:21    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2260/4064]  eta: 0:00:21    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2280/4064]  eta: 0:00:21    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2300/4064]  eta: 0:00:20    time: 0.0124  data: 0.0117  max mem: 14785
 0: Epoch: [2]  [2320/4064]  eta: 0:00:20    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [2340/4064]  eta: 0:00:20    time: 0.0125  data: 0.0118  max mem: 14785
 0: Epoch: [2]  [2360/4064]  eta: 0:00:20    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [2380/4064]  eta: 0:00:19    time: 0.0119  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2400/4064]  eta: 0:00:19    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2420/4064]  eta: 0:00:19    time: 0.0126  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2440/4064]  eta: 0:00:19    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2460/4064]  eta: 0:00:18    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2480/4064]  eta: 0:00:18    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [2500/4064]  eta: 0:00:18    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2520/4064]  eta: 0:00:18    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2540/4064]  eta: 0:00:17    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [2]  [2560/4064]  eta: 0:00:17    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [2580/4064]  eta: 0:00:17    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2600/4064]  eta: 0:00:17    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [2620/4064]  eta: 0:00:17    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2640/4064]  eta: 0:00:16    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [2660/4064]  eta: 0:00:16    time: 0.0124  data: 0.0116  max mem: 14785
 0: Epoch: [2]  [2680/4064]  eta: 0:00:16    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2700/4064]  eta: 0:00:16    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2720/4064]  eta: 0:00:15    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2740/4064]  eta: 0:00:15    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2760/4064]  eta: 0:00:15    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2780/4064]  eta: 0:00:15    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [2800/4064]  eta: 0:00:14    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [2820/4064]  eta: 0:00:14    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [2840/4064]  eta: 0:00:14    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [2860/4064]  eta: 0:00:14    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [2]  [2880/4064]  eta: 0:00:13    time: 0.0121  data: 0.0117  max mem: 14785
 0: Epoch: [2]  [2900/4064]  eta: 0:00:13    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2920/4064]  eta: 0:00:13    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2940/4064]  eta: 0:00:13    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [2960/4064]  eta: 0:00:13    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [2980/4064]  eta: 0:00:12    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3000/4064]  eta: 0:00:12    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [3020/4064]  eta: 0:00:12    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3040/4064]  eta: 0:00:12    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3060/4064]  eta: 0:00:11    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3080/4064]  eta: 0:00:11    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [3100/4064]  eta: 0:00:11    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3120/4064]  eta: 0:00:11    time: 0.0121  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [3140/4064]  eta: 0:00:10    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3160/4064]  eta: 0:00:10    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [3180/4064]  eta: 0:00:10    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [3200/4064]  eta: 0:00:10    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [3220/4064]  eta: 0:00:09    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [3240/4064]  eta: 0:00:09    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3260/4064]  eta: 0:00:09    time: 0.0125  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3280/4064]  eta: 0:00:09    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3300/4064]  eta: 0:00:09    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [2]  [3320/4064]  eta: 0:00:08    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [3340/4064]  eta: 0:00:08    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [3360/4064]  eta: 0:00:08    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [2]  [3380/4064]  eta: 0:00:08    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [2]  [3400/4064]  eta: 0:00:07    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3420/4064]  eta: 0:00:07    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3440/4064]  eta: 0:00:07    time: 0.0134  data: 0.0127  max mem: 14785
 0: Epoch: [2]  [3460/4064]  eta: 0:00:07    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [3480/4064]  eta: 0:00:06    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [3500/4064]  eta: 0:00:06    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [2]  [3520/4064]  eta: 0:00:06    time: 0.0128  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3540/4064]  eta: 0:00:06    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3560/4064]  eta: 0:00:05    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3580/4064]  eta: 0:00:05    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [2]  [3600/4064]  eta: 0:00:05    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3620/4064]  eta: 0:00:05    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [2]  [3640/4064]  eta: 0:00:05    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3660/4064]  eta: 0:00:04    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3680/4064]  eta: 0:00:04    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3700/4064]  eta: 0:00:04    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [3720/4064]  eta: 0:00:04    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3740/4064]  eta: 0:00:03    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3760/4064]  eta: 0:00:03    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [3780/4064]  eta: 0:00:03    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3800/4064]  eta: 0:00:03    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3820/4064]  eta: 0:00:02    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3840/4064]  eta: 0:00:02    time: 0.0126  data: 0.0119  max mem: 14785
 0: Epoch: [2]  [3860/4064]  eta: 0:00:02    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [3880/4064]  eta: 0:00:02    time: 0.0128  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [3900/4064]  eta: 0:00:01    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [2]  [3920/4064]  eta: 0:00:01    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [3940/4064]  eta: 0:00:01    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [2]  [3960/4064]  eta: 0:00:01    time: 0.0124  data: 0.0117  max mem: 14785
 0: Epoch: [2]  [3980/4064]  eta: 0:00:00    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [2]  [4000/4064]  eta: 0:00:00    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [4020/4064]  eta: 0:00:00    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [2]  [4040/4064]  eta: 0:00:00    time: 0.0123  data: 0.0115  max mem: 14785
 0: Epoch: [2]  [4060/4064]  eta: 0:00:00    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [2]  [4063/4064]  eta: 0:00:00    time: 0.0152  data: 0.0145  max mem: 14785
 0: Epoch: [2] Total time: 0:00:48 (0.0119 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155636924, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 2}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155636924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 24290.7209906904, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 3}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155636926, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 3}}
 0: Test:  [ 0/11]  eta: 0:00:04  model_time: 0.4184 (0.4184)  evaluator_time: 0.0035 (0.0035)  time: 0.4230  data: 0.0010  max mem: 14785
 0: Test:  [10/11]  eta: 0:00:00  model_time: 0.4104 (0.4075)  evaluator_time: 0.0036 (0.0037)  time: 0.4122  data: 0.0009  max mem: 14785
 0: Test: Total time: 0:00:04 (0.4123 s / it)
 0: Averaged stats: model_time: 0.4104 (0.4363)  evaluator_time: 0.0036 (0.0038)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155642869, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 3}}
 1: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 2: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 3: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
16: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
56: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
32: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
24: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
70: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
64: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
28: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 4: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
12: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 8: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
36: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
60: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
53: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
46: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
48: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
17: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
42: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
20: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
34: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
25: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
19: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
71: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
66: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
33: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
26: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
18: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
68: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
67: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 5: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
27: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 9: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
69: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 6: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
13: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
58: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
10: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
62: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 7: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
15: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
59: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
49: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
11: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
21: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
37: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
14: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
50: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
22: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
38: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
29: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
51: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
23: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
39: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
30: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
31: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
55: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
47: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
41: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
52: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
44: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
40: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
54: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
45: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
43: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
57: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
61: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
65: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
35: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
63: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: Epoch: [3]  [   0/4063]  eta: 0:00:10    time: 0.0027  data: 0.0009  max mem: 14785
 0: Epoch: [3]  [  20/4063]  eta: 0:00:47    time: 0.0123  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [  40/4063]  eta: 0:00:47    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [  60/4063]  eta: 0:00:46    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [  80/4063]  eta: 0:00:46    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [ 100/4063]  eta: 0:00:46    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [ 120/4063]  eta: 0:00:46    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [ 140/4063]  eta: 0:00:45    time: 0.0119  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [ 160/4063]  eta: 0:00:45    time: 0.0121  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [ 180/4063]  eta: 0:00:45    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [ 200/4063]  eta: 0:00:45    time: 0.0129  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [ 220/4063]  eta: 0:00:45    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [ 240/4063]  eta: 0:00:45    time: 0.0119  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [ 260/4063]  eta: 0:00:45    time: 0.0124  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [ 280/4063]  eta: 0:00:45    time: 0.0124  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [ 300/4063]  eta: 0:00:44    time: 0.0124  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [ 320/4063]  eta: 0:00:44    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [3]  [ 340/4063]  eta: 0:00:44    time: 0.0118  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [ 360/4063]  eta: 0:00:44    time: 0.0116  data: 0.0109  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1746155647395, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3130692280260455, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 3}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155647395, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 3}}
 0: Epoch: [3]  [ 380/4063]  eta: 0:00:43    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [ 400/4063]  eta: 0:00:43    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [ 420/4063]  eta: 0:00:43    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [ 440/4063]  eta: 0:00:43    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [ 460/4063]  eta: 0:00:42    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [ 480/4063]  eta: 0:00:42    time: 0.0129  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [ 500/4063]  eta: 0:00:42    time: 0.0124  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [ 520/4063]  eta: 0:00:42    time: 0.0113  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [ 540/4063]  eta: 0:00:41    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [ 560/4063]  eta: 0:00:41    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [ 580/4063]  eta: 0:00:41    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [ 600/4063]  eta: 0:00:41    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [ 620/4063]  eta: 0:00:40    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [3]  [ 640/4063]  eta: 0:00:40    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [ 660/4063]  eta: 0:00:40    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [ 680/4063]  eta: 0:00:40    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [ 700/4063]  eta: 0:00:39    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [ 720/4063]  eta: 0:00:39    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [ 740/4063]  eta: 0:00:39    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [ 760/4063]  eta: 0:00:39    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [ 780/4063]  eta: 0:00:39    time: 0.0124  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [ 800/4063]  eta: 0:00:38    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [ 820/4063]  eta: 0:00:38    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [ 840/4063]  eta: 0:00:38    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [3]  [ 860/4063]  eta: 0:00:38    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [ 880/4063]  eta: 0:00:37    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [3]  [ 900/4063]  eta: 0:00:37    time: 0.0116  data: 0.0107  max mem: 14785
 0: Epoch: [3]  [ 920/4063]  eta: 0:00:37    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [ 940/4063]  eta: 0:00:37    time: 0.0118  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [ 960/4063]  eta: 0:00:36    time: 0.0115  data: 0.0107  max mem: 14785
 0: Epoch: [3]  [ 980/4063]  eta: 0:00:36    time: 0.0125  data: 0.0118  max mem: 14785
 0: Epoch: [3]  [1000/4063]  eta: 0:00:36    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1020/4063]  eta: 0:00:36    time: 0.0117  data: 0.0110  max mem: 14785
 0: Epoch: [3]  [1040/4063]  eta: 0:00:35    time: 0.0130  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [1060/4063]  eta: 0:00:35    time: 0.0113  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1080/4063]  eta: 0:00:35    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1100/4063]  eta: 0:00:35    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [1120/4063]  eta: 0:00:35    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1140/4063]  eta: 0:00:34    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [1160/4063]  eta: 0:00:34    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1180/4063]  eta: 0:00:34    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1200/4063]  eta: 0:00:34    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1220/4063]  eta: 0:00:33    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1240/4063]  eta: 0:00:33    time: 0.0122  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [1260/4063]  eta: 0:00:33    time: 0.0121  data: 0.0117  max mem: 14785
 0: Epoch: [3]  [1280/4063]  eta: 0:00:33    time: 0.0126  data: 0.0118  max mem: 14785
 0: Epoch: [3]  [1300/4063]  eta: 0:00:32    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1320/4063]  eta: 0:00:32    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1340/4063]  eta: 0:00:32    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1360/4063]  eta: 0:00:32    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [1380/4063]  eta: 0:00:31    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1400/4063]  eta: 0:00:31    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1420/4063]  eta: 0:00:31    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1440/4063]  eta: 0:00:31    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1460/4063]  eta: 0:00:30    time: 0.0123  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [1480/4063]  eta: 0:00:30    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1500/4063]  eta: 0:00:30    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1520/4063]  eta: 0:00:30    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1540/4063]  eta: 0:00:29    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [1560/4063]  eta: 0:00:29    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1580/4063]  eta: 0:00:29    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1600/4063]  eta: 0:00:29    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1620/4063]  eta: 0:00:28    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [1640/4063]  eta: 0:00:28    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [1660/4063]  eta: 0:00:28    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [1680/4063]  eta: 0:00:28    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1700/4063]  eta: 0:00:27    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1720/4063]  eta: 0:00:27    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [1740/4063]  eta: 0:00:27    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1760/4063]  eta: 0:00:27    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1780/4063]  eta: 0:00:27    time: 0.0117  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [1800/4063]  eta: 0:00:26    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1820/4063]  eta: 0:00:26    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [1840/4063]  eta: 0:00:26    time: 0.0129  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1860/4063]  eta: 0:00:26    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1880/4063]  eta: 0:00:25    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [1900/4063]  eta: 0:00:25    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [1920/4063]  eta: 0:00:25    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [1940/4063]  eta: 0:00:25    time: 0.0126  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [1960/4063]  eta: 0:00:24    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [1980/4063]  eta: 0:00:24    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2000/4063]  eta: 0:00:24    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [2020/4063]  eta: 0:00:24    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2040/4063]  eta: 0:00:23    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2060/4063]  eta: 0:00:23    time: 0.0131  data: 0.0127  max mem: 14785
 0: Epoch: [3]  [2080/4063]  eta: 0:00:23    time: 0.0121  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [2100/4063]  eta: 0:00:23    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2120/4063]  eta: 0:00:23    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [2140/4063]  eta: 0:00:22    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2160/4063]  eta: 0:00:22    time: 0.0121  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [2180/4063]  eta: 0:00:22    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2200/4063]  eta: 0:00:22    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2220/4063]  eta: 0:00:21    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2240/4063]  eta: 0:00:21    time: 0.0124  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [2260/4063]  eta: 0:00:21    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2280/4063]  eta: 0:00:21    time: 0.0121  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [2300/4063]  eta: 0:00:20    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2320/4063]  eta: 0:00:20    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [2340/4063]  eta: 0:00:20    time: 0.0128  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2360/4063]  eta: 0:00:20    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [2380/4063]  eta: 0:00:19    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2400/4063]  eta: 0:00:19    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2420/4063]  eta: 0:00:19    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2440/4063]  eta: 0:00:19    time: 0.0118  data: 0.0114  max mem: 14785
 0: Epoch: [3]  [2460/4063]  eta: 0:00:18    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [2480/4063]  eta: 0:00:18    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2500/4063]  eta: 0:00:18    time: 0.0125  data: 0.0118  max mem: 14785
 0: Epoch: [3]  [2520/4063]  eta: 0:00:18    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2540/4063]  eta: 0:00:18    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2560/4063]  eta: 0:00:17    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [2580/4063]  eta: 0:00:17    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2600/4063]  eta: 0:00:17    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [2620/4063]  eta: 0:00:17    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2640/4063]  eta: 0:00:16    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [2660/4063]  eta: 0:00:16    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2680/4063]  eta: 0:00:16    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2700/4063]  eta: 0:00:16    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2720/4063]  eta: 0:00:15    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [2740/4063]  eta: 0:00:15    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2760/4063]  eta: 0:00:15    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2780/4063]  eta: 0:00:15    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [2800/4063]  eta: 0:00:14    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2820/4063]  eta: 0:00:14    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2840/4063]  eta: 0:00:14    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2860/4063]  eta: 0:00:14    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2880/4063]  eta: 0:00:13    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2900/4063]  eta: 0:00:13    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2920/4063]  eta: 0:00:13    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [2940/4063]  eta: 0:00:13    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [2960/4063]  eta: 0:00:13    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [2980/4063]  eta: 0:00:12    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3000/4063]  eta: 0:00:12    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3020/4063]  eta: 0:00:12    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3040/4063]  eta: 0:00:12    time: 0.0125  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3060/4063]  eta: 0:00:11    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [3080/4063]  eta: 0:00:11    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3100/4063]  eta: 0:00:11    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [3120/4063]  eta: 0:00:11    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3140/4063]  eta: 0:00:10    time: 0.0123  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [3160/4063]  eta: 0:00:10    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3180/4063]  eta: 0:00:10    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [3]  [3200/4063]  eta: 0:00:10    time: 0.0119  data: 0.0115  max mem: 14785
 0: Epoch: [3]  [3220/4063]  eta: 0:00:09    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [3240/4063]  eta: 0:00:09    time: 0.0123  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3260/4063]  eta: 0:00:09    time: 0.0116  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3280/4063]  eta: 0:00:09    time: 0.0122  data: 0.0117  max mem: 14785
 0: Epoch: [3]  [3300/4063]  eta: 0:00:09    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3320/4063]  eta: 0:00:08    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3340/4063]  eta: 0:00:08    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [3360/4063]  eta: 0:00:08    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3380/4063]  eta: 0:00:08    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3400/4063]  eta: 0:00:07    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3420/4063]  eta: 0:00:07    time: 0.0116  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3440/4063]  eta: 0:00:07    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3460/4063]  eta: 0:00:07    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3480/4063]  eta: 0:00:06    time: 0.0123  data: 0.0118  max mem: 14785
 0: Epoch: [3]  [3500/4063]  eta: 0:00:06    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3520/4063]  eta: 0:00:06    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3540/4063]  eta: 0:00:06    time: 0.0119  data: 0.0114  max mem: 14785
 0: Epoch: [3]  [3560/4063]  eta: 0:00:05    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3580/4063]  eta: 0:00:05    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3600/4063]  eta: 0:00:05    time: 0.0116  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3620/4063]  eta: 0:00:05    time: 0.0120  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [3640/4063]  eta: 0:00:04    time: 0.0117  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3660/4063]  eta: 0:00:04    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3680/4063]  eta: 0:00:04    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3700/4063]  eta: 0:00:04    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3720/4063]  eta: 0:00:04    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3740/4063]  eta: 0:00:03    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3760/4063]  eta: 0:00:03    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3780/4063]  eta: 0:00:03    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3800/4063]  eta: 0:00:03    time: 0.0120  data: 0.0113  max mem: 14785
 0: Epoch: [3]  [3820/4063]  eta: 0:00:02    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3840/4063]  eta: 0:00:02    time: 0.0119  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3860/4063]  eta: 0:00:02    time: 0.0123  data: 0.0116  max mem: 14785
 0: Epoch: [3]  [3880/4063]  eta: 0:00:02    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [3900/4063]  eta: 0:00:01    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3920/4063]  eta: 0:00:01    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [3940/4063]  eta: 0:00:01    time: 0.0141  data: 0.0133  max mem: 14785
 0: Epoch: [3]  [3960/4063]  eta: 0:00:01    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [3]  [3980/4063]  eta: 0:00:00    time: 0.0124  data: 0.0117  max mem: 14785
 0: Epoch: [3]  [4000/4063]  eta: 0:00:00    time: 0.0120  data: 0.0110  max mem: 14785
 0: Epoch: [3]  [4020/4063]  eta: 0:00:00    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [3]  [4040/4063]  eta: 0:00:00    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [3]  [4060/4063]  eta: 0:00:00    time: 0.0118  data: 0.0111  max mem: 14785
 0: Epoch: [3]  [4062/4063]  eta: 0:00:00    time: 0.0158  data: 0.0151  max mem: 14785
 0: Epoch: [3] Total time: 0:00:48 (0.0118 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155691178, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 3}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155691178, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 24300.251638319045, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155691179, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 4}}
 0: Test:  [ 0/11]  eta: 0:00:04  model_time: 0.3727 (0.3727)  evaluator_time: 0.0031 (0.0031)  time: 0.3769  data: 0.0010  max mem: 14785
 0: Test:  [10/11]  eta: 0:00:00  model_time: 0.3727 (0.3709)  evaluator_time: 0.0033 (0.0034)  time: 0.3754  data: 0.0009  max mem: 14785
 0: Test: Total time: 0:00:04 (0.3754 s / it)
 0: Averaged stats: model_time: 0.3727 (0.3959)  evaluator_time: 0.0033 (0.0057)
 0: :::MLLOG {"namespace": "", "time_ms": 1746155696541, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 4}}
 0: Epoch: [4]  [   0/4064]  eta: 0:00:09    time: 0.0023  data: 0.0008  max mem: 14785
 0: Epoch: [4]  [  20/4064]  eta: 0:00:44    time: 0.0113  data: 0.0108  max mem: 14785
 0: Epoch: [4]  [  40/4064]  eta: 0:00:46    time: 0.0121  data: 0.0114  max mem: 14785
 0: Epoch: [4]  [  60/4064]  eta: 0:00:46    time: 0.0117  data: 0.0109  max mem: 14785
 0: Epoch: [4]  [  80/4064]  eta: 0:00:46    time: 0.0116  data: 0.0109  max mem: 14785
 0: Epoch: [4]  [ 100/4064]  eta: 0:00:45    time: 0.0115  data: 0.0108  max mem: 14785
 0: Epoch: [4]  [ 120/4064]  eta: 0:00:45    time: 0.0119  data: 0.0112  max mem: 14785
 0: Epoch: [4]  [ 140/4064]  eta: 0:00:47    time: 0.0155  data: 0.0151  max mem: 14785
 0: Epoch: [4]  [ 160/4064]  eta: 0:00:47    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [4]  [ 180/4064]  eta: 0:00:46    time: 0.0115  data: 0.0111  max mem: 14785
 0: Epoch: [4]  [ 200/4064]  eta: 0:00:46    time: 0.0115  data: 0.0110  max mem: 14785
 0: Epoch: [4]  [ 220/4064]  eta: 0:00:45    time: 0.0116  data: 0.0107  max mem: 14785
 0: Epoch: [4]  [ 240/4064]  eta: 0:00:45    time: 0.0117  data: 0.0111  max mem: 14785
 0: Epoch: [4]  [ 260/4064]  eta: 0:00:45    time: 0.0115  data: 0.0107  max mem: 14785
 0: Epoch: [4]  [ 280/4064]  eta: 0:00:45    time: 0.0127  data: 0.0118  max mem: 14785
 0: Epoch: [4]  [ 300/4064]  eta: 0:00:44    time: 0.0120  data: 0.0112  max mem: 14785
 0: Epoch: [4]  [ 320/4064]  eta: 0:00:44    time: 0.0122  data: 0.0118  max mem: 14785
 0: Epoch: [4]  [ 340/4064]  eta: 0:00:44    time: 0.0125  data: 0.0117  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1746155700683, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.34314593679912553, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155700683, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155700962, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 315, "status": "success"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155700962, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746155700963, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23634.413828377692, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 5}}
 0: Run time 0:03:48
 0: :::MLLOG {"namespace": "", "time_ms": 1746155700963, "event_type": "POINT_IN_TIME", "key": "status", "value": "success", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 783}}
40: | distributed init (rank 40): env://
56: | distributed init (rank 56): env://
58: | distributed init (rank 58): env://
48: | distributed init (rank 48): env://
 0: Loading annotations into memory...
 0: Done (t=0.24s)
 0: Creating index...
 0: Done (t=0.68s)
 0: Loading and preparing results...
 0: DONE (t=3.16s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=2.33s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.18271
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.30063
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.19141
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00468
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.04753
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.20148
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.32156
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.46863
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.49099
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.01964
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.18015
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.53702
 0: Loading and preparing results...
 0: DONE (t=2.54s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=2.05s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.27632
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.40979
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.29677
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00638
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.07699
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.30561
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.37553
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.53743
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.56118
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.02771
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.22523
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.61134
 0: Loading and preparing results...
 0: DONE (t=2.47s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=1.99s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.31307
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.45573
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.33570
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00897
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.08975
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.34664
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.38987
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.56275
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.59092
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03432
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.25297
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.64017
 0: Loading and preparing results...
 0: DONE (t=2.25s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=1.85s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34315
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.48679
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.36937
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00982
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.10291
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.37906
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.40763
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.58245
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.61044
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03136
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.26209
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.66130
61: | distributed init (rank 61): env://
41: | distributed init (rank 41): env://
11: | distributed init (rank 11): env://
62: | distributed init (rank 62): env://
51: | distributed init (rank 51): env://
19: | distributed init (rank 19): env://
27: | distributed init (rank 27): env://
28: | distributed init (rank 28): env://
 8: | distributed init (rank 8): env://
37: | distributed init (rank 37): env://
20: | distributed init (rank 20): env://
21: | distributed init (rank 21): env://
60: | distributed init (rank 60): env://
57: | distributed init (rank 57): env://
44: | distributed init (rank 44): env://
39: | distributed init (rank 39): env://
54: | distributed init (rank 54): env://
35: | distributed init (rank 35): env://
34: | distributed init (rank 34): env://
55: | distributed init (rank 55): env://
18: | distributed init (rank 18): env://
71: | distributed init (rank 71): env://
47: | distributed init (rank 47): env://
46: | distributed init (rank 46): env://
68: | distributed init (rank 68): env://
50: | distributed init (rank 50): env://
43: | distributed init (rank 43): env://
64: | distributed init (rank 64): env://
66: | distributed init (rank 66): env://
 4: | distributed init (rank 4): env://
29: | distributed init (rank 29): env://
31: | distributed init (rank 31): env://
30: | distributed init (rank 30): env://
22: | distributed init (rank 22): env://
32: | distributed init (rank 32): env://
33: | distributed init (rank 33): env://
36: | distributed init (rank 36): env://
38: | distributed init (rank 38): env://
24: | distributed init (rank 24): env://
13: | distributed init (rank 13): env://
14: | distributed init (rank 14): env://
15: | distributed init (rank 15): env://
70: | distributed init (rank 70): env://
69: | distributed init (rank 69): env://
 9: | distributed init (rank 9): env://
52: | distributed init (rank 52): env://
53: | distributed init (rank 53): env://
26: | distributed init (rank 26): env://
25: | distributed init (rank 25): env://
49: | distributed init (rank 49): env://
67: | distributed init (rank 67): env://
 1: | distributed init (rank 1): env://
 2: | distributed init (rank 2): env://
 3: | distributed init (rank 3): env://
17: | distributed init (rank 17): env://
65: | distributed init (rank 65): env://
63: | distributed init (rank 63): env://
59: | distributed init (rank 59): env://
 6: | distributed init (rank 6): env://
12: | distributed init (rank 12): env://
 7: | distributed init (rank 7): env://
 5: | distributed init (rank 5): env://
10: | distributed init (rank 10): env://
16: | distributed init (rank 16): env://
45: | distributed init (rank 45): env://
23: | distributed init (rank 23): env://
42: | distributed init (rank 42): env://
 0: [W501 20:15:02.310345358 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 4: [rank4]:[W501 20:15:02.186884833 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
 8: [rank8]:[W501 20:15:02.815666046 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
12: [rank12]:[W501 20:15:02.695308196 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
16: [rank16]:[W501 20:15:02.272091074 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
20: [rank20]:[W501 20:15:02.298295467 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
24: [rank24]:[W501 20:15:02.662209724 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
40: [rank40]:[W501 20:15:02.030880437 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
28: [rank28]:[W501 20:15:02.367171823 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
32: [rank32]:[W501 20:15:02.290946926 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
36: [rank36]:[W501 20:15:02.669779749 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
56: [rank56]:[W501 20:15:02.595175032 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
48: [rank48]:[W501 20:15:02.213478819 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
44: [rank44]:[W501 20:15:02.200159507 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
52: [rank52]:[W501 20:15:02.523041171 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
60: [rank60]:[W501 20:15:02.219016532 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
64: [rank64]:[W501 20:15:02.025955489 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
68: [rank68]:[W501 20:15:02.339115851 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
 0: [rank0]:[W501 20:15:02.962894973 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
 4: [rank4]:[W501 20:15:02.796484784 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 8: [rank8]:[W501 20:15:02.427386958 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 9: [rank9]:[W501 20:15:02.455759439 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 1: [rank1]:[W501 20:15:02.111997707 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 2: [rank2]:[W501 20:15:02.113721229 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
10: [rank10]:[W501 20:15:02.461743586 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
11: [rank11]:[W501 20:15:02.473470760 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 3: [rank3]:[W501 20:15:02.130880381 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
13: [rank13]:[W501 20:15:02.316126524 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 5: [rank5]:[W501 20:15:02.866532178 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 6: [rank6]:[W501 20:15:02.870027501 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
12: [rank12]:[W501 20:15:02.331011756 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
14: [rank14]:[W501 20:15:02.335583581 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 7: [rank7]:[W501 20:15:02.878092227 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
40: [rank40]:[W501 20:15:02.574574429 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
15: [rank15]:[W501 20:15:02.359455311 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
17: [rank17]:[W501 20:15:02.908428384 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
16: [rank16]:[W501 20:15:02.908640734 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
18: [rank18]:[W501 20:15:02.914216056 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
19: [rank19]:[W501 20:15:02.916170507 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
21: [rank21]:[W501 20:15:02.922985965 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
20: [rank20]:[W501 20:15:03.931981385 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
25: [rank25]:[W501 20:15:03.275754050 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
24: [rank24]:[W501 20:15:03.282098662 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
26: [rank26]:[W501 20:15:03.282866275 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
22: [rank22]:[W501 20:15:03.955667959 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
27: [rank27]:[W501 20:15:03.299675129 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
23: [rank23]:[W501 20:15:03.970138506 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
29: [rank29]:[W501 20:15:03.982821634 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
28: [rank28]:[W501 20:15:03.986325129 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
41: [rank41]:[W501 20:15:03.662515456 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
48: [rank48]:[W501 20:15:03.768667080 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
30: [rank30]:[W501 20:15:03.002605271 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
56: [rank56]:[W501 20:15:03.155107490 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
33: [rank33]:[W501 20:15:03.900221493 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
32: [rank32]:[W501 20:15:03.907069594 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
58: [rank58]:[W501 20:15:03.172078538 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
31: [rank31]:[W501 20:15:03.026655472 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
42: [rank42]:[W501 20:15:03.705052716 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
34: [rank34]:[W501 20:15:03.922102109 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
43: [rank43]:[W501 20:15:03.728457619 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
37: [rank37]:[W501 20:15:03.301520408 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
35: [rank35]:[W501 20:15:03.945736158 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
38: [rank38]:[W501 20:15:03.306827686 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
36: [rank36]:[W501 20:15:03.314239808 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
39: [rank39]:[W501 20:15:03.326764297 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
44: [rank44]:[W501 20:15:03.849754320 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
57: [rank57]:[W501 20:15:03.278868203 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
49: [rank49]:[W501 20:15:03.900537930 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
50: [rank50]:[W501 20:15:03.906008881 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
54: [rank54]:[W501 20:15:03.145248045 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
46: [rank46]:[W501 20:15:03.874439403 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
45: [rank45]:[W501 20:15:03.876440773 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
51: [rank51]:[W501 20:15:03.910824859 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
52: [rank52]:[W501 20:15:03.150141305 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
53: [rank53]:[W501 20:15:03.156693662 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
47: [rank47]:[W501 20:15:03.890454270 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
55: [rank55]:[W501 20:15:03.163513347 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
59: [rank59]:[W501 20:15:03.308670313 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
62: [rank62]:[W501 20:15:03.874915325 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
64: [rank64]:[W501 20:15:03.684912457 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
60: [rank60]:[W501 20:15:03.898334954 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
65: [rank65]:[W501 20:15:03.685445925 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
66: [rank66]:[W501 20:15:03.687599927 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
61: [rank61]:[W501 20:15:03.902112930 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
63: [rank63]:[W501 20:15:03.914655911 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
67: [rank67]:[W501 20:15:03.704372971 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 0: [rank0]:[W501 20:15:03.527041915 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
70: [rank70]:[W501 20:15:03.010674905 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
71: [rank71]:[W501 20:15:03.026179092 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
69: [rank69]:[W501 20:15:03.028585349 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
68: [rank68]:[W501 20:15:03.036466354 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
++ date +%s
+ echo 'RUNANDTIME_STOP 1746155707'
RUNANDTIME_STOP 1746155707
+ set -e
