+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/resource_nvme/mlperf/ssd/ssd_tv50.sqsh 173 8 gpu-[733,999,153,740,959,383,928,335] '\''unknown'\'' DGXB200_008x08x004'
:::DLPAL /mnt/resource_nvme/mlperf/ssd/ssd_tv50.sqsh 173 8 gpu-[733,999,153,740,959,383,928,335] 'unknown' DGXB200_008x08x004
++ srun -N1 -n1 --container-name=single_stage_detector_173 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8592+","host_processor_core_count":"64","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3-fix-v2.25.1-1","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-1074-oracle","nvidia_kernel_driver":"570.124.06"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8592+","host_processor_core_count":"64","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3-fix-v2.25.1-1","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-1074-oracle","nvidia_kernel_driver":"570.124.06"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=single_stage_detector_173 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID 8c262b08b227bd43cefaeedd061fe71d67271031 
+ srun -N1 -n1 --container-name=single_stage_detector_173 --no-container-mount-home --container-remap-root --container-writable python -c ''
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on gpu-733
Clearing cache on gpu-153
Clearing cache on gpu-383
Clearing cache on gpu-959
Clearing cache on gpu-335
Clearing cache on gpu-740
Clearing cache on gpu-999
Clearing cache on gpu-928
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks-per-node=1 --container-name=single_stage_detector_173 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1745985094947, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1745985094979, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1745985094989, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1745985095000, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1745985095009, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1745985095014, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1745985095021, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1745985095052, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ sleep 10
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1745985105'
RUNANDTIME_START 1745985105
+ srun -l --ntasks-per-node=8 --time=10 --container-name=single_stage_detector_173 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/mnt/resource_nvme/mlperf/ssd/data/open-images-v6:/datasets/open-images-v6,/mnt/resource_nvme/mlperf/ssd/logs:/results,/mnt/resource_nvme/mlperf/ssd/chk_pnts:/root/.cache/torch --container-workdir=/workspace/ssd --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
33: RANK 33: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 33, SLURM_LOCALID 1, OMP_NUM_THREADS 1
33: running benchmark
36: RANK 36: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 36, SLURM_LOCALID 4, OMP_NUM_THREADS 1
36: running benchmark
34: RANK 34: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 34, SLURM_LOCALID 2, OMP_NUM_THREADS 1
34: running benchmark
57: RANK 57: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 57, SLURM_LOCALID 1, OMP_NUM_THREADS 1
57: running benchmark
38: RANK 38: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 38, SLURM_LOCALID 6, OMP_NUM_THREADS 1
38: running benchmark
47: RANK 47: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 47, SLURM_LOCALID 7, OMP_NUM_THREADS 1
47: running benchmark
29: RANK 29: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 29, SLURM_LOCALID 5, OMP_NUM_THREADS 1
29: running benchmark
18: RANK 18: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 18, SLURM_LOCALID 2, OMP_NUM_THREADS 1
18: running benchmark
56: RANK 56: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 56, SLURM_LOCALID 0, OMP_NUM_THREADS 1
56: running benchmark
42: RANK 42: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 42, SLURM_LOCALID 2, OMP_NUM_THREADS 1
42: running benchmark
60: RANK 60: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 60, SLURM_LOCALID 4, OMP_NUM_THREADS 1
60: running benchmark
37: RANK 37: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 37, SLURM_LOCALID 5, OMP_NUM_THREADS 1
37: running benchmark
39: RANK 39: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 39, SLURM_LOCALID 7, OMP_NUM_THREADS 1
39: running benchmark
32: RANK 32: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 32, SLURM_LOCALID 0, OMP_NUM_THREADS 1
32: running benchmark
35: RANK 35: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 35, SLURM_LOCALID 3, OMP_NUM_THREADS 1
35: running benchmark
58: RANK 58: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 58, SLURM_LOCALID 2, OMP_NUM_THREADS 1
58: running benchmark
45: RANK 45: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 45, SLURM_LOCALID 5, OMP_NUM_THREADS 1
45: running benchmark
62: RANK 62: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 62, SLURM_LOCALID 6, OMP_NUM_THREADS 1
62: running benchmark
26: RANK 26: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 26, SLURM_LOCALID 2, OMP_NUM_THREADS 1
26: running benchmark
30: RANK 30: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 30, SLURM_LOCALID 6, OMP_NUM_THREADS 1
30: running benchmark
46: RANK 46: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 46, SLURM_LOCALID 6, OMP_NUM_THREADS 1
46: running benchmark
41: RANK 41: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 41, SLURM_LOCALID 1, OMP_NUM_THREADS 1
41: running benchmark
 6: RANK 6: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 6, SLURM_LOCALID 6, OMP_NUM_THREADS 1
 6: running benchmark
27: RANK 27: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 27, SLURM_LOCALID 3, OMP_NUM_THREADS 1
27: running benchmark
61: RANK 61: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 61, SLURM_LOCALID 5, OMP_NUM_THREADS 1
61: running benchmark
21: RANK 21: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 21, SLURM_LOCALID 5, OMP_NUM_THREADS 1
21: running benchmark
22: RANK 22: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 22, SLURM_LOCALID 6, OMP_NUM_THREADS 1
22: running benchmark
63: RANK 63: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 63, SLURM_LOCALID 7, OMP_NUM_THREADS 1
63: running benchmark
59: RANK 59: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 59, SLURM_LOCALID 3, OMP_NUM_THREADS 1
59: running benchmark
44: RANK 44: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 44, SLURM_LOCALID 4, OMP_NUM_THREADS 1
44: running benchmark
28: RANK 28: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 28, SLURM_LOCALID 4, OMP_NUM_THREADS 1
28: running benchmark
43: RANK 43: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 43, SLURM_LOCALID 3, OMP_NUM_THREADS 1
43: running benchmark
40: RANK 40: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 40, SLURM_LOCALID 0, OMP_NUM_THREADS 1
40: running benchmark
24: RANK 24: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 24, SLURM_LOCALID 0, OMP_NUM_THREADS 1
24: running benchmark
25: RANK 25: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 25, SLURM_LOCALID 1, OMP_NUM_THREADS 1
25: running benchmark
 2: RANK 2: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 2, SLURM_LOCALID 2, OMP_NUM_THREADS 1
 2: running benchmark
31: RANK 31: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 31, SLURM_LOCALID 7, OMP_NUM_THREADS 1
31: running benchmark
19: RANK 19: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 19, SLURM_LOCALID 3, OMP_NUM_THREADS 1
19: running benchmark
23: RANK 23: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 23, SLURM_LOCALID 7, OMP_NUM_THREADS 1
23: running benchmark
20: RANK 20: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 20, SLURM_LOCALID 4, OMP_NUM_THREADS 1
20: running benchmark
16: RANK 16: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 16, SLURM_LOCALID 0, OMP_NUM_THREADS 1
16: running benchmark
17: RANK 17: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 17, SLURM_LOCALID 1, OMP_NUM_THREADS 1
17: running benchmark
 5: RANK 5: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 5, SLURM_LOCALID 5, OMP_NUM_THREADS 1
 5: running benchmark
10: RANK 10: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 10, SLURM_LOCALID 2, OMP_NUM_THREADS 1
10: running benchmark
 0: RANK 0: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 0, SLURM_LOCALID 0, OMP_NUM_THREADS 1
 0: running benchmark
 8: RANK 8: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 8, SLURM_LOCALID 0, OMP_NUM_THREADS 1
 8: running benchmark
 1: RANK 1: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 1, SLURM_LOCALID 1, OMP_NUM_THREADS 1
 1: running benchmark
 4: RANK 4: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 4, SLURM_LOCALID 4, OMP_NUM_THREADS 1
 4: running benchmark
 3: RANK 3: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 3, SLURM_LOCALID 3, OMP_NUM_THREADS 1
 3: running benchmark
 7: RANK 7: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 7, SLURM_LOCALID 7, OMP_NUM_THREADS 1
 7: running benchmark
15: RANK 15: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 15, SLURM_LOCALID 7, OMP_NUM_THREADS 1
15: running benchmark
11: RANK 11: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 11, SLURM_LOCALID 3, OMP_NUM_THREADS 1
11: running benchmark
12: RANK 12: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 12, SLURM_LOCALID 4, OMP_NUM_THREADS 1
12: running benchmark
 9: RANK 9: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 9, SLURM_LOCALID 1, OMP_NUM_THREADS 1
 9: running benchmark
14: RANK 14: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 14, SLURM_LOCALID 6, OMP_NUM_THREADS 1
14: running benchmark
13: RANK 13: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 13, SLURM_LOCALID 5, OMP_NUM_THREADS 1
13: running benchmark
53: RANK 53: LOCAL_RANK 5, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 53, SLURM_LOCALID 5, OMP_NUM_THREADS 1
53: running benchmark
51: RANK 51: LOCAL_RANK 3, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 51, SLURM_LOCALID 3, OMP_NUM_THREADS 1
51: running benchmark
52: RANK 52: LOCAL_RANK 4, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 52, SLURM_LOCALID 4, OMP_NUM_THREADS 1
52: running benchmark
48: RANK 48: LOCAL_RANK 0, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 48, SLURM_LOCALID 0, OMP_NUM_THREADS 1
48: running benchmark
50: RANK 50: LOCAL_RANK 2, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 50, SLURM_LOCALID 2, OMP_NUM_THREADS 1
50: running benchmark
54: RANK 54: LOCAL_RANK 6, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 54, SLURM_LOCALID 6, OMP_NUM_THREADS 1
54: running benchmark
49: RANK 49: LOCAL_RANK 1, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 49, SLURM_LOCALID 1, OMP_NUM_THREADS 1
49: running benchmark
55: RANK 55: LOCAL_RANK 7, MASTER_ADDR gpu-733, MASTER_PORT 29500, WORLD_SIZE 64, MLPERF_SLURM_FIRSTNODE , SLURM_JOB_ID 173, SLURM_NTASKS 64, SLURM_PROCID 55, SLURM_LOCALID 7, OMP_NUM_THREADS 1
55: running benchmark
 0: | distributed init (rank 0): env://
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126452, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "retinanet", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126452, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126452, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126452, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126452, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 352}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126453, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 353}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126483, "event_type": "POINT_IN_TIME", "key": "seed", "value": 473273376, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 366}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126483, "event_type": "POINT_IN_TIME", "key": "local_batch_size", "value": 4, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 369}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126484, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 256, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 370}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126484, "event_type": "POINT_IN_TIME", "key": "epoch_count", "value": 6, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 371}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126484, "event_type": "POINT_IN_TIME", "key": "first_epoch_num", "value": 0, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 372}}
 0: Namespace(backbone='resnext50_32x4d', trainable_backbone_layers=3, sync_bn=False, data_layout='channels_last', amp=True, async_coco=True, async_coco_check_freq=20, num_eval_ranks=64, dataset='openimages-mlperf', dataset_path='/datasets/open-images-v6', num_classes=None, train_data_path=None, train_annotations_file=None, val_data_path=None, val_annotations_file=None, image_size=[800, 800], data_augmentation='hflip', epochs=6, max_iters_per_epoch=None, max_eval_iters_per_epoch=None, start_epoch=0, output_dir=None, target_map=0.34, resume='', pretrained=False, batch_size=4, eval_batch_size=32, lr=0.0001, warmup_epochs=1, warmup_factor=0.001, workers=4, print_freq=20, eval_print_freq=20, test_only=False, seed=473273376, device='cuda', cocoeval='nvidia', coco_threads=8, world_size=64, dist_url='env://', frozen_bn_opt=True, frozen_bn_fp16=True, jit=True, cuda_graphs=True, cuda_graphs_eval=False, cls_head_pad=True, reg_head_pad=True, cuda_graphs_syn=True, model_warmup_epochs=16, master_weights=True, dali=True, dali_
 0: matched_idxs=True, dali_eval=True, dali_eval_cache=False, dali_prefetch_queue_depth=2, dali_cpu_decode=False, dali_pinned_memory_size=268435456, dali_cmn=0, dali_cmn_hint=0, dali_decoder_hint_height=7360, dali_decoder_hint_width=7360, dali_decoder_hw_load=0.65, dali_input_batch_multiplier=1, dali_eval_cmn_hint=0, dali_eval_decoder_hint_height=0, dali_eval_decoder_hint_width=0, dali_eval_decoder_hw_load=0.65, dali_eval_input_batch_multiplier=1, dali_sync=False, dali_resize_first=False, apex_adam=True, apex_focal_loss=True, apex_backbone_fusion=False, apex_head_fusion=True, broadcast_buffers=False, fp16_allreduce=False, ddp_bucket_sz=25, ddp_first_bucket_sz=None, no_gradient_as_bucket_view=False, max_boxes=1000, cudnn_bench=False, deterministic=False, not_graphed_prologues=False, metric_loss=False, syn_dataset=0, sync_after_graph_replay=False, allreduce_barrier=False, skip_eval=False, cuda_profiler=False, cuda_profiler_eval=False, cuda_profiler_start=-1, cuda_profiler_stop=-1, power_benchmark=False, power_susta
 0: in_time=600, rank=0, gpu=0, distributed=True, dist_backend='nccl', ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], num_train_ranks=64, train_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], eval_ranks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], train_rank=0, eval_rank=0)
 0: Getting dataset information
 0: Creating model
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126491, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 209, "tensor": "module.backbone.fpn.extra_blocks.p6.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126500, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 211, "tensor": "module.backbone.fpn.extra_blocks.p6.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126500, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 209, "tensor": "module.backbone.fpn.extra_blocks.p7.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126503, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 211, "tensor": "module.backbone.fpn.extra_blocks.p7.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126642, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126643, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126643, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126643, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126643, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126643, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126644, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126644, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126644, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126644, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126645, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer1.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126645, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126645, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126645, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126646, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126647, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126648, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126648, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126649, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126649, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126650, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126650, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.3.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126651, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.3.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126651, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer2.3.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126652, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126653, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126654, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126657, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126659, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126662, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126662, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126665, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126668, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126669, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126672, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.3.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126675, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.3.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126675, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.3.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126678, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.4.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126681, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.4.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126682, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.4.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126684, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.5.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126687, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.5.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126687, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer3.5.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126690, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126695, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126697, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126707, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.0.downsample.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126718, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.1.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126728, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.1.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126729, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.1.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126739, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.2.conv1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126749, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.2.conv2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126751, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/resnet.py", "lineno": 286, "tensor": "module.backbone.body.layer4.2.conv3.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126851, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.inner_blocks.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.inner_blocks.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126852, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.inner_blocks.1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.inner_blocks.1.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126853, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.inner_blocks.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.inner_blocks.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126856, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.layer_blocks.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.layer_blocks.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126858, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.layer_blocks.1.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.layer_blocks.1.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126861, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 108, "tensor": "module.backbone.fpn.layer_blocks.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126863, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/feature_pyramid_network.py", "lineno": 110, "tensor": "module.backbone.fpn.layer_blocks.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126875, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126878, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126878, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126881, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126881, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.4.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126883, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.4.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126884, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 142, "tensor": "module.head.classification_head.conv.6.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126886, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 144, "tensor": "module.head.classification_head.conv.6.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126908, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 148, "tensor": "module.head.classification_head.cls_logits.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126933, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 150, "tensor": "module.head.classification_head.cls_logits.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126943, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 317, "tensor": "module.head.regression_head.bbox_reg.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126944, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 319, "tensor": "module.head.regression_head.bbox_reg.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126944, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.0.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126947, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.0.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126947, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.2.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126950, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.2.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126950, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.4.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126953, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.4.bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126953, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 324, "tensor": "module.head.regression_head.conv.6.weight"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985126956, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/ssd/model/retinanet.py", "lineno": 326, "tensor": "module.head.regression_head.conv.6.bias"}}
 0: Casting convolutional layers to half
 0: :::MLLOG {"namespace": "", "time_ms": 1745985127101, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 449}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985127102, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0001, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 450}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985127102, "event_type": "POINT_IN_TIME", "key": "opt_weight_decay", "value": 0, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 451}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985127102, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_epochs", "value": 1, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 452}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985127102, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.001, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 453}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985127103, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 454}}
20: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
20:   warnings.warn(
18: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
18:   warnings.warn(
17: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
17:   warnings.warn(
23: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
23:   warnings.warn(
16: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
16:   warnings.warn(
19: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
19:   warnings.warn(
22: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
22:   warnings.warn(
21: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
21:   warnings.warn(
51: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
51:   warnings.warn(
53: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
53:   warnings.warn(
49: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
49:   warnings.warn(
55: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
55:   warnings.warn(
48: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
48:   warnings.warn(
54: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
54:   warnings.warn(
52: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
52:   warnings.warn(
50: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
50:   warnings.warn(
62: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
62:   warnings.warn(
56: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
56:   warnings.warn(
60: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
60:   warnings.warn(
58: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
58:   warnings.warn(
59: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
59:   warnings.warn(
63: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
63:   warnings.warn(
61: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
61:   warnings.warn(
57: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
57:   warnings.warn(
24: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
24:   warnings.warn(
29: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
29:   warnings.warn(
28: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
28:   warnings.warn(
27: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
27:   warnings.warn(
30: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
30:   warnings.warn(
26: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
26:   warnings.warn(
25: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
25:   warnings.warn(
31: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
31:   warnings.warn(
 4: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
 6: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
 1: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
 5: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
 3: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
 0: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
 2: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
 7: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
44: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
44:   warnings.warn(
42: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
42:   warnings.warn(
43: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
43:   warnings.warn(
45: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
45:   warnings.warn(
40: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
40:   warnings.warn(
41: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
41:   warnings.warn(
47: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
47:   warnings.warn(
46: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
46:   warnings.warn(
38: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
38:   warnings.warn(
33: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
33:   warnings.warn(
32: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
32:   warnings.warn(
37: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
37:   warnings.warn(
39: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
39:   warnings.warn(
35: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
35:   warnings.warn(
34: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
34:   warnings.warn(
36: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
36:   warnings.warn(
15: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
12: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
10: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
 9: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
13: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
11: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
 8: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
14: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
 0: Model eval warmup
 0: Time: 6.813153505325317 sec
 0: Creating Dali training dataloader
 0: Creating Dali eval dataloader
 0: CUDA graph capture for training
 0: CUDA graphs: data preprocessing complete
 0: CUDA graphs: warmup iterations complete
 0: CUDA graphs: capture complete
 0: CUDA graph capture for training complete
 0: :::MLLOG {"namespace": "", "time_ms": 1745985158509, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 574}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985158511, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 578}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985158511, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 4572, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985158512, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 13, "metadata": {"file": "/workspace/ssd/train.py", "lineno": 635}}
 0: Running ...
 0: :::MLLOG {"namespace": "", "time_ms": 1745985158513, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 0, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 0}}
 0: Epoch: [0]  [   0/4572]  eta: 0:00:12    time: 0.0028  data: 0.0001  max mem: 14785
 0: Epoch: [0]  [  20/4572]  eta: 0:00:57    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [  40/4572]  eta: 0:00:59    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [  60/4572]  eta: 0:00:59    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [  80/4572]  eta: 0:01:00    time: 0.0138  data: 0.0127  max mem: 14785
 0: Epoch: [0]  [ 100/4572]  eta: 0:00:59    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 120/4572]  eta: 0:00:59    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 140/4572]  eta: 0:00:59    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [ 160/4572]  eta: 0:00:58    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 180/4572]  eta: 0:00:58    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 200/4572]  eta: 0:00:58    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 220/4572]  eta: 0:00:57    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 240/4572]  eta: 0:00:57    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [ 260/4572]  eta: 0:00:57    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 280/4572]  eta: 0:00:57    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [ 300/4572]  eta: 0:00:56    time: 0.0127  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [ 320/4572]  eta: 0:00:56    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [ 340/4572]  eta: 0:00:56    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 360/4572]  eta: 0:00:55    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 380/4572]  eta: 0:00:55    time: 0.0133  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [ 400/4572]  eta: 0:00:55    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [ 420/4572]  eta: 0:00:55    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [ 440/4572]  eta: 0:00:54    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 460/4572]  eta: 0:00:54    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 480/4572]  eta: 0:00:54    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 500/4572]  eta: 0:00:54    time: 0.0132  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [ 520/4572]  eta: 0:00:53    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [ 540/4572]  eta: 0:00:53    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 560/4572]  eta: 0:00:53    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 580/4572]  eta: 0:00:53    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 600/4572]  eta: 0:00:52    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 620/4572]  eta: 0:00:52    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 640/4572]  eta: 0:00:52    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 660/4572]  eta: 0:00:51    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 680/4572]  eta: 0:00:51    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 700/4572]  eta: 0:00:51    time: 0.0132  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 720/4572]  eta: 0:00:51    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 740/4572]  eta: 0:00:50    time: 0.0139  data: 0.0129  max mem: 14785
 0: Epoch: [0]  [ 760/4572]  eta: 0:00:50    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [ 780/4572]  eta: 0:00:50    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [ 800/4572]  eta: 0:00:50    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 820/4572]  eta: 0:00:49    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 840/4572]  eta: 0:00:49    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 860/4572]  eta: 0:00:49    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 880/4572]  eta: 0:00:49    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 900/4572]  eta: 0:00:48    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 920/4572]  eta: 0:00:48    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [ 940/4572]  eta: 0:00:48    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [ 960/4572]  eta: 0:00:47    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [ 980/4572]  eta: 0:00:47    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1000/4572]  eta: 0:00:47    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [1020/4572]  eta: 0:00:47    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1040/4572]  eta: 0:00:46    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1060/4572]  eta: 0:00:46    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1080/4572]  eta: 0:00:46    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1100/4572]  eta: 0:00:46    time: 0.0130  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [1120/4572]  eta: 0:00:45    time: 0.0132  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [1140/4572]  eta: 0:00:45    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1160/4572]  eta: 0:00:45    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1180/4572]  eta: 0:00:44    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1200/4572]  eta: 0:00:44    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [1220/4572]  eta: 0:00:44    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1240/4572]  eta: 0:00:44    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [1260/4572]  eta: 0:00:43    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1280/4572]  eta: 0:00:43    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1300/4572]  eta: 0:00:43    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1320/4572]  eta: 0:00:43    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1340/4572]  eta: 0:00:42    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [1360/4572]  eta: 0:00:42    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1380/4572]  eta: 0:00:42    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1400/4572]  eta: 0:00:41    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1420/4572]  eta: 0:00:41    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1440/4572]  eta: 0:00:41    time: 0.0137  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [1460/4572]  eta: 0:00:41    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1480/4572]  eta: 0:00:40    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1500/4572]  eta: 0:00:40    time: 0.0137  data: 0.0129  max mem: 14785
 0: Epoch: [0]  [1520/4572]  eta: 0:00:40    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [1540/4572]  eta: 0:00:40    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [1560/4572]  eta: 0:00:39    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1580/4572]  eta: 0:00:39    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1600/4572]  eta: 0:00:39    time: 0.0133  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1620/4572]  eta: 0:00:39    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [1640/4572]  eta: 0:00:38    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [1660/4572]  eta: 0:00:38    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [1680/4572]  eta: 0:00:38    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [1700/4572]  eta: 0:00:38    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1720/4572]  eta: 0:00:37    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1740/4572]  eta: 0:00:37    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1760/4572]  eta: 0:00:37    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1780/4572]  eta: 0:00:36    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1800/4572]  eta: 0:00:36    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1820/4572]  eta: 0:00:36    time: 0.0133  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [1840/4572]  eta: 0:00:36    time: 0.0138  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [1860/4572]  eta: 0:00:35    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [1880/4572]  eta: 0:00:35    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [1900/4572]  eta: 0:00:35    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [1920/4572]  eta: 0:00:35    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1940/4572]  eta: 0:00:34    time: 0.0141  data: 0.0130  max mem: 14785
 0: Epoch: [0]  [1960/4572]  eta: 0:00:34    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [1980/4572]  eta: 0:00:34    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2000/4572]  eta: 0:00:34    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2020/4572]  eta: 0:00:33    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [2040/4572]  eta: 0:00:33    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2060/4572]  eta: 0:00:33    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [2080/4572]  eta: 0:00:33    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2100/4572]  eta: 0:00:32    time: 0.0137  data: 0.0129  max mem: 14785
 0: Epoch: [0]  [2120/4572]  eta: 0:00:32    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2140/4572]  eta: 0:00:32    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [2160/4572]  eta: 0:00:31    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [2180/4572]  eta: 0:00:31    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [2200/4572]  eta: 0:00:31    time: 0.0140  data: 0.0127  max mem: 14785
 0: Epoch: [0]  [2220/4572]  eta: 0:00:31    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2240/4572]  eta: 0:00:30    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2260/4572]  eta: 0:00:30    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2280/4572]  eta: 0:00:30    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2300/4572]  eta: 0:00:30    time: 0.0137  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [2320/4572]  eta: 0:00:29    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [2340/4572]  eta: 0:00:29    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2360/4572]  eta: 0:00:29    time: 0.0134  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2380/4572]  eta: 0:00:29    time: 0.0142  data: 0.0128  max mem: 14785
 0: Epoch: [0]  [2400/4572]  eta: 0:00:28    time: 0.0134  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2420/4572]  eta: 0:00:28    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2440/4572]  eta: 0:00:28    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2460/4572]  eta: 0:00:28    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2480/4572]  eta: 0:00:27    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2500/4572]  eta: 0:00:27    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2520/4572]  eta: 0:00:27    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2540/4572]  eta: 0:00:26    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2560/4572]  eta: 0:00:26    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2580/4572]  eta: 0:00:26    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2600/4572]  eta: 0:00:26    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2620/4572]  eta: 0:00:25    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2640/4572]  eta: 0:00:25    time: 0.0133  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2660/4572]  eta: 0:00:25    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [2680/4572]  eta: 0:00:25    time: 0.0139  data: 0.0131  max mem: 14785
 0: Epoch: [0]  [2700/4572]  eta: 0:00:24    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [2720/4572]  eta: 0:00:24    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [2740/4572]  eta: 0:00:24    time: 0.0145  data: 0.0137  max mem: 14785
 0: Epoch: [0]  [2760/4572]  eta: 0:00:24    time: 0.0131  data: 0.0118  max mem: 14785
 0: Epoch: [0]  [2780/4572]  eta: 0:00:23    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2800/4572]  eta: 0:00:23    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [2820/4572]  eta: 0:00:23    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2840/4572]  eta: 0:00:22    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [2860/4572]  eta: 0:00:22    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [2880/4572]  eta: 0:00:22    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [2900/4572]  eta: 0:00:22    time: 0.0159  data: 0.0148  max mem: 14785
 0: Epoch: [0]  [2920/4572]  eta: 0:00:21    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2940/4572]  eta: 0:00:21    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [2960/4572]  eta: 0:00:21    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [2980/4572]  eta: 0:00:21    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [3000/4572]  eta: 0:00:20    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3020/4572]  eta: 0:00:20    time: 0.0132  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [3040/4572]  eta: 0:00:20    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3060/4572]  eta: 0:00:20    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3080/4572]  eta: 0:00:19    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3100/4572]  eta: 0:00:19    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3120/4572]  eta: 0:00:19    time: 0.0129  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [3140/4572]  eta: 0:00:19    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [3160/4572]  eta: 0:00:18    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3180/4572]  eta: 0:00:18    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3200/4572]  eta: 0:00:18    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3220/4572]  eta: 0:00:17    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [3240/4572]  eta: 0:00:17    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3260/4572]  eta: 0:00:17    time: 0.0136  data: 0.0126  max mem: 14785
 0: Epoch: [0]  [3280/4572]  eta: 0:00:17    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [3300/4572]  eta: 0:00:16    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [3320/4572]  eta: 0:00:16    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [0]  [3340/4572]  eta: 0:00:16    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3360/4572]  eta: 0:00:16    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3380/4572]  eta: 0:00:15    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3400/4572]  eta: 0:00:15    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [3420/4572]  eta: 0:00:15    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3440/4572]  eta: 0:00:15    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [3460/4572]  eta: 0:00:14    time: 0.0141  data: 0.0131  max mem: 14785
 0: Epoch: [0]  [3480/4572]  eta: 0:00:14    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [3500/4572]  eta: 0:00:14    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3520/4572]  eta: 0:00:13    time: 0.0132  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [3540/4572]  eta: 0:00:13    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3560/4572]  eta: 0:00:13    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3580/4572]  eta: 0:00:13    time: 0.0132  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [3600/4572]  eta: 0:00:12    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3620/4572]  eta: 0:00:12    time: 0.0134  data: 0.0127  max mem: 14785
 0: Epoch: [0]  [3640/4572]  eta: 0:00:12    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3660/4572]  eta: 0:00:12    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3680/4572]  eta: 0:00:11    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3700/4572]  eta: 0:00:11    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3720/4572]  eta: 0:00:11    time: 0.0140  data: 0.0128  max mem: 14785
 0: Epoch: [0]  [3740/4572]  eta: 0:00:11    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [3760/4572]  eta: 0:00:10    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [3780/4572]  eta: 0:00:10    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [3800/4572]  eta: 0:00:10    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3820/4572]  eta: 0:00:09    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3840/4572]  eta: 0:00:09    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3860/4572]  eta: 0:00:09    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [3880/4572]  eta: 0:00:09    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [3900/4572]  eta: 0:00:08    time: 0.0131  data: 0.0122  max mem: 14785
 0: Epoch: [0]  [3920/4572]  eta: 0:00:08    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3940/4572]  eta: 0:00:08    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [3960/4572]  eta: 0:00:08    time: 0.0133  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [3980/4572]  eta: 0:00:07    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4000/4572]  eta: 0:00:07    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4020/4572]  eta: 0:00:07    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4040/4572]  eta: 0:00:07    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4060/4572]  eta: 0:00:06    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4080/4572]  eta: 0:00:06    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [4100/4572]  eta: 0:00:06    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4120/4572]  eta: 0:00:06    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [4140/4572]  eta: 0:00:05    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [4160/4572]  eta: 0:00:05    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4180/4572]  eta: 0:00:05    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4200/4572]  eta: 0:00:04    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [4220/4572]  eta: 0:00:04    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4240/4572]  eta: 0:00:04    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4260/4572]  eta: 0:00:04    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4280/4572]  eta: 0:00:03    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4300/4572]  eta: 0:00:03    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4320/4572]  eta: 0:00:03    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [4340/4572]  eta: 0:00:03    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4360/4572]  eta: 0:00:02    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4380/4572]  eta: 0:00:02    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4400/4572]  eta: 0:00:02    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4420/4572]  eta: 0:00:02    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4440/4572]  eta: 0:00:01    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [0]  [4460/4572]  eta: 0:00:01    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [0]  [4480/4572]  eta: 0:00:01    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [0]  [4500/4572]  eta: 0:00:00    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [0]  [4520/4572]  eta: 0:00:00    time: 0.0132  data: 0.0119  max mem: 14785
 0: Epoch: [0]  [4540/4572]  eta: 0:00:00    time: 0.0136  data: 0.0127  max mem: 14785
 0: Epoch: [0]  [4560/4572]  eta: 0:00:00    time: 0.0134  data: 0.0125  max mem: 14785
 0: Epoch: [0]  [4571/4572]  eta: 0:00:00    time: 0.0141  data: 0.0133  max mem: 14785
 0: Epoch: [0] Total time: 0:01:00 (0.0133 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985219313, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 0, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985219313, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 19263.285646649583, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985219314, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 1}}
 6: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 6:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
19: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
19:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
11: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
11:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 9: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 9:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
16: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
16:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 3: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 3:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
44: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
44:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
41: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
41:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
38: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
38:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
59: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
59:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
12: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
12:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
15: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
15:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
33: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
33:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
29: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
29:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
25: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
25:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
35: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
35:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
18: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
18:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
39: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
39:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
22: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
22:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
13: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
13:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
24: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
24:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
32: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
32:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
17: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
17:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
61: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
61:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
14: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
14:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
26: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
26:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 0: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 0:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
54: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
54:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
60: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
60:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
58: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
58:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
34: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
34:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
48: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
48:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
55: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
55:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 8: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 8:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
50: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
50:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
23: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
23:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
20: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
20:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
37: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
37:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
36: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
36:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 5: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 5:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
43: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
43:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
56: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
56:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
42: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
42:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
21: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
21:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
28: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
28:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
30: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
30:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
62: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
62:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 7: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 7:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
47: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
47:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
27: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
27:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
10: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
10:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
40: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
40:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 2: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 2:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
53: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
53:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
45: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
45:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
51: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
51:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
49: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
49:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 1: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 1:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
46: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
46:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 4: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
 4:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
31: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
31:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
52: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
52:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
63: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
63:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
57: /workspace/ssd/model/transform.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
57:   torch.tensor(s, dtype=torch.float32, device=boxes.device) /
 0: Test:  [ 0/13]  eta: 0:00:06  model_time: 0.4881 (0.4881)  evaluator_time: 0.0060 (0.0060)  time: 0.4960  data: 0.0005  max mem: 14785
 0: Test:  [12/13]  eta: 0:00:00  model_time: 0.3572 (0.3441)  evaluator_time: 0.0057 (0.0053)  time: 0.3504  data: 0.0007  max mem: 14785
 0: Test: Total time: 0:00:04 (0.3504 s / it)
 0: Averaged stats: model_time: 0.3572 (0.3613)  evaluator_time: 0.0057 (0.0054)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985224769, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 1}}
 0: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
32: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
48: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
41: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
24: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
40: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
35: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 3: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 9: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
26: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
42: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 8: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
16: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
44: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
56: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
17: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
18: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 1: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
20: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
19: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
60: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 5: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
11: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
57: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
10: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
58: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
59: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
62: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
25: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
43: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
33: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
27: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
14: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
28: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 2: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
34: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
29: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
49: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
38: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 6: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
36: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
12: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
50: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
45: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
23: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 4: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
15: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
30: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
51: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
52: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
22: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
54: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
53: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
13: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
55: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
37: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
46: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 7: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
21: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
47: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
61: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
31: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
39: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
63: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: Epoch: [1]  [   0/4571]  eta: 0:00:14    time: 0.0031  data: 0.0009  max mem: 14785
 0: Epoch: [1]  [  20/4571]  eta: 0:00:56    time: 0.0129  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [  40/4571]  eta: 0:00:58    time: 0.0134  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [  60/4571]  eta: 0:00:58    time: 0.0130  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [  80/4571]  eta: 0:00:58    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 100/4571]  eta: 0:00:58    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 120/4571]  eta: 0:00:58    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 140/4571]  eta: 0:00:57    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 160/4571]  eta: 0:00:58    time: 0.0140  data: 0.0128  max mem: 14785
 0: Epoch: [1]  [ 180/4571]  eta: 0:00:57    time: 0.0132  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [ 200/4571]  eta: 0:00:57    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [ 220/4571]  eta: 0:00:57    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [ 240/4571]  eta: 0:00:57    time: 0.0135  data: 0.0128  max mem: 14785
 0: Epoch: [1]  [ 260/4571]  eta: 0:00:57    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 280/4571]  eta: 0:00:56    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [ 300/4571]  eta: 0:00:56    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [ 320/4571]  eta: 0:00:56    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 340/4571]  eta: 0:00:55    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [ 360/4571]  eta: 0:00:55    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 380/4571]  eta: 0:00:55    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [ 400/4571]  eta: 0:00:55    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 420/4571]  eta: 0:00:54    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 440/4571]  eta: 0:00:54    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [ 460/4571]  eta: 0:00:54    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [ 480/4571]  eta: 0:00:54    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [ 500/4571]  eta: 0:00:53    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 520/4571]  eta: 0:00:53    time: 0.0136  data: 0.0127  max mem: 14785
 0: Epoch: [1]  [ 540/4571]  eta: 0:00:53    time: 0.0139  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [ 560/4571]  eta: 0:00:53    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [ 580/4571]  eta: 0:00:52    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [ 600/4571]  eta: 0:00:52    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 620/4571]  eta: 0:00:52    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 640/4571]  eta: 0:00:52    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 660/4571]  eta: 0:00:51    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [ 680/4571]  eta: 0:00:51    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 700/4571]  eta: 0:00:51    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 720/4571]  eta: 0:00:50    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [ 740/4571]  eta: 0:00:50    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 760/4571]  eta: 0:00:50    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 780/4571]  eta: 0:00:50    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [ 800/4571]  eta: 0:00:49    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [ 820/4571]  eta: 0:00:49    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 840/4571]  eta: 0:00:49    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [1]  [ 860/4571]  eta: 0:00:49    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 880/4571]  eta: 0:00:48    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [ 900/4571]  eta: 0:00:48    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [ 920/4571]  eta: 0:00:48    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [ 940/4571]  eta: 0:00:48    time: 0.0132  data: 0.0124  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1745985237340, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19573032200848547, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985237341, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 1}}
 0: Epoch: [1]  [ 960/4571]  eta: 0:00:47    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [ 980/4571]  eta: 0:00:47    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1000/4571]  eta: 0:00:47    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1020/4571]  eta: 0:00:47    time: 0.0134  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1040/4571]  eta: 0:00:46    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [1060/4571]  eta: 0:00:46    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [1080/4571]  eta: 0:00:46    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1100/4571]  eta: 0:00:45    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1120/4571]  eta: 0:00:45    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1140/4571]  eta: 0:00:45    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1160/4571]  eta: 0:00:45    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1180/4571]  eta: 0:00:44    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [1200/4571]  eta: 0:00:44    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [1220/4571]  eta: 0:00:44    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1240/4571]  eta: 0:00:44    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1260/4571]  eta: 0:00:43    time: 0.0133  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1280/4571]  eta: 0:00:43    time: 0.0130  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [1300/4571]  eta: 0:00:43    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [1320/4571]  eta: 0:00:43    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [1340/4571]  eta: 0:00:42    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1360/4571]  eta: 0:00:42    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1380/4571]  eta: 0:00:42    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1400/4571]  eta: 0:00:41    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1420/4571]  eta: 0:00:41    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1440/4571]  eta: 0:00:41    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [1460/4571]  eta: 0:00:41    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1480/4571]  eta: 0:00:40    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [1500/4571]  eta: 0:00:40    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1520/4571]  eta: 0:00:40    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1540/4571]  eta: 0:00:40    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [1560/4571]  eta: 0:00:39    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1580/4571]  eta: 0:00:39    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1600/4571]  eta: 0:00:39    time: 0.0134  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1620/4571]  eta: 0:00:39    time: 0.0130  data: 0.0118  max mem: 14785
 0: Epoch: [1]  [1640/4571]  eta: 0:00:38    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1660/4571]  eta: 0:00:38    time: 0.0133  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [1680/4571]  eta: 0:00:38    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1700/4571]  eta: 0:00:37    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [1720/4571]  eta: 0:00:37    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [1740/4571]  eta: 0:00:37    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1760/4571]  eta: 0:00:37    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1780/4571]  eta: 0:00:36    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1800/4571]  eta: 0:00:36    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1820/4571]  eta: 0:00:36    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [1840/4571]  eta: 0:00:36    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [1860/4571]  eta: 0:00:35    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1880/4571]  eta: 0:00:35    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [1900/4571]  eta: 0:00:35    time: 0.0137  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1920/4571]  eta: 0:00:35    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [1940/4571]  eta: 0:00:34    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [1960/4571]  eta: 0:00:34    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [1980/4571]  eta: 0:00:34    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [2000/4571]  eta: 0:00:34    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2020/4571]  eta: 0:00:33    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2040/4571]  eta: 0:00:33    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2060/4571]  eta: 0:00:33    time: 0.0130  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2080/4571]  eta: 0:00:32    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [2100/4571]  eta: 0:00:32    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2120/4571]  eta: 0:00:32    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [2140/4571]  eta: 0:00:32    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2160/4571]  eta: 0:00:31    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [2180/4571]  eta: 0:00:31    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2200/4571]  eta: 0:00:31    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [1]  [2220/4571]  eta: 0:00:31    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [2240/4571]  eta: 0:00:30    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2260/4571]  eta: 0:00:30    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2280/4571]  eta: 0:00:30    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2300/4571]  eta: 0:00:30    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [2320/4571]  eta: 0:00:29    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [2340/4571]  eta: 0:00:29    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [2360/4571]  eta: 0:00:29    time: 0.0129  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [2380/4571]  eta: 0:00:28    time: 0.0135  data: 0.0128  max mem: 14785
 0: Epoch: [1]  [2400/4571]  eta: 0:00:28    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2420/4571]  eta: 0:00:28    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2440/4571]  eta: 0:00:28    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [2460/4571]  eta: 0:00:27    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2480/4571]  eta: 0:00:27    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2500/4571]  eta: 0:00:27    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2520/4571]  eta: 0:00:27    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2540/4571]  eta: 0:00:26    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [2560/4571]  eta: 0:00:26    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2580/4571]  eta: 0:00:26    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2600/4571]  eta: 0:00:26    time: 0.0133  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [2620/4571]  eta: 0:00:25    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [2640/4571]  eta: 0:00:25    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [2660/4571]  eta: 0:00:25    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2680/4571]  eta: 0:00:25    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2700/4571]  eta: 0:00:24    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [2720/4571]  eta: 0:00:24    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2740/4571]  eta: 0:00:24    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [2760/4571]  eta: 0:00:23    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [2780/4571]  eta: 0:00:23    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [2800/4571]  eta: 0:00:23    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2820/4571]  eta: 0:00:23    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [2840/4571]  eta: 0:00:22    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2860/4571]  eta: 0:00:22    time: 0.0135  data: 0.0128  max mem: 14785
 0: Epoch: [1]  [2880/4571]  eta: 0:00:22    time: 0.0130  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2900/4571]  eta: 0:00:22    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [2920/4571]  eta: 0:00:21    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [2940/4571]  eta: 0:00:21    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [2960/4571]  eta: 0:00:21    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [2980/4571]  eta: 0:00:21    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3000/4571]  eta: 0:00:20    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3020/4571]  eta: 0:00:20    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3040/4571]  eta: 0:00:20    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [3060/4571]  eta: 0:00:20    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3080/4571]  eta: 0:00:19    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3100/4571]  eta: 0:00:19    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3120/4571]  eta: 0:00:19    time: 0.0139  data: 0.0128  max mem: 14785
 0: Epoch: [1]  [3140/4571]  eta: 0:00:18    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3160/4571]  eta: 0:00:18    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [3180/4571]  eta: 0:00:18    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3200/4571]  eta: 0:00:18    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3220/4571]  eta: 0:00:17    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [3240/4571]  eta: 0:00:17    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [3260/4571]  eta: 0:00:17    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3280/4571]  eta: 0:00:17    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3300/4571]  eta: 0:00:16    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [3320/4571]  eta: 0:00:16    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [3340/4571]  eta: 0:00:16    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3360/4571]  eta: 0:00:16    time: 0.0133  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [3380/4571]  eta: 0:00:15    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [3400/4571]  eta: 0:00:15    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3420/4571]  eta: 0:00:15    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [3440/4571]  eta: 0:00:14    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3460/4571]  eta: 0:00:14    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3480/4571]  eta: 0:00:14    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [3500/4571]  eta: 0:00:14    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3520/4571]  eta: 0:00:13    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3540/4571]  eta: 0:00:13    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [3560/4571]  eta: 0:00:13    time: 0.0137  data: 0.0130  max mem: 14785
 0: Epoch: [1]  [3580/4571]  eta: 0:00:13    time: 0.0138  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [3600/4571]  eta: 0:00:12    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [3620/4571]  eta: 0:00:12    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3640/4571]  eta: 0:00:12    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [1]  [3660/4571]  eta: 0:00:12    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3680/4571]  eta: 0:00:11    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [3700/4571]  eta: 0:00:11    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [3720/4571]  eta: 0:00:11    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3740/4571]  eta: 0:00:11    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3760/4571]  eta: 0:00:10    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3780/4571]  eta: 0:00:10    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [3800/4571]  eta: 0:00:10    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3820/4571]  eta: 0:00:09    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [3840/4571]  eta: 0:00:09    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [3860/4571]  eta: 0:00:09    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3880/4571]  eta: 0:00:09    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3900/4571]  eta: 0:00:08    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3920/4571]  eta: 0:00:08    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [3940/4571]  eta: 0:00:08    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [3960/4571]  eta: 0:00:08    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [3980/4571]  eta: 0:00:07    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4000/4571]  eta: 0:00:07    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4020/4571]  eta: 0:00:07    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4040/4571]  eta: 0:00:07    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [4060/4571]  eta: 0:00:06    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [1]  [4080/4571]  eta: 0:00:06    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4100/4571]  eta: 0:00:06    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [4120/4571]  eta: 0:00:05    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4140/4571]  eta: 0:00:05    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4160/4571]  eta: 0:00:05    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [4180/4571]  eta: 0:00:05    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [4200/4571]  eta: 0:00:04    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4220/4571]  eta: 0:00:04    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4240/4571]  eta: 0:00:04    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [4260/4571]  eta: 0:00:04    time: 0.0137  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [4280/4571]  eta: 0:00:03    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4300/4571]  eta: 0:00:03    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [1]  [4320/4571]  eta: 0:00:03    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [1]  [4340/4571]  eta: 0:00:03    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [1]  [4360/4571]  eta: 0:00:02    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4380/4571]  eta: 0:00:02    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4400/4571]  eta: 0:00:02    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4420/4571]  eta: 0:00:02    time: 0.0135  data: 0.0127  max mem: 14785
 0: Epoch: [1]  [4440/4571]  eta: 0:00:01    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4460/4571]  eta: 0:00:01    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [4480/4571]  eta: 0:00:01    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [4500/4571]  eta: 0:00:00    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [1]  [4520/4571]  eta: 0:00:00    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4540/4571]  eta: 0:00:00    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [1]  [4560/4571]  eta: 0:00:00    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [1]  [4570/4571]  eta: 0:00:00    time: 0.0147  data: 0.0135  max mem: 14785
 0: Epoch: [1] Total time: 0:01:00 (0.0133 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985285484, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 1, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985285484, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 19292.73094371693, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 2}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985285485, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 2}}
 0: Test:  [ 0/13]  eta: 0:00:04  model_time: 0.3157 (0.3157)  evaluator_time: 0.0039 (0.0039)  time: 0.3208  data: 0.0010  max mem: 14785
 0: Test:  [12/13]  eta: 0:00:00  model_time: 0.3065 (0.2854)  evaluator_time: 0.0040 (0.0038)  time: 0.2902  data: 0.0008  max mem: 14785
 0: Test: Total time: 0:00:03 (0.2902 s / it)
 0: Averaged stats: model_time: 0.3065 (0.2853)  evaluator_time: 0.0040 (0.0039)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985289759, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 2}}
 0: Epoch: [2]  [   0/4572]  eta: 0:00:11    time: 0.0026  data: 0.0008  max mem: 14785
 0: Epoch: [2]  [  20/4572]  eta: 0:00:56    time: 0.0129  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [  40/4572]  eta: 0:00:57    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [2]  [  60/4572]  eta: 0:00:58    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [  80/4572]  eta: 0:00:58    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [ 100/4572]  eta: 0:00:58    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 120/4572]  eta: 0:00:58    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 140/4572]  eta: 0:00:57    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 160/4572]  eta: 0:00:57    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 180/4572]  eta: 0:00:57    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 200/4572]  eta: 0:00:57    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 220/4572]  eta: 0:00:56    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 240/4572]  eta: 0:00:56    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 260/4572]  eta: 0:00:56    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 280/4572]  eta: 0:00:56    time: 0.0134  data: 0.0127  max mem: 14785
 0: Epoch: [2]  [ 300/4572]  eta: 0:00:56    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 320/4572]  eta: 0:00:55    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 340/4572]  eta: 0:00:55    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 360/4572]  eta: 0:00:55    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 380/4572]  eta: 0:00:55    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 400/4572]  eta: 0:00:54    time: 0.0132  data: 0.0120  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1745985295280, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2750666911463832, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 2}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985295280, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 2}}
 0: Epoch: [2]  [ 420/4572]  eta: 0:00:54    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [2]  [ 440/4572]  eta: 0:00:54    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 460/4572]  eta: 0:00:54    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [ 480/4572]  eta: 0:00:53    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 500/4572]  eta: 0:00:53    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 520/4572]  eta: 0:00:53    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 540/4572]  eta: 0:00:53    time: 0.0138  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [ 560/4572]  eta: 0:00:52    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 580/4572]  eta: 0:00:52    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 600/4572]  eta: 0:00:52    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 620/4572]  eta: 0:00:52    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [ 640/4572]  eta: 0:00:51    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 660/4572]  eta: 0:00:51    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 680/4572]  eta: 0:00:51    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 700/4572]  eta: 0:00:51    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [ 720/4572]  eta: 0:00:50    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 740/4572]  eta: 0:00:50    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [ 760/4572]  eta: 0:00:50    time: 0.0138  data: 0.0127  max mem: 14785
 0: Epoch: [2]  [ 780/4572]  eta: 0:00:50    time: 0.0134  data: 0.0127  max mem: 14785
 0: Epoch: [2]  [ 800/4572]  eta: 0:00:49    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 820/4572]  eta: 0:00:49    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 840/4572]  eta: 0:00:49    time: 0.0128  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 860/4572]  eta: 0:00:49    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [2]  [ 880/4572]  eta: 0:00:48    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [ 900/4572]  eta: 0:00:48    time: 0.0143  data: 0.0136  max mem: 14785
 0: Epoch: [2]  [ 920/4572]  eta: 0:00:48    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 940/4572]  eta: 0:00:48    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 960/4572]  eta: 0:00:47    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [ 980/4572]  eta: 0:00:47    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1000/4572]  eta: 0:00:47    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [1020/4572]  eta: 0:00:47    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [1040/4572]  eta: 0:00:46    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [1060/4572]  eta: 0:00:46    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1080/4572]  eta: 0:00:46    time: 0.0138  data: 0.0131  max mem: 14785
 0: Epoch: [2]  [1100/4572]  eta: 0:00:46    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1120/4572]  eta: 0:00:45    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [1140/4572]  eta: 0:00:45    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [1160/4572]  eta: 0:00:45    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [1180/4572]  eta: 0:00:44    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1200/4572]  eta: 0:00:44    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [1220/4572]  eta: 0:00:44    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1240/4572]  eta: 0:00:44    time: 0.0134  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [1260/4572]  eta: 0:00:43    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [2]  [1280/4572]  eta: 0:00:43    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [1300/4572]  eta: 0:00:43    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1320/4572]  eta: 0:00:43    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [1340/4572]  eta: 0:00:42    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [2]  [1360/4572]  eta: 0:00:42    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [1380/4572]  eta: 0:00:42    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1400/4572]  eta: 0:00:42    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1420/4572]  eta: 0:00:41    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [1440/4572]  eta: 0:00:41    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1460/4572]  eta: 0:00:41    time: 0.0133  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [1480/4572]  eta: 0:00:40    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1500/4572]  eta: 0:00:40    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [1520/4572]  eta: 0:00:40    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [2]  [1540/4572]  eta: 0:00:40    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1560/4572]  eta: 0:00:39    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1580/4572]  eta: 0:00:39    time: 0.0149  data: 0.0137  max mem: 14785
 0: Epoch: [2]  [1600/4572]  eta: 0:00:39    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1620/4572]  eta: 0:00:39    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [1640/4572]  eta: 0:00:38    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [1660/4572]  eta: 0:00:38    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1680/4572]  eta: 0:00:38    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1700/4572]  eta: 0:00:38    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1720/4572]  eta: 0:00:37    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1740/4572]  eta: 0:00:37    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1760/4572]  eta: 0:00:37    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1780/4572]  eta: 0:00:37    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [1800/4572]  eta: 0:00:36    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [1820/4572]  eta: 0:00:36    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1840/4572]  eta: 0:00:36    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1860/4572]  eta: 0:00:35    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1880/4572]  eta: 0:00:35    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1900/4572]  eta: 0:00:35    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1920/4572]  eta: 0:00:35    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1940/4572]  eta: 0:00:34    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [1960/4572]  eta: 0:00:34    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [1980/4572]  eta: 0:00:34    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [2000/4572]  eta: 0:00:34    time: 0.0148  data: 0.0137  max mem: 14785
 0: Epoch: [2]  [2020/4572]  eta: 0:00:33    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2040/4572]  eta: 0:00:33    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2060/4572]  eta: 0:00:33    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [2080/4572]  eta: 0:00:33    time: 0.0134  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [2100/4572]  eta: 0:00:32    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2120/4572]  eta: 0:00:32    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2140/4572]  eta: 0:00:32    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2160/4572]  eta: 0:00:32    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2180/4572]  eta: 0:00:31    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2200/4572]  eta: 0:00:31    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2220/4572]  eta: 0:00:31    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2240/4572]  eta: 0:00:30    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2260/4572]  eta: 0:00:30    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [2280/4572]  eta: 0:00:30    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2300/4572]  eta: 0:00:30    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2320/4572]  eta: 0:00:29    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2340/4572]  eta: 0:00:29    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2360/4572]  eta: 0:00:29    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [2380/4572]  eta: 0:00:29    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [2400/4572]  eta: 0:00:28    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2420/4572]  eta: 0:00:28    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [2440/4572]  eta: 0:00:28    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [2460/4572]  eta: 0:00:28    time: 0.0134  data: 0.0127  max mem: 14785
 0: Epoch: [2]  [2480/4572]  eta: 0:00:27    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2500/4572]  eta: 0:00:27    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2520/4572]  eta: 0:00:27    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [2540/4572]  eta: 0:00:26    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2560/4572]  eta: 0:00:26    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [2580/4572]  eta: 0:00:26    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2600/4572]  eta: 0:00:26    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [2620/4572]  eta: 0:00:25    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [2640/4572]  eta: 0:00:25    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [2660/4572]  eta: 0:00:25    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2680/4572]  eta: 0:00:25    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2700/4572]  eta: 0:00:24    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [2720/4572]  eta: 0:00:24    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2740/4572]  eta: 0:00:24    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2760/4572]  eta: 0:00:24    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2780/4572]  eta: 0:00:23    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2800/4572]  eta: 0:00:23    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [2820/4572]  eta: 0:00:23    time: 0.0134  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [2840/4572]  eta: 0:00:22    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2860/4572]  eta: 0:00:22    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [2880/4572]  eta: 0:00:22    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2900/4572]  eta: 0:00:22    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2920/4572]  eta: 0:00:21    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [2940/4572]  eta: 0:00:21    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [2960/4572]  eta: 0:00:21    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [2980/4572]  eta: 0:00:21    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3000/4572]  eta: 0:00:20    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3020/4572]  eta: 0:00:20    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3040/4572]  eta: 0:00:20    time: 0.0129  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [3060/4572]  eta: 0:00:20    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [3080/4572]  eta: 0:00:19    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3100/4572]  eta: 0:00:19    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3120/4572]  eta: 0:00:19    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [3140/4572]  eta: 0:00:19    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [3160/4572]  eta: 0:00:18    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3180/4572]  eta: 0:00:18    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3200/4572]  eta: 0:00:18    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [2]  [3220/4572]  eta: 0:00:17    time: 0.0141  data: 0.0130  max mem: 14785
 0: Epoch: [2]  [3240/4572]  eta: 0:00:17    time: 0.0128  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3260/4572]  eta: 0:00:17    time: 0.0138  data: 0.0127  max mem: 14785
 0: Epoch: [2]  [3280/4572]  eta: 0:00:17    time: 0.0135  data: 0.0126  max mem: 14785
 0: Epoch: [2]  [3300/4572]  eta: 0:00:16    time: 0.0140  data: 0.0133  max mem: 14785
 0: Epoch: [2]  [3320/4572]  eta: 0:00:16    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [3340/4572]  eta: 0:00:16    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3360/4572]  eta: 0:00:16    time: 0.0139  data: 0.0128  max mem: 14785
 0: Epoch: [2]  [3380/4572]  eta: 0:00:15    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [3400/4572]  eta: 0:00:15    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3420/4572]  eta: 0:00:15    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [3440/4572]  eta: 0:00:15    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3460/4572]  eta: 0:00:14    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3480/4572]  eta: 0:00:14    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3500/4572]  eta: 0:00:14    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3520/4572]  eta: 0:00:13    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3540/4572]  eta: 0:00:13    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3560/4572]  eta: 0:00:13    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3580/4572]  eta: 0:00:13    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3600/4572]  eta: 0:00:12    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3620/4572]  eta: 0:00:12    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3640/4572]  eta: 0:00:12    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3660/4572]  eta: 0:00:12    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3680/4572]  eta: 0:00:11    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [3700/4572]  eta: 0:00:11    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [3720/4572]  eta: 0:00:11    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [2]  [3740/4572]  eta: 0:00:11    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [3760/4572]  eta: 0:00:10    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3780/4572]  eta: 0:00:10    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [3800/4572]  eta: 0:00:10    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3820/4572]  eta: 0:00:09    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3840/4572]  eta: 0:00:09    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [3860/4572]  eta: 0:00:09    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3880/4572]  eta: 0:00:09    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [3900/4572]  eta: 0:00:08    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3920/4572]  eta: 0:00:08    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [3940/4572]  eta: 0:00:08    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [3960/4572]  eta: 0:00:08    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [3980/4572]  eta: 0:00:07    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4000/4572]  eta: 0:00:07    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4020/4572]  eta: 0:00:07    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4040/4572]  eta: 0:00:07    time: 0.0131  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [4060/4572]  eta: 0:00:06    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [4080/4572]  eta: 0:00:06    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4100/4572]  eta: 0:00:06    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [4120/4572]  eta: 0:00:05    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [4140/4572]  eta: 0:00:05    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4160/4572]  eta: 0:00:05    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [4180/4572]  eta: 0:00:05    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [4200/4572]  eta: 0:00:04    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4220/4572]  eta: 0:00:04    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [4240/4572]  eta: 0:00:04    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [2]  [4260/4572]  eta: 0:00:04    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4280/4572]  eta: 0:00:03    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4300/4572]  eta: 0:00:03    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4320/4572]  eta: 0:00:03    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [4340/4572]  eta: 0:00:03    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4360/4572]  eta: 0:00:02    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [4380/4572]  eta: 0:00:02    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [2]  [4400/4572]  eta: 0:00:02    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [4420/4572]  eta: 0:00:02    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [4440/4572]  eta: 0:00:01    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4460/4572]  eta: 0:00:01    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [4480/4572]  eta: 0:00:01    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [4500/4572]  eta: 0:00:00    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [2]  [4520/4572]  eta: 0:00:00    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [2]  [4540/4572]  eta: 0:00:00    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [2]  [4560/4572]  eta: 0:00:00    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [2]  [4571/4572]  eta: 0:00:00    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [2] Total time: 0:01:00 (0.0133 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985350558, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 2, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 2}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985350559, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 19271.90641912746, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 3}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985350559, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 3}}
 0: Test:  [ 0/13]  eta: 0:00:03  model_time: 0.2728 (0.2728)  evaluator_time: 0.0034 (0.0034)  time: 0.2773  data: 0.0010  max mem: 14785
 0: Test:  [12/13]  eta: 0:00:00  model_time: 0.2853 (0.2671)  evaluator_time: 0.0038 (0.0036)  time: 0.2717  data: 0.0008  max mem: 14785
 0: Test: Total time: 0:00:03 (0.2717 s / it)
 0: Averaged stats: model_time: 0.2853 (0.2659)  evaluator_time: 0.0038 (0.0052)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985354657, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 3}}
32: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
48: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
35: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
24: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 1: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 9: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
26: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
56: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
16: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 8: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
18: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
57: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
20: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
17: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
58: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
60: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
41: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
40: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
19: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
42: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
44: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
25: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
34: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
11: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
59: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
27: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
49: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 5: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
33: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
10: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
62: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
50: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
38: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 2: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
36: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
52: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 3: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
28: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 6: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
29: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
51: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 4: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
37: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
14: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
54: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 7: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
12: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
55: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
15: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
53: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
43: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
13: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
30: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
39: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
61: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
45: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
23: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
21: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
31: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
46: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
22: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
63: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
47: WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
 0: Epoch: [3]  [   0/4571]  eta: 0:00:12    time: 0.0027  data: 0.0009  max mem: 14785
 0: Epoch: [3]  [  20/4571]  eta: 0:00:56    time: 0.0128  data: 0.0117  max mem: 14785
 0: Epoch: [3]  [  40/4571]  eta: 0:00:57    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [  60/4571]  eta: 0:00:57    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [  80/4571]  eta: 0:00:58    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [ 100/4571]  eta: 0:00:58    time: 0.0136  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [ 120/4571]  eta: 0:00:58    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [ 140/4571]  eta: 0:00:58    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 160/4571]  eta: 0:00:58    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [ 180/4571]  eta: 0:00:58    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 200/4571]  eta: 0:00:57    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 220/4571]  eta: 0:00:58    time: 0.0168  data: 0.0156  max mem: 14785
 0: Epoch: [3]  [ 240/4571]  eta: 0:00:58    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [ 260/4571]  eta: 0:00:58    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [ 280/4571]  eta: 0:00:57    time: 0.0130  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [ 300/4571]  eta: 0:00:57    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [ 320/4571]  eta: 0:00:56    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [ 340/4571]  eta: 0:00:56    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [ 360/4571]  eta: 0:00:56    time: 0.0135  data: 0.0124  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1745985359660, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3264233917904958, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 3}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985359660, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 3}}
 0: Epoch: [3]  [ 380/4571]  eta: 0:00:56    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 400/4571]  eta: 0:00:55    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 420/4571]  eta: 0:00:55    time: 0.0136  data: 0.0129  max mem: 14785
 0: Epoch: [3]  [ 440/4571]  eta: 0:00:55    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [ 460/4571]  eta: 0:00:55    time: 0.0138  data: 0.0131  max mem: 14785
 0: Epoch: [3]  [ 480/4571]  eta: 0:00:54    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [ 500/4571]  eta: 0:00:54    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [ 520/4571]  eta: 0:00:54    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 540/4571]  eta: 0:00:53    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [ 560/4571]  eta: 0:00:53    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 580/4571]  eta: 0:00:53    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 600/4571]  eta: 0:00:53    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 620/4571]  eta: 0:00:52    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [ 640/4571]  eta: 0:00:52    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 660/4571]  eta: 0:00:52    time: 0.0135  data: 0.0128  max mem: 14785
 0: Epoch: [3]  [ 680/4571]  eta: 0:00:51    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [ 700/4571]  eta: 0:00:51    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 720/4571]  eta: 0:00:51    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 740/4571]  eta: 0:00:51    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 760/4571]  eta: 0:00:50    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 780/4571]  eta: 0:00:50    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 800/4571]  eta: 0:00:50    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [ 820/4571]  eta: 0:00:49    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [ 840/4571]  eta: 0:00:49    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [ 860/4571]  eta: 0:00:49    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 880/4571]  eta: 0:00:49    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 900/4571]  eta: 0:00:48    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [ 920/4571]  eta: 0:00:48    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [ 940/4571]  eta: 0:00:48    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [ 960/4571]  eta: 0:00:48    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [ 980/4571]  eta: 0:00:47    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [1000/4571]  eta: 0:00:47    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1020/4571]  eta: 0:00:47    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1040/4571]  eta: 0:00:46    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [1060/4571]  eta: 0:00:46    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [1080/4571]  eta: 0:00:46    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1100/4571]  eta: 0:00:46    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1120/4571]  eta: 0:00:45    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1140/4571]  eta: 0:00:45    time: 0.0138  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [1160/4571]  eta: 0:00:45    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1180/4571]  eta: 0:00:45    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1200/4571]  eta: 0:00:44    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [1220/4571]  eta: 0:00:44    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1240/4571]  eta: 0:00:44    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1260/4571]  eta: 0:00:44    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1280/4571]  eta: 0:00:43    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1300/4571]  eta: 0:00:43    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [1320/4571]  eta: 0:00:43    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [1340/4571]  eta: 0:00:42    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [1360/4571]  eta: 0:00:42    time: 0.0183  data: 0.0175  max mem: 14785
 0: Epoch: [3]  [1380/4571]  eta: 0:00:42    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1400/4571]  eta: 0:00:42    time: 0.0180  data: 0.0170  max mem: 14785
 0: Epoch: [3]  [1420/4571]  eta: 0:00:42    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [1440/4571]  eta: 0:00:42    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1460/4571]  eta: 0:00:41    time: 0.0131  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [1480/4571]  eta: 0:00:41    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [1500/4571]  eta: 0:00:41    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1520/4571]  eta: 0:00:40    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1540/4571]  eta: 0:00:40    time: 0.0131  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1560/4571]  eta: 0:00:40    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1580/4571]  eta: 0:00:40    time: 0.0140  data: 0.0129  max mem: 14785
 0: Epoch: [3]  [1600/4571]  eta: 0:00:39    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1620/4571]  eta: 0:00:39    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1640/4571]  eta: 0:00:39    time: 0.0143  data: 0.0135  max mem: 14785
 0: Epoch: [3]  [1660/4571]  eta: 0:00:39    time: 0.0134  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [1680/4571]  eta: 0:00:38    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1700/4571]  eta: 0:00:38    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [1720/4571]  eta: 0:00:38    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [1740/4571]  eta: 0:00:38    time: 0.0172  data: 0.0161  max mem: 14785
 0: Epoch: [3]  [1760/4571]  eta: 0:00:37    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [1780/4571]  eta: 0:00:37    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1800/4571]  eta: 0:00:37    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1820/4571]  eta: 0:00:36    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1840/4571]  eta: 0:00:36    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1860/4571]  eta: 0:00:36    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1880/4571]  eta: 0:00:36    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1900/4571]  eta: 0:00:35    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [1920/4571]  eta: 0:00:35    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [1940/4571]  eta: 0:00:35    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [1960/4571]  eta: 0:00:35    time: 0.0135  data: 0.0127  max mem: 14785
 0: Epoch: [3]  [1980/4571]  eta: 0:00:34    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2000/4571]  eta: 0:00:34    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [2020/4571]  eta: 0:00:34    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2040/4571]  eta: 0:00:33    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [2060/4571]  eta: 0:00:33    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2080/4571]  eta: 0:00:33    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [2100/4571]  eta: 0:00:33    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2120/4571]  eta: 0:00:32    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2140/4571]  eta: 0:00:32    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [2160/4571]  eta: 0:00:32    time: 0.0174  data: 0.0163  max mem: 14785
 0: Epoch: [3]  [2180/4571]  eta: 0:00:32    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2200/4571]  eta: 0:00:31    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2220/4571]  eta: 0:00:31    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2240/4571]  eta: 0:00:31    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [2260/4571]  eta: 0:00:31    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [2280/4571]  eta: 0:00:30    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [2300/4571]  eta: 0:00:30    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [2320/4571]  eta: 0:00:30    time: 0.0140  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2340/4571]  eta: 0:00:29    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [2360/4571]  eta: 0:00:29    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2380/4571]  eta: 0:00:29    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2400/4571]  eta: 0:00:29    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [2420/4571]  eta: 0:00:28    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2440/4571]  eta: 0:00:28    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2460/4571]  eta: 0:00:28    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2480/4571]  eta: 0:00:28    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2500/4571]  eta: 0:00:27    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2520/4571]  eta: 0:00:27    time: 0.0184  data: 0.0177  max mem: 14785
 0: Epoch: [3]  [2540/4571]  eta: 0:00:27    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2560/4571]  eta: 0:00:27    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [2580/4571]  eta: 0:00:26    time: 0.0138  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [2600/4571]  eta: 0:00:26    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2620/4571]  eta: 0:00:26    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2640/4571]  eta: 0:00:25    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2660/4571]  eta: 0:00:25    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [2680/4571]  eta: 0:00:25    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2700/4571]  eta: 0:00:25    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2720/4571]  eta: 0:00:24    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2740/4571]  eta: 0:00:24    time: 0.0181  data: 0.0174  max mem: 14785
 0: Epoch: [3]  [2760/4571]  eta: 0:00:24    time: 0.0134  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [2780/4571]  eta: 0:00:24    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2800/4571]  eta: 0:00:23    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2820/4571]  eta: 0:00:23    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2840/4571]  eta: 0:00:23    time: 0.0138  data: 0.0127  max mem: 14785
 0: Epoch: [3]  [2860/4571]  eta: 0:00:23    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2880/4571]  eta: 0:00:22    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [2900/4571]  eta: 0:00:22    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [2920/4571]  eta: 0:00:22    time: 0.0131  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [2940/4571]  eta: 0:00:21    time: 0.0133  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [2960/4571]  eta: 0:00:21    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [2980/4571]  eta: 0:00:21    time: 0.0130  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3000/4571]  eta: 0:00:21    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [3020/4571]  eta: 0:00:20    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [3040/4571]  eta: 0:00:20    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [3060/4571]  eta: 0:00:20    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3080/4571]  eta: 0:00:20    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3100/4571]  eta: 0:00:19    time: 0.0132  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [3120/4571]  eta: 0:00:19    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [3140/4571]  eta: 0:00:19    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [3160/4571]  eta: 0:00:18    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [3180/4571]  eta: 0:00:18    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3200/4571]  eta: 0:00:18    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [3220/4571]  eta: 0:00:18    time: 0.0168  data: 0.0157  max mem: 14785
 0: Epoch: [3]  [3240/4571]  eta: 0:00:17    time: 0.0130  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3260/4571]  eta: 0:00:17    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [3280/4571]  eta: 0:00:17    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [3300/4571]  eta: 0:00:17    time: 0.0186  data: 0.0175  max mem: 14785
 0: Epoch: [3]  [3320/4571]  eta: 0:00:16    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3340/4571]  eta: 0:00:16    time: 0.0181  data: 0.0170  max mem: 14785
 0: Epoch: [3]  [3360/4571]  eta: 0:00:16    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [3380/4571]  eta: 0:00:16    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3400/4571]  eta: 0:00:15    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [3420/4571]  eta: 0:00:15    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3440/4571]  eta: 0:00:15    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3460/4571]  eta: 0:00:15    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3480/4571]  eta: 0:00:14    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3500/4571]  eta: 0:00:14    time: 0.0171  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3520/4571]  eta: 0:00:14    time: 0.0129  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3540/4571]  eta: 0:00:13    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3560/4571]  eta: 0:00:13    time: 0.0136  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [3580/4571]  eta: 0:00:13    time: 0.0137  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [3600/4571]  eta: 0:00:13    time: 0.0135  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [3620/4571]  eta: 0:00:12    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3640/4571]  eta: 0:00:12    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3660/4571]  eta: 0:00:12    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [3680/4571]  eta: 0:00:12    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [3700/4571]  eta: 0:00:11    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3720/4571]  eta: 0:00:11    time: 0.0132  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [3740/4571]  eta: 0:00:11    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3760/4571]  eta: 0:00:10    time: 0.0136  data: 0.0126  max mem: 14785
 0: Epoch: [3]  [3780/4571]  eta: 0:00:10    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [3800/4571]  eta: 0:00:10    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [3820/4571]  eta: 0:00:10    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3840/4571]  eta: 0:00:09    time: 0.0127  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [3860/4571]  eta: 0:00:09    time: 0.0150  data: 0.0139  max mem: 14785
 0: Epoch: [3]  [3880/4571]  eta: 0:00:09    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [3900/4571]  eta: 0:00:09    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3920/4571]  eta: 0:00:08    time: 0.0133  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [3940/4571]  eta: 0:00:08    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [3960/4571]  eta: 0:00:08    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [3980/4571]  eta: 0:00:07    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [4000/4571]  eta: 0:00:07    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [4020/4571]  eta: 0:00:07    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4040/4571]  eta: 0:00:07    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [3]  [4060/4571]  eta: 0:00:06    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4080/4571]  eta: 0:00:06    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4100/4571]  eta: 0:00:06    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4120/4571]  eta: 0:00:06    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4140/4571]  eta: 0:00:05    time: 0.0171  data: 0.0163  max mem: 14785
 0: Epoch: [3]  [4160/4571]  eta: 0:00:05    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [4180/4571]  eta: 0:00:05    time: 0.0134  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [4200/4571]  eta: 0:00:05    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4220/4571]  eta: 0:00:04    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4240/4571]  eta: 0:00:04    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [4260/4571]  eta: 0:00:04    time: 0.0140  data: 0.0129  max mem: 14785
 0: Epoch: [3]  [4280/4571]  eta: 0:00:03    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [4300/4571]  eta: 0:00:03    time: 0.0134  data: 0.0124  max mem: 14785
 0: Epoch: [3]  [4320/4571]  eta: 0:00:03    time: 0.0132  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [4340/4571]  eta: 0:00:03    time: 0.0139  data: 0.0128  max mem: 14785
 0: Epoch: [3]  [4360/4571]  eta: 0:00:02    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [4380/4571]  eta: 0:00:02    time: 0.0133  data: 0.0122  max mem: 14785
 0: Epoch: [3]  [4400/4571]  eta: 0:00:02    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4420/4571]  eta: 0:00:02    time: 0.0134  data: 0.0127  max mem: 14785
 0: Epoch: [3]  [4440/4571]  eta: 0:00:01    time: 0.0131  data: 0.0123  max mem: 14785
 0: Epoch: [3]  [4460/4571]  eta: 0:00:01    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4480/4571]  eta: 0:00:01    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [3]  [4500/4571]  eta: 0:00:00    time: 0.0179  data: 0.0168  max mem: 14785
 0: Epoch: [3]  [4520/4571]  eta: 0:00:00    time: 0.0130  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4540/4571]  eta: 0:00:00    time: 0.0133  data: 0.0121  max mem: 14785
 0: Epoch: [3]  [4560/4571]  eta: 0:00:00    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [3]  [4570/4571]  eta: 0:00:00    time: 0.0137  data: 0.0125  max mem: 14785
 0: Epoch: [3] Total time: 0:01:01 (0.0135 s / it)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985416579, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 3, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 3}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985416580, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 18922.345457362244, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985416581, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 347, "epoch_num": 4}}
 0: Test:  [ 0/13]  eta: 0:00:03  model_time: 0.2748 (0.2748)  evaluator_time: 0.0035 (0.0035)  time: 0.2794  data: 0.0010  max mem: 14785
 0: Test:  [12/13]  eta: 0:00:00  model_time: 0.2746 (0.2584)  evaluator_time: 0.0038 (0.0130)  time: 0.2723  data: 0.0008  max mem: 14785
 0: Test: Total time: 0:00:03 (0.2723 s / it)
 0: Averaged stats: model_time: 0.2746 (0.2585)  evaluator_time: 0.0038 (0.0081)
 0: :::MLLOG {"namespace": "", "time_ms": 1745985420564, "event_type": "INTERVAL_START", "key": "epoch_start", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 158, "epoch_num": 4}}
 0: Epoch: [4]  [   0/4572]  eta: 0:00:12    time: 0.0028  data: 0.0008  max mem: 14785
 0: Epoch: [4]  [  20/4572]  eta: 0:00:56    time: 0.0129  data: 0.0122  max mem: 14785
 0: Epoch: [4]  [  40/4572]  eta: 0:00:58    time: 0.0135  data: 0.0124  max mem: 14785
 0: Epoch: [4]  [  60/4572]  eta: 0:00:58    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [  80/4572]  eta: 0:00:58    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [ 100/4572]  eta: 0:00:58    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [ 120/4572]  eta: 0:00:58    time: 0.0132  data: 0.0125  max mem: 14785
 0: Epoch: [4]  [ 140/4572]  eta: 0:00:58    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [4]  [ 160/4572]  eta: 0:00:57    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [ 180/4572]  eta: 0:00:57    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [ 200/4572]  eta: 0:00:57    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [4]  [ 220/4572]  eta: 0:00:57    time: 0.0131  data: 0.0119  max mem: 14785
 0: Epoch: [4]  [ 240/4572]  eta: 0:00:56    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [ 260/4572]  eta: 0:00:56    time: 0.0131  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [ 280/4572]  eta: 0:00:56    time: 0.0131  data: 0.0124  max mem: 14785
 0: Epoch: [4]  [ 300/4572]  eta: 0:00:56    time: 0.0136  data: 0.0127  max mem: 14785
 0: Epoch: [4]  [ 320/4572]  eta: 0:00:56    time: 0.0136  data: 0.0125  max mem: 14785
 0: Epoch: [4]  [ 340/4572]  eta: 0:00:55    time: 0.0132  data: 0.0120  max mem: 14785
 0: Epoch: [4]  [ 360/4572]  eta: 0:00:55    time: 0.0132  data: 0.0124  max mem: 14785
 0: :::MLLOG {"namespace": "", "time_ms": 1745985425338, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.34311045769916243, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 440, "epoch_num": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985425338, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 441, "epoch_num": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985425608, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 315, "status": "success"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985425608, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": 4, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 330, "epoch_num": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1745985425609, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 19437.58799717122, "max_memory_usage": 14.438}, "metadata": {"file": "/workspace/ssd/engine.py", "lineno": 334, "step": 5}}
 0: Run time 0:04:27
 0: :::MLLOG {"namespace": "", "time_ms": 1745985425609, "event_type": "POINT_IN_TIME", "key": "status", "value": "success", "metadata": {"file": "/workspace/ssd/train.py", "lineno": 783}}
20: | distributed init (rank 20): env://
32: | distributed init (rank 32): env://
48: | distributed init (rank 48): env://
 2: | distributed init (rank 2): env://
13: | distributed init (rank 13): env://
11: | distributed init (rank 11): env://
57: | distributed init (rank 57): env://
 3: | distributed init (rank 3): env://
39: | distributed init (rank 39): env://
38: | distributed init (rank 38): env://
22: | distributed init (rank 22): env://
 9: | distributed init (rank 9): env://
18: | distributed init (rank 18): env://
59: | distributed init (rank 59): env://
21: | distributed init (rank 21): env://
23: | distributed init (rank 23): env://
16: | distributed init (rank 16): env://
19: | distributed init (rank 19): env://
51: | distributed init (rank 51): env://
60: | distributed init (rank 60): env://
 7: | distributed init (rank 7): env://
17: | distributed init (rank 17): env://
 4: | distributed init (rank 4): env://
63: | distributed init (rank 63): env://
35: | distributed init (rank 35): env://
 5: | distributed init (rank 5): env://
 0: Loading annotations into memory...
 0: Done (t=0.48s)
 0: Creating index...
 0: Done (t=1.31s)
 0: Loading and preparing results...
 0: DONE (t=4.14s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=2.95s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.19573
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.31778
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.20491
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00436
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.04813
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.21652
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.33389
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.48129
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.50311
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.01737
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.18222
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.54913
 0: Loading and preparing results...
 0: DONE (t=2.91s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=2.41s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.27507
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.41691
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.29296
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00539
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.07766
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.30476
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.37187
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.53242
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.55809
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.02983
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.23529
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.60699
 0: Loading and preparing results...
 0: DONE (t=2.56s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=2.22s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.32642
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.46952
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.35077
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00780
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.09297
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.36109
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.39881
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.57025
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.59762
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03705
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24736
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.64899
 0: Loading and preparing results...
 0: DONE (t=2.47s)
 0: Running per image evaluation...
 0: Evaluate annotation type *bbox*
 0: DONE (t=2.11s).
 0: Accumulating evaluation results...
 0: DONE (t=0.00s).
 0: IoU metric: bbox
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34311
 0:  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.48934
 0:  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.37004
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.00841
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.09057
 0:  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.38030
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.40674
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.57876
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.60583
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.03762
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.24754
 0:  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.65849
29: | distributed init (rank 29): env://
49: | distributed init (rank 49): env://
33: | distributed init (rank 33): env://
44: | distributed init (rank 44): env://
40: | distributed init (rank 40): env://
10: | distributed init (rank 10): env://
36: | distributed init (rank 36): env://
52: | distributed init (rank 52): env://
50: | distributed init (rank 50): env://
 6: | distributed init (rank 6): env://
58: | distributed init (rank 58): env://
45: | distributed init (rank 45): env://
56: | distributed init (rank 56): env://
 1: | distributed init (rank 1): env://
55: | distributed init (rank 55): env://
61: | distributed init (rank 61): env://
42: | distributed init (rank 42): env://
62: | distributed init (rank 62): env://
54: | distributed init (rank 54): env://
37: | distributed init (rank 37): env://
26: | distributed init (rank 26): env://
43: | distributed init (rank 43): env://
53: | distributed init (rank 53): env://
27: | distributed init (rank 27): env://
47: | distributed init (rank 47): env://
46: | distributed init (rank 46): env://
41: | distributed init (rank 41): env://
25: | distributed init (rank 25): env://
31: | distributed init (rank 31): env://
34: | distributed init (rank 34): env://
24: | distributed init (rank 24): env://
28: | distributed init (rank 28): env://
30: | distributed init (rank 30): env://
 8: | distributed init (rank 8): env://
15: | distributed init (rank 15): env://
14: | distributed init (rank 14): env://
12: | distributed init (rank 12): env://
 8: [rank8]:[W430 03:57:07.513901182 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
16: [rank16]:[W430 03:57:07.158211437 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
24: [rank24]:[W430 03:57:07.429352491 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
32: [rank32]:[W430 03:57:07.717428381 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
 0: [W430 03:57:07.732979981 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
48: [rank48]:[W430 03:57:07.709711636 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
40: [rank40]:[W430 03:57:07.760316969 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
56: [rank56]:[W430 03:57:07.732843421 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
15: [rank15]:[W430 03:57:08.035995913 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
16: [rank16]:[W430 03:57:08.606403602 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
11: [rank11]:[W430 03:57:08.045057628 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
13: [rank13]:[W430 03:57:08.047231772 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 9: [rank9]:[W430 03:57:08.049768393 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
22: [rank22]:[W430 03:57:08.621664180 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 8: [rank8]:[W430 03:57:08.059910872 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
14: [rank14]:[W430 03:57:08.061831845 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
10: [rank10]:[W430 03:57:08.062369561 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
18: [rank18]:[W430 03:57:08.626116158 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
17: [rank17]:[W430 03:57:08.627117434 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
12: [rank12]:[W430 03:57:08.064179104 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
20: [rank20]:[W430 03:57:08.628757078 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
19: [rank19]:[W430 03:57:08.631570611 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
21: [rank21]:[W430 03:57:08.635929439 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
56: [rank56]:[W430 03:57:08.002791512 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
32: [rank32]:[W430 03:57:08.185810890 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
34: [rank34]:[W430 03:57:08.206084147 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
38: [rank38]:[W430 03:57:08.209074361 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
36: [rank36]:[W430 03:57:08.210138887 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
35: [rank35]:[W430 03:57:08.216514158 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
37: [rank37]:[W430 03:57:08.218786829 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
27: [rank27]:[W430 03:57:08.950876995 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
33: [rank33]:[W430 03:57:08.220659092 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
25: [rank25]:[W430 03:57:08.953295178 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
29: [rank29]:[W430 03:57:08.955690996 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
24: [rank24]:[W430 03:57:08.961153827 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
26: [rank26]:[W430 03:57:08.961298417 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
28: [rank28]:[W430 03:57:08.963470940 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
41: [rank41]:[W430 03:57:08.214514931 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
30: [rank30]:[W430 03:57:08.966070060 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
48: [rank48]:[W430 03:57:08.176460554 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
40: [rank40]:[W430 03:57:08.229409971 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
42: [rank42]:[W430 03:57:08.233328953 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
43: [rank43]:[W430 03:57:08.342939011 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
45: [rank45]:[W430 03:57:08.351353375 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
44: [rank44]:[W430 03:57:08.352545318 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
47: [rank47]:[W430 03:57:08.354845107 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
46: [rank46]:[W430 03:57:08.354979527 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
49: [rank49]:[W430 03:57:08.309733675 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
50: [rank50]:[W430 03:57:08.310106311 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
53: [rank53]:[W430 03:57:08.310229513 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
51: [rank51]:[W430 03:57:08.310415977 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
52: [rank52]:[W430 03:57:08.311111015 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
54: [rank54]:[W430 03:57:08.311678020 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
60: [rank60]:[W430 03:57:08.258870318 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
59: [rank59]:[W430 03:57:08.260553331 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
62: [rank62]:[W430 03:57:08.262027505 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
57: [rank57]:[W430 03:57:08.262506511 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
58: [rank58]:[W430 03:57:08.264084460 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
61: [rank61]:[W430 03:57:08.264649352 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
63: [rank63]:[W430 03:57:08.411434555 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
23: [rank23]:[W430 03:57:08.117116120 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
31: [rank31]:[W430 03:57:08.371941847 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 0: NCCL version 2.25.1+cuda12.9
 0: [rank0]:[W430 03:57:08.650606183 ProcessGroupNCCL.cpp:1477] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
39: [rank39]:[W430 03:57:08.674991289 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
55: [rank55]:[W430 03:57:08.805441423 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 7: [rank7]:[W430 03:57:08.920987909 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 6: [rank6]:[W430 03:57:08.931124913 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 3: [rank3]:[W430 03:57:08.943619231 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 5: [rank5]:[W430 03:57:08.946784523 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 4: [rank4]:[W430 03:57:08.947009933 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 1: [rank1]:[W430 03:57:08.947082276 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 2: [rank2]:[W430 03:57:08.950065279 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 0: [rank0]:[W430 03:57:09.415340697 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
++ date +%s
+ echo 'RUNANDTIME_STOP 1745985435'
RUNANDTIME_STOP 1745985435
+ set -e
